[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "DIY Wine Database with postgreSQL\n\n\n\n\n\n\n\nbash\n\n\nYAML\n\n\nGitHub Actions\n\n\n\n\nImagine the LCBO website without ads, overlays, cookies, and clunky UI.\n\n\n\n\n\n\nJul 1, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nMaking my own scraper bot!\n\n\n\n\n\n\n\nbash\n\n\nYAML\n\n\nGitHub Actions\n\n\n\n\nI made a bot that runs on GitHub to regularly collect LCBO data.\n\n\n\n\n\n\nJun 22, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nScraping LCBO Data (Part 3: Product Descriptions)\n\n\n\n\n\n\n\nPython\n\n\nHTML\n\n\nbash\n\n\nJSON\n\n\n\n\nLearn how to find and download data products from the LCBO webpage.\n\n\n\n\n\n\nJun 17, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nScraping LCBO Data (Part 2: Product Inventory)\n\n\n\n\n\n\n\nPython\n\n\nHTML\n\n\nbash\n\n\nJSON\n\n\n\n\nScraping with regex.\n\n\n\n\n\n\nJun 10, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nScraping LCBO Data (Part 1: Store Information)\n\n\n\n\n\n\n\nPython\n\n\nHTML\n\n\nbash\n\n\nJSON\n\n\n\n\nLearn how to find and download data products from the LCBO webpage.\n\n\n\n\n\n\nJun 1, 2023\n\n\nStephen Ro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#table-details",
    "href": "posts/2023-07-01-lcbo-psql/index.html#table-details",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Table details",
    "text": "Table details\nThe LCBO assigns a unique identifier for each store and product, store_id and sku number. These identifiers are the primary keys for each row of data. Below is a snapshot of the column names, data types, and nullability (?) in both tables.\n\n\n\n\n\n\n\n(a) products\n\n\n\n\n\n\n\n(b) products (cont’d)\n\n\n\n\n\n\n\n(c) stores\n\n\n\n\n\n\n\n\n\n(d) notes\n\n\n\n\n\n\n\n(e) prices\n\n\n\n\n\n\n\n(f) inventory\n\n\n\n\nFigure 1: List of column names, data types, and nullability for five tables. prices and inventory tables have composite (two) primary keys shown.\n\n\nI want to keep track of the price history of a product and only make updates when a price change has been noticed. The prices table uses a composite primary key of two columns, sku and checktime. checktime is the UNIX time for when the row was inserted into the table.\nUnlike the prices table, the inventory history is not recorded. This could be changed by creating a composite primary key of three columns. However, this could generate a lot of data that I do not currently need for my app."
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#table-relationships",
    "href": "posts/2023-07-01-lcbo-psql/index.html#table-relationships",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Table relationships",
    "text": "Table relationships\n\nThe lines connecting the tables indicate the relationships between them. The prices nad notes use composite primary keys derived from products and stores. The inventory has no primary keys but does require the sku value must exist in products.\nThese tables were designed using DrawSQL. You can freely download the PSQL commands to create these tables in your database. Here is a copy of the script:\n\n\n\n\n\n\nPSQL commands to CREATE and ALTER tables.\n\n\n\n\n\n-- 1. STORES\nCREATE TABLE \"stores\"(\n    \"store_id\" INTEGER NOT NULL,\n    \"name\" TEXT NULL,\n    \"lat\" DECIMAL(8, 2) NULL,\n    \"lng\" DECIMAL(8, 2) NULL,\n    \"address\" TEXT NULL,\n    \"city\" TEXT NULL,\n    \"zipcode\" TEXT NULL,\n    \"intersection\" TEXT NULL,\n    \"phone\" TEXT NULL,\n    \"url\" TEXT NULL\n);\nALTER TABLE\n    \"stores\" ADD PRIMARY KEY(\"store_id\");\n\n-- 2. products\nCREATE TABLE \"products\"(\n    \"sku\" INTEGER NOT NULL,\n    \"name\" TEXT NULL,\n    \"is_available\" BOOLEAN NOT NULL,\n    \"abv\" DECIMAL(8, 2) NULL,\n    \"volume\" INTEGER NULL,\n    \"quantity_per_package\" INTEGER NULL DEFAULT '1',\n    \"package_type\" TEXT NULL,\n    \"category\" TEXT NULL,\n    \"region\" TEXT NULL,\n    \"country\" TEXT NULL,\n    \"brand\" TEXT NULL,\n    \"description\" TEXT NULL DEFAULT 'No description available.',\n    \"url\" TEXT NULL,\n    \"url_thumbnail\" TEXT NULL,\n    \"notes\" TEXT NULL,\n    \"sugar_gm_per_ltr\" INTEGER NULL,\n    \"calories\" INTEGER NULL,\n    \"varietal\" TEXT NULL,\n    \"sweetness\" INTEGER NULL,\n    \"body\" INTEGER NULL,\n    \"flavor\" INTEGER NULL,\n    \"tannins\" INTEGER NULL,\n    \"acidity\" INTEGER NULL\n);\nALTER TABLE\n    \"products\" ADD PRIMARY KEY(\"sku\");\n\n-- 3. PRICES\nCREATE TABLE \"prices\"(\n    \"sku\" INTEGER NOT NULL,\n    \"price_cents\" INTEGER NULL,\n    \"promo_price_cents\" INTEGER NULL,\n    \"checktime\" BIGINT NOT NULL\n);\nALTER TABLE\n    \"prices\" ADD PRIMARY KEY(\"sku\", \"checktime\");\nALTER TABLE\n    \"prices\" ADD CONSTRAINT \"prices_sku_foreign\" FOREIGN KEY(\"sku\") REFERENCES \"products\"(\"sku\");\n    \n-- 4. INVENTORY\nCREATE TABLE \"inventory\"(\n    \"sku\" INTEGER NOT NULL,\n    \"store_id\" INTEGER NOT NULL,\n    \"quantity\" INTEGER NOT NULL,\n    \"checktime\" BIGINT NOT NULL\n);\nALTER TABLE\n    \"inventory\" ADD PRIMARY KEY(\"store_id\", \"sku\");\nALTER TABLE\n    \"inventory\" ADD CONSTRAINT \"inventory_store_id_foreign\" FOREIGN KEY(\"store_id\") REFERENCES \"stores\"(\"store_id\");\nALTER TABLE\n    \"inventory\" ADD CONSTRAINT \"inventory_sku_foreign\" FOREIGN KEY(\"sku\") REFERENCES \"products\"(\"sku\");\n\n-- 5. NOTES\nCREATE TABLE \"notes\"(\n    \"sku\" INTEGER NOT NULL,\n    \"note\" TEXT NOT NULL\n);\nALTER TABLE\n    \"notes\" ADD CONSTRAINT \"notes_sku_foreign\" FOREIGN KEY(\"sku\") REFERENCES \"products\"(\"sku\");\n\n\n\nNote, you can delete a table with the simple command:\nDROP TABLE tablename;\nMake sure to first delete tables that have keys or indices that are derivative of other tables (i.e., tables with foreign keys)."
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#basic-python-functions-for-psql",
    "href": "posts/2023-07-01-lcbo-psql/index.html#basic-python-functions-for-psql",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Basic Python Functions for PSQL",
    "text": "Basic Python Functions for PSQL\n\nOne-way Query:\nThe following is a barebones one-way query tool that requires the PSQL address and the query to send. I use this to drop and re-create a table, for example. Do not forget the .commit() to execute your query.\n\n\n\n\n\n\nOne-way Query Function\n\n\n\n\n\nimport psycopg2\ndef query(sql_address, query):\n    connection = psycopg2.connect(sql_address)\n    cur = connection.cursor()\n    cur.execute(query)\n    connection.commit()\n    connection.close()\n\n\n\n\n\nTable Query:\nThe following can return table queries such as “SELECT * FROM products LIMIT 10;”. The return is a list of tuples and the column name for each element in the tuple.\n\n\n\n\n\n\nTable Query Function\n\n\n\n\n\nimport psycopg2\ndef query_table(sql_address, query):\n    connection = psycopg2.connect(sql_address)\n    cur = connection.cursor()\n    cur.execute(query)\n    columnName = [desc[0] for desc in cur.description]\n    tbl = cur.fetchall()\n    connection.close()\n\n    return tbl, columnName  \n\n\n\n\n\nInsert Table Rows:\nHere is a function that inserts rows of data to my PSQL database (ref). It has a nice catch for SQL exceptions.\nThe arguments require the PSQL address, the table name, and column names. records is the data represented as a list of tuples where each tuple’s elements are defined and ordered identically to the list of column names, columns.\nNote, psycopg2 has multiple solutions for inserting rows and some are much faster than others (by orders of magnitude). I found psycopg2.extras.execute_values to be the fastest.\n\n\n\n\n\n\nInsert Table Rows Function\n\n\n\n\n\nimport psycopg2\nimport psycopg2.extras \ndef bulkInsert(sql_address, tableName, records, columns):\n    try:\n        connection = psycopg2.connect(sql_address)        \n        cursor = connection.cursor()\n\n        tableColumns = list(columns)\n        \n        sql_insert_query = f\"\"\"\n        INSERT INTO {tableName} ({', '.join(tableColumns)})\n        VALUES %s\n        \"\"\"\n        result = psycopg2.extras.execute_values(cursor, sql_insert_query, records)\n         \n        connection.commit()\n        print(cursor.rowcount, f\"Record inserted successfully into {tableName} table\")\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Error while updating PostgreSQL table\", error)\n\n    finally:\n        # closing database connection.\n        if connection:\n            cursor.close()\n            connection.close()\n            print(\"PostgreSQL connection is closed\")\n\n\n\nHere is an example of how to insert multiple rows with the above code.\nsql_address = \"postgres://USERNAME:PASSWORD@SERVER/DATABASE\"\ntablename = \"products\"\ndata = [\n        ('La Pamplemousse', 750, 14.5, 19.99),\n        ('Bargain Red Wine', 750, 14.0, 26.99),\n        ]\ncolumns = ['name', 'volume', 'abv', 'price']\n\nbulkInsert(sql_address, tablename, data, columns)\nThe bulkInsert function should suffice for inserting rows of data into a table. If I obtain a new product to insert into the table, I can use query_table to obtain the existing list to ensure the product is indeed new. A simple python solution for products is:\nskuOldList = query_table(sql_address, \"select sku from products;\")\nskuList = read_from_data_files()\nskuNewList = set(skuList) - set(skuOldList)"
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#creating-a-price-history-table",
    "href": "posts/2023-07-01-lcbo-psql/index.html#creating-a-price-history-table",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Creating a Price History Table",
    "text": "Creating a Price History Table\nI only insert the price if it has changed in value. I need to create a table from our prices table the most recent prices to make a comparison. This could be done using two approaches: temporary tables and views. Choosing which depends on the context and query complexity. The SQL commands are identical except for TEMP TABLE and VIEW.\n--CREATE VIEW price_temp AS\nCREATE TEMP TABLE temp_prices AS\nselect sku, price_cents, promo_price_cents, checktime from prices\ninner join (\n    select sku as sku2, max(checktime) as checktime2 from prices\n    group by sku\n) as max_checktime \non sku = sku2\nand checktime = checktime2\n;\nTemporary tables exist until the connection to the database is closed. The data added to the table can be updated or changed. It does not depend on original table(s) after creation. This is a more memory intensive process.\nViews exist only for one query! Each time the table is called upon, it is regenerated and can differ if the original tables change values (i.e., the view table is always current). This is a more computing intensive process.\nThe views solution is not suitable for my task because:\n\nIt can be completed in one query\nA single product is considered only once (so, regeneration is not necessary)\nMy server has low CPU speeds but plenty of memory\nThe task is executed on GitHub Actions. So, using views with low database CPU speeds with views would waste computing time on GitHub actions\nMy data is relatively small in memory (~45 MB).\n\nAfter creating a temporary table, the price data is inserted using the following sample code. Suppose I checked a product and find $15.00 and $10.00 are the regular and promotional prices. If either of these values differ from the last price entry then we need to insert the change.\nINSERT into prices (sku, price_cents, promo_price_cents, checktime)\nselect 10101, 1499, 999, 1686005186\nwhere not exists  (\n    select checktime from temp_prices where\n    1499 = price_cents and\n    999 = promo_price_cents and\n    1686005186 &gt;= checktime and\n    10101 = sku\n)\n;"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html",
    "href": "posts/2023-06-17-lcbo-scraper/index.html",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "",
    "text": "Designing a web-scraping strategy depends on your constraints (e.g.,deadlines, uncertainties, awareness), objectives (e.g., features, speed, reproducability), and experience! In this post, I talk about my experience of trying two different strategies to collect the product description data. Trying both strategies revealed valuable insights into the strengths and weaknesses of each. I share these insights below."
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#brute-forcing-strategy",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#brute-forcing-strategy",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Brute-forcing strategy:",
    "text": "Brute-forcing strategy:\nI noticed the product page contains a link “Check Availability in All Stores” that sends you to a well organized inventory page for this bottle! (We will get back to this inventory data later.) The inventory page has a “View Details” button that sends you back to the product page.\n\n\n\n\n\n\nSample inventory page\n\n\n\n\n\n\n\n\nInventory for Jackson-Triggs Cabernet Sauvignon\n\n\n\n\n\nUnlike the product page URL, the product inventory URL is structured simply as https://www.lcbo.com/en/storeinventory/?sku=32853. It looks like I can query inventory pages by selecting SKU numbers. If I have the SKU number then I can use the URL behind the “View Details” button to get the product descriptions URL. From there, I can use the techniques in Part 1 of the blog series to obtain JSON data containing product descriptions.\nThis technique works. The solution is a minor adaptation to the script shown in Part 1 of the blog series. I found approximately 13,000 SKU numbers have products associated with them. However, this brute-force approach took several days to perform. The reason is LCBO’s SKU number is up to 6 digits long which has 1 million possible combinations. So, only 1.3% of the requests were successful."
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Pros & Cons",
    "text": "Pros & Cons\nThis strategy feels quite embarassing to share. But, its valuable to make a poorly performing but “successful” solution. It made me appreciate how thoughtful and clever web scraping can be. A few small mistakes or accounting errors can lead to restarting the entire scraping process. Even worse is to patch together two poorly executed scraping results and hope things are okay. The frustrations compel me to consider more than one solution before deciding to commit and build.\nHere are my observations of the pros and cons for brute-forcing.\n\nPros to brute-forcing\n\nEasy to implement\nEasy to debug\nCan do other work while waiting\nFeels like you are working\n\n\n\nCons to brute-forcing\n\nExtremely slow and inefficient\nVulnerable to computer and network instabilities\nRequires babysitting\nRequires two steps (product inventory -&gt; product page)\nVery likely to have IP banned\nSuspicious and straining to servers\nGenerates enormous inertia to run again\nGives a false impression that you are working"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#query-strategy",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#query-strategy",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Query strategy:",
    "text": "Query strategy:\n\nPart 1: Finding the data\nUsing the techniques in Part 1 of the blog series, I found a relatively large JSON file is transferred when querying the LCBO product list. See the image below.\n\n\n\n\n\n\nJSON for product exploration page\n\n\n\n\n\n\n\n\n\nThe primary variables we care about are the first and last variables, “totalCount” and “results”.\n“totalCount” is the number of products resulting from my query. Since the query had no filters, there were 13,186 results products to be found.\n“results” is the list of wine products on display. It contains all of the product descriptions we are interested in and more. There is a LOT of data here that we can unpack. Here are some snapshots for one product:\n\nJSON part 1\nJSON part 2\nJSON part 3.\n\n\n\nPart 2: Requesting the data\nThis is where things become a little tricky. In Part 1, we found the cURL command to request store information data. The command had a single integer as an argument corresponding to the store ID. Here, we will have several parameters due to the numerous filters available. The parameters also contain hidden limits. For example, I cannot simply request a single JSON with 13,186 products (I tried, of course). As a result, we need to procede carefully with designing a scraping strategy.\nAfter importing the cURL command in Postman, this is what I see:\n\n\n\n\n\n\nJSON for product exploration page\n\n\n\n\n\n\n\n\n\nThe code snippet on the right is this:\n\n\n\n\n\n\ncURL command\n\n\n\n\n\ncurl –location ‘https://platform.cloud.coveo.com/rest/search/v2?organizationId=lcboproductionx2kwygnc’\n–header ‘User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0’\n–header ‘Accept: /’\n–header ‘Accept-Language: en-CA,en-US;q=0.7,en;q=0.3’\n–header ‘Accept-Encoding: gzip, deflate, br’\n–header ‘Authorization: Bearer xx883b5583-07fb-416b-874b-77cce565d927’\n–header ‘Content-Type: application/x-www-form-urlencoded; charset=UTF-8’\n–header ‘Origin: https://www.lcbo.com’\n–header ‘DNT: 1’\n–header ‘Connection: keep-alive’\n–header ‘Referer: https://www.lcbo.com/’\n–header ‘Sec-Fetch-Dest: empty’\n–header ‘Sec-Fetch-Mode: cors’\n–header ‘Sec-Fetch-Site: cross-site’\n–data ‘actionsHistory=%5B%5D&referrer=&analytics=%7B%22clientId%22%3A%22%22%2C%22documentLocation%22%3A%22https%3A%2F%2Fwww.lcbo.com%2Fen%2Fproducts%23t%3Dclp-products%26sort%3Drelevancy%26layout%3Dcard%22%2C%22documentReferrer%22%3A%22%22%2C%22pageId%22%3A%22%22%7D&isGuestUser=false&aq=%40ec_category%3D%3DProducts&searchHub=Web_Listing_EN&tab=clp-products&locale=en&firstResult=0&numberOfResults=48&excerptLength=200&enableDidYouMean=true&sortCriteria=relevancy&queryFunctions=%5B%5D&rankingFunctions=%5B%5D&groupBy=%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_sweetness%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_sweetness%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_sweetness%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_body%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_body%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_body%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_flavor_intensity%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_flavor_intensity%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_flavor_intensity%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_acidity%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_acidity%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_acidity%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_tannins%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_tannins%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_tannins%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40ec_price%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40ec_price%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40ec_price%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_total_volume%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_total_volume%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_total_volume%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_alcohol_percent%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_alcohol_percent%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_alcohol_percent%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_sugar_gm_per_ltr%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_sugar_gm_per_ltr%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_sugar_gm_per_ltr%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%5D&facets=%5B%7B%22facetId%22%3A%22%40ec_category%22%2C%22field%22%3A%22ec_category%22%2C%22type%22%3A%22hierarchical%22%2C%22injectionDepth%22%3A1000%2C%22delimitingCharacter%22%3A%22%7C%22%2C%22filterFacetCount%22%3Atrue%2C%22basePath%22%3A%5B%22Products%22%5D%2C%22filterByBasePath%22%3Afalse%2C%22currentValues%22%3A%5B%5D%2C%22preventAutoSelect%22%3Afalse%2C%22numberOfValues%22%3A5%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_varietal_name%22%2C%22field%22%3A%22lcbo_varietal_name%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_vqa_code%22%2C%22field%22%3A%22lcbo_vqa_code%22%2C%22type%22%3A%22specific%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A8%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40country_of_manufacture%22%2C%22field%22%3A%22country_of_manufacture%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_region_name%22%2C%22field%22%3A%22lcbo_region_name%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_program%22%2C%22field%22%3A%22lcbo_program%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_current_offer%22%2C%22field%22%3A%22lcbo_current_offer%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A8%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40stores_stock%22%2C%22field%22%3A%22stores_stock%22%2C%22type%22%3A%22specific%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A8%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40ec_rating%22%2C%22field%22%3A%22ec_rating%22%2C%22type%22%3A%22numericalRange%22%2C%22sortCriteria%22%3A%22descending%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%7B%22start%22%3A1%2C%22end%22%3A1.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A2%2C%22end%22%3A2.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A3%2C%22end%22%3A3.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A4%2C%22end%22%3A4.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A5%2C%22end%22%3A5%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22generateAutomaticRanges%22%3Afalse%7D%5D&facetOptions=%7B%7D&categoryFacets=%5B%5D&retrieveFirstSentences=true&timezone=America%2FNew_York&enableQuerySyntax=false&enableDuplicateFiltering=false&enableCollaborativeRating=false&debug=false&allowQueriesWithoutKeywords=true&dictionaryFieldContext=%7B%22stores_stock%22%3A%22%22%2C%22stores_inventory%22%3A%22217%22%2C%22stores_stock_combined%22%3A%22217%22%2C%22stores_low_stock_combined%22%3A%22217%22%7D’\n\n\n\nYIKES! The big block of text in the “–data” argument is URL encoded. We can decode this using online converters such as https://meyerweb.com/eric/tools/dencoder/ and prettify with http://urlprettyprint.com/. Here’s what I find:\nactionsHistory              = []\nreferrer                    = \nanalytics                   = {\"clientId\":\"\",\"documentLocation\":\"https://www.lcbo.com/en/products#t=clp-products&sort=relevancy&layout=card\",\"documentReferrer\":\"\",\"pageId\":\"\"}\nisGuestUser                 = false\naq                          = @ec_category==Products\nsearchHub                   = Web_Listing_EN\ntab                         = clp-products\nlocale                      = en\nfirstResult                 = 0\nnumberOfResults             = 48\nexcerptLength               = 200\nenableDidYouMean            = true\nsortCriteria                = relevancy\nqueryFunctions              = []\nrankingFunctions            = []\ngroupBy                     = [{\"field\":\"@lcbo_bintag_wine_sweetness\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_sweetness\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_sweetness\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_body\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_body\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_body\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_flavor_intensity\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_flavor_intensity\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_flavor_intensity\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_acidity\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_acidity\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_acidity\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_tannins\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_tannins\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_tannins\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@ec_price\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@ec_price\",\"operation\":\"minimum\"},{\"field\":\"@ec_price\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_total_volume\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_total_volume\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_total_volume\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_alcohol_percent\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_alcohol_percent\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_alcohol_percent\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_sugar_gm_per_ltr\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_sugar_gm_per_ltr\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_sugar_gm_per_ltr\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1}]\nfacets                      = [{\"facetId\":\"@ec_category\",\"field\":\"ec_category\",\"type\":\"hierarchical\",\"injectionDepth\":1000,\"delimitingCharacter\":\"|\",\"filterFacetCount\":true,\"basePath\":[\"Products\"],\"filterByBasePath\":false,\"currentValues\":[],\"preventAutoSelect\":false,\"numberOfValues\":5,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_varietal_name\",\"field\":\"lcbo_varietal_name\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_vqa_code\",\"field\":\"lcbo_vqa_code\",\"type\":\"specific\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":8,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@country_of_manufacture\",\"field\":\"country_of_manufacture\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_region_name\",\"field\":\"lcbo_region_name\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_program\",\"field\":\"lcbo_program\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_current_offer\",\"field\":\"lcbo_current_offer\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":8,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@stores_stock\",\"field\":\"stores_stock\",\"type\":\"specific\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":8,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@ec_rating\",\"field\":\"ec_rating\",\"type\":\"numericalRange\",\"sortCriteria\":\"descending\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[{\"start\":1,\"end\":1.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":2,\"end\":2.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":3,\"end\":3.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":4,\"end\":4.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":5,\"end\":5,\"endInclusive\":true,\"state\":\"idle\"}],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"generateAutomaticRanges\":false}]\nfacetOptions                = {}\ncategoryFacets              = []\nretrieveFirstSentences      = true\ntimezone                    = America/New_York\nenableQuerySyntax           = false\nenableDuplicateFiltering    = false\nenableCollaborativeRating   = false\ndebug                       = false\nallowQueriesWithoutKeywords = true\ndictionaryFieldContext      = {\"stores_stock\":\"\",\"stores_inventory\":\"217\",\"stores_stock_combined\":\"217\",\"stores_low_stock_combined\":\"217\"}\nWe can start throwing away some arguments that we do not need such as “groupBy” and “analytics”. With some experimentation1, I was able to widdle it down to this:\nlocale                      = en\nfirstResult                 = '\"$1\"'\nnumberOfResults             = '\"$2\"'\nexcerptLength               = 200\nsortCriteria                = @ec_price '\"$3\"'\nfacets                      = [{\"facetId\":\"@ec_category\",\"field\":\"ec_category\",\"type\":\"hierarchical\",\"injectionDepth\":1000,\"delimitingCharacter\":\"|\",\"filterFacetCount\":true,\"basePath\":[\"Products\"],\"filterByBasePath\":false,\"currentValues\":[{\"value\":\"wine\",\"state\":\"idle\",\"children\":[{\"value\":\"'\"$4\"'\",\"state\":\"selected\",\"children\":[],\"retrieveChildren\":false,\"retrieveCount\":0}],\"retrieveChildren\":false,\"retrieveCount\":0}],\"preventAutoSelect\":true,\"numberOfValues\":1,\"isFieldExpanded\":false}]\nfacetOptions                = {\"freezeFacetOrder\":true}\ncategoryFacets              = []\nretrieveFirstSentences      = true\ntimezone                    = America/New_York\nenableQuerySyntax           = false\nenableDuplicateFiltering    = false\nenableCollaborativeRating   = false\ndebug                       = false\nallowQueriesWithoutKeywords = true\nA few additional modifications have been made to the above.\n\nAdded a sort criterion “ec_price” for price.\nModified “facets” to return only wine-related products. See the end of this section for non-wine products.\nAdded 4 variable placeholders in particular locations with the following meanings:\n\n$1: page number (starts at 0)\n$2: number of requests per page number (max 10002)\n$3: sort order; use “ascending” or “descending” text without quotation marks.\n$4: wine-category; must be URL encoded. E.g., red$20wine\n\n\nWhen we re-encode the above, we can rebuild our cURL command:\ncurl --location 'https://platform.cloud.coveo.com/rest/search/v2?organizationId=lcboproductionx2kwygnc' \\\n--header 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0' \\\n--header 'Accept: */*' \\\n--header 'Accept-Language: en-CA,en-US;q=0.7,en;q=0.3' \\\n--header 'Accept-Encoding: gzip, deflate, br' \\\n--header 'Authorization: Bearer xx883b5583-07fb-416b-874b-77cce565d927' \\\n--header 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \\\n--header 'Origin: https://www.lcbo.com' \\\n--header 'DNT: 1' \\\n--header 'Connection: keep-alive' \\\n--header 'Referer: https://www.lcbo.com/' \\\n--header 'Sec-Fetch-Dest: empty' \\\n--header 'Sec-Fetch-Mode: cors' \\\n--header 'Sec-Fetch-Site: cross-site' \\\n--data 'locale=en&firstResult='\"$1\"'&numberOfResults='\"$2\"'&excerptLength=2000&sortCriteria=%40ec_price%20'\"$3\"'&facets=%5B%7B%22facetId%22%3A%22%40ec_category%22%2C%22field%22%3A%22ec_category%22%2C%22type%22%3A%22hierarchical%22%2C%22injectionDepth%22%3A1000%2C%22delimitingCharacter%22%3A%22%7C%22%2C%22filterFacetCount%22%3Atrue%2C%22basePath%22%3A%5B%22Products%22%5D%2C%22filterByBasePath%22%3Afalse%2C%22currentValues%22%3A%5B%7B%22value%22%3A%22wine%22%2C%22state%22%3A%22idle%22%2C%22children%22%3A%5B%7B%22value%22%3A%22'\"$4\"'%22%2C%22state%22%3A%22selected%22%2C%22children%22%3A%5B%5D%2C%22retrieveChildren%22%3Afalse%2C%22retrieveCount%22%3A0%7D%5D%2C%22retrieveChildren%22%3Afalse%2C%22retrieveCount%22%3A0%7D%5D%2C%22preventAutoSelect%22%3Atrue%2C%22numberOfValues%22%3A1%2C%22isFieldExpanded%22%3Afalse%7D%5D&facetOptions=%7B%22freezeFacetOrder%22%3Atrue%7D&categoryFacets=%5B%5D&retrieveFirstSentences=true&timezone=America%2FNew_York&enableQuerySyntax=false&enableDuplicateFiltering=false&enableCollaborativeRating=false&debug=false&allowQueriesWithoutKeywords=true' \\\n--output ''\"$5\"'' \nI added the “–output” command and a 5th variable placeholder which represents the output filename. Save the script and run it in a terminal with the following commands:\nchmod +x wine_request.sh\n./wine_request.sh 0 1000 ascending red%20wine redwine.json\nThe arguments trailing the script “./wine_request.sh” are passed to the numbered arguments $1 through $5, respectively. Congratulations! We have downloaded and saved the JSON file as ‘redwine.json’. Better yet, we can download 1,000 products per query!\nThere are non-wine beverages as well (e.g., beer, spirits, cider, icewine). Due to the query structure, we cannot use the same script above for non-wine products. We can create a second script that is nearly identical with the same arguments.\ncurl --location 'https://platform.cloud.coveo.com/rest/search/v2?organizationId=lcboproductionx2kwygnc' \\\n--header 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0' \\\n--header 'Accept: */*' \\\n--header 'Accept-Language: en-CA,en-US;q=0.7,en;q=0.3' \\\n--header 'Accept-Encoding: gzip, deflate, br' \\\n--header 'Authorization: Bearer xx883b5583-07fb-416b-874b-77cce565d927' \\\n--header 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \\\n--header 'Origin: https://www.lcbo.com' \\\n--header 'DNT: 1' \\\n--header 'Connection: keep-alive' \\\n--header 'Referer: https://www.lcbo.com/' \\\n--header 'Sec-Fetch-Dest: empty' \\\n--header 'Sec-Fetch-Mode: cors' \\\n--header 'Sec-Fetch-Site: cross-site' \\\n--data 'locale=en&firstResult='\"$1\"'&numberOfResults='\"$2\"'&excerptLength=2000&sortCriteria=%40ec_price%20'\"$3\"'&facets=%5B%7B%22facetId%22%3A%22%40ec_category%22%2C%22field%22%3A%22ec_category%22%2C%22type%22%3A%22hierarchical%22%2C%22injectionDepth%22%3A1000%2C%22delimitingCharacter%22%3A%22%7C%22%2C%22filterFacetCount%22%3Atrue%2C%22basePath%22%3A%5B%22Products%22%5D%2C%22filterByBasePath%22%3Afalse%2C%22currentValues%22%3A%5B%7B%22value%22%3A%22'\"$4\"'%22%2C%22state%22%3A%22selected%22%2C%22children%22%3A%5B%5D%2C%22retrieveChildren%22%3Afalse%2C%22retrieveCount%22%3A0%7D%5D%2C%22preventAutoSelect%22%3Atrue%2C%22numberOfValues%22%3A1%2C%22isFieldExpanded%22%3Afalse%7D%5D&facetOptions=%7B%22freezeFacetOrder%22%3Atrue%7D&categoryFacets=%5B%5D&retrieveFirstSentences=true&timezone=America%2FNew_York&enableQuerySyntax=false&enableDuplicateFiltering=false&enableCollaborativeRating=false&debug=false&allowQueriesWithoutKeywords=true' \\\n--output ''\"$5\"''  \n\n\nPart 3: Collecting all the data\nOkay… so how do I get all the products?\nWe need to create another script that executes multiple requests. Below, I will describe pieces of a script that we will combine at the end.\n\nA. Run a loop over each category of wine\nI have a text file containing wine categories in each row. The first column is the URL encoded string to pass into the cURL command. The second column is the prefix for the filenames.\n\n\n\n\n\n\ncode/wine_names.txt\n\n\n\n\n\nred%20wine redwine\nwhite%20wine whitewine\nsparkling%20wine sparklingwine\nrose%20wine rosewine\nfortified%20wine fortifiedwine\nchampagne champagne\nsake%20%26%20rice%20wine sakericewine\nspecialty%20wine specialtywine\ngifts%20and%20samplers giftsandsamplers\nicewine icewine\n\n\n\nThe following while loop iterates over each row in the text file above. The first and second columns are represented as variables “htag” and “ftag”. These variables are into a function called “scrape_request_loop()”.\nwhile IFS= read -r htag ftag; do\n    scrape_request_loop $htag $ftag\ndone &lt; ./code/wine_names.txt\n\n\nB. Loop over page number requests\nThe function “scrape_request_loop()” has multiple steps and can be seen in its entirety at the end of this subsection. I first describe the function in chunks before combining the chunks together.\nWe request the first 1,000 products for the specified category with the command below. You can assume the function “request_ascending()” executes the cURL command and saves the JSON file.\n# $1: $htag; wine-category URL substring; e.g., red$20wine\n# $2: $ftag; wine-category filename prefix; e.g., redwine\n# E.g., request_ascending pageNumber numProducts htag ftag\nrequest_ascending 0 1000 $1 $2  \nThe JSON file contains a variable “totalCount” which is the number of products found in the category specified. This is important since it determines whether there are more than 1,000 wines in this category. The following commands read the newly made JSON file (using “jq”) and determines how many more query requests are needed. It then makes these query requests with the for loop. The for loop is skipped if “totalCount” is less than 1000.\ntotalCount=$(jq '.totalCount' $fname) # Read newly made JSON file with jq. Get totalCount.\ncountPer1000=$((totalCount/1000)) # Number of 1000-batches needed.\nmaxPages=$countPer1000 # Maximum number of pages (redundant for now...)\n\nfor pageNumber in $(seq 1000 1000 $(($maxPages*1000)))\ndo\n  request_ascending $pageNumber 1000 $1 $2\ndone\nUnfortunately, the solution above does not work if “totalCount” exceeds 5,000. If you request “pageNumber” 5 and beyond (the index starts at 0), the server will return results for “pageNumber = 4”. There appears to be a hidden limit set by the server. While this is mildly annoying, the simple fix is to reverse the sort order from ascending to descending and query again. Of course, this solution cannot work immediately since there are 13,186 products and only 10,000 will be delivered. This is why I filtered by product category to reduce the maximum results. This solution will also fail for red wines since there are 9,400 product descriptions available. That will not happen any time soon and is a future Stephen problem3.\nWe need to change the “maxPages” definition to take the minimum between (“countPer1000”, 4).\nmaxPages=$(($countPer1000&lt;4?$countPer1000:4)) # Minimum value b/w (countPer1000, 4)\nAfter the for loop executing the function “request_ascending()” is complete, we need to append the following for loop to obtain the remaining solutions:\nif [ $countPer1000 -ge 5 ]; then\n    for pageNumber in $(seq 0 1000 $(($countPer1000*1000-4000)))\n    do\n        request_descending $pageNumber 1000 $1 $2\n    done\nfi\n\n\nC. The entire script\nThe entire script is shown below. After setting permissions (i.e., “chmod +x”), the script starts at the bottom where the while loops begin.\nThe functions “request_ascending()” and “request_descending()” contain the command to execute the cURL scripts.\n#!/bin/bash\nchmod +x code/wine_request.sh\nchmod +x code/nonwine_request.sh\n\nrequest_ascending () {\n    # $1: start index\n    # $2: number of requests\n    # $3: wine-category URL substring; e.g., red$20wine\n    # $4: wine-category filename prefix; e.g., redwine\n    fname=\"json/products/$4.a.$1.json\"\n    sudo $SCRAPE_SH $1 $2 ascending $3 $fname\n    sleep 1\n}\nrequest_descending () {\n    fname=\"json/products/$4.d.$1.json\"\n    sudo $SCRAPE_SH $1 $2 descending $3 $fname\n    sleep 1\n}\n\nscrape_request_loop (){\n    # $1: $htag; wine-category URL substring; e.g., red$20wine\n    # $2: $ftag; wine-category filename prefix; e.g., redwine\n    request_ascending 0 1000 $1 $2\n    totalCount=$(jq '.totalCount' $fname)\n    countPer1000=$((totalCount/1000))\n    maxPages=$(($countPer1000&lt;4?$countPer1000:4))  \n\n    for i in $(seq 1000 1000 $(($maxPages*1000)))\n    do\n        request_ascending $i 1000 $1 $2\n    done\n\n    if [ $countPer1000 -ge 5 ]; then\n        for i in $(seq 0 1000 $(($countPer1000*1000-4000)))\n        do\n            request_descending $i 1000 $1 $2\n        done\n    fi\n\n}\n\nSCRAPE_SH=\"./code/wine_request.sh\"\nwhile IFS= read -r htag ftag; do\n    scrape_request_loop $htag $ftag\ndone &lt; ./code/wine_names.txt\n\nSCRAPE_SH=\"./code/nonwine_request.sh\"\nwhile IFS= read -r htag ftag; do\n    scrape_request_loop $htag $ftag\ndone &lt; ./code/nonwine_names.txt"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons-1",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons-1",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Pros & Cons",
    "text": "Pros & Cons\n\nPros\n\nExtremely fast (1 min!) and no babysitting\nVery cheap\nAutomatable for Github Actions (a big deal)\nGentle on servers\nCan execute more frequently for up to date price changes\nRequests existing products only (unlike the brute-force method)\n\n\n\nCons\n\nRead and manage URL-encoded variables carefully\nTime to understand cURL parameters\nClose attention to hidden limits"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#data-product-summary",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#data-product-summary",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Data product summary",
    "text": "Data product summary\nOnce I have all of the product description data, I can generate a list of SKU numbers that I can query to obtain the live inventory (see Part 2 of this blog series)! There are approximately registered products: 9,400 wine and 4,100 non-wine. “Non-wine” products can include beer, liquor, and reusuable bags. Only 2,800 wines and 2,100 non-wines are purchasable as of 5/14/2023.\nThe files described above are located in the repository: https://github.com/royourboat/lcbo-wine-scraper. Specifically,\ncode/all_product_request.sh\n\ncode/wine_request.sh\n\ncode/nonwine_request.sh\n\ncode/wine_names.txt\n\ncode/nonwine_names.txt"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#footnotes",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#footnotes",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Postman, you can delete lines of code in the Code Snippet panel and see its impact by clicking “Send”. Super convenient.↩︎\nfound through trial and error↩︎\nI can partition the query using different categories.↩︎"
  },
  {
    "objectID": "posts/2023-06-01-lcbo-scraper/index.html",
    "href": "posts/2023-06-01-lcbo-scraper/index.html",
    "title": "Scraping LCBO Data (Part 1: Store Information)",
    "section": "",
    "text": "In Canada, the government of Ontario owns a Crown corporation called the Liquor Control Board of Ontario (LCBO). The LCBO distributes practically all alcoholic drinks in the province of Ontario and is one of the largest purchasers of alcohol in the world1."
  },
  {
    "objectID": "posts/2023-06-01-lcbo-scraper/index.html#data-product-store-information",
    "href": "posts/2023-06-01-lcbo-scraper/index.html#data-product-store-information",
    "title": "Scraping LCBO Data (Part 1: Store Information)",
    "section": "Data Product: Store Information",
    "text": "Data Product: Store Information\n\nPart 1: Finding the data\nEach LCBO store location has a webpage containing its details such as hours, address, and phone number:\n\n\n\n\n\n\nhttps://www.lcbo.com/en/stores/queens-quay-cooper-street-217\n\n\n\n\n\n\n\n\n\nOpen the webpage on your browser: https://www.lcbo.com/en/stores/queens-quay-cooper-street-217\nWhen clicking the above link, you sent a large array of requests for instructions about the website layout, fonts, colours, images, and text. We can monitor this traffic of information and try to find where the store information is. Often times, the store information (or any data) is sent to your browser in the form of a JSON file, which is the case here!\nOn the webpage, do the following :\n\nRight-Click on an empty space\nClick Inspect\nSelect the “Network” tab\nSelect the “XHR” sub-tab (on Chrome it is “Fetch/XHR”)\nReload the page and wait.\n\nYou’ll see a table being filled as data is being sent to your browser.\n\nSort the table by file size (optional)\nClick on the “Response” sub-tab located at the bottom.\n\nHere’s how it looks on my screen so far.\n\n\n\n\n\n\nStore Information (JSON)\n\n\n\n\n\n\n\n\n\nClick a row in the table with JSON file types. You will see the contents of the JSON file beneath the “Response” sub-tab. Keep clicking until you find the store information.\nCongratulations! You’ve found the data. It contains the store address, longitude and latitude, city, area (zip) code, phone number, URL, store hours, and store ID. When I first found the data, I was not expecting the longitude and latitude to be available. So, it was a nice feature that I can use right away for my app. You’ll be surprised what unexpected features there might be when you collect data elsewhere with this technique!\nNow lets grab it.\n\n\nPart 2: Requesting the data\ncURL is a command line tool used to transfer data. It was made by a dev who wanted to automatically fetch currency exchange rates! We can get the cURL command to obtain the data product easily by\n\nRight-clicking on the row with the data product\nSelect “Copy Value”\nSelect “Copy as cURL (Windows)” or (POSIX)\n\n\n\n\n\n\n\ncURL command location\n\n\n\n\n\n\n\n\n\nThe cURL command is now on your clipboard. If you paste the cURL command into an editor, you’ll find a wall of intimidating text. To better understand this, I found Postman incredibly useful. Make a free account, agree to things, and you’ll arrive at a workspace. Click on “Import” in the top-left (see note below).\n\n\n\n\n\n\nPostman - Import button\n\n\n\n\n\n\n\n\n\nA little window will pop up. Paste the cURL command you obtained earlier and Postman will automatically prepare the workspace. Click the blue button “Send” to execute the cURL command. You’ll find the JSON displayed at the bottom. Next, select the “Header” tab and then select the code snippet button “&lt;/&gt;” on the very right margin. See image below.\n\n\n\n\n\n\nPostman - Import button\n\n\n\n\n\n\n\n\n\nEach row in the “Header” tab is an argument in the cURL command. If you uncheck an option, the cURL command will update on the right. Most of these arguments are not important for us. You can uncheck them and click “Send” again to see whether the JSON product is delivered. I found the bare minimum needed is “Content-type” and “X-Requested-With”. So, we should include them.\nLooking at the cURL command (see my reduced version below), you’ll find an argument that requests the specific store corresponding to “Queen’s Quay”.\ncurl --location 'https://www.lcbo.com/en/storepickup/selection/store/' \\\n--header 'Accept-Encoding: gzip, deflate, br' \\\n--header 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \\\n--header 'X-Requested-With: XMLHttpRequest' \\\n--data 'value=217&st_loc_flag=true'\nCopy your cURL command and paste it into a notepad/editor. Change the value from 217 to 218. Copy the new command, click “Import” again in Postman, paste the command, and click “Send”. You’ll obtain a different store location’s details corresponding to 218.\nNow we know what to change to start collecting all store information.\n\n\nPart 3: Collecting all the data\nOur next goal is to write a bash script that will loop cURL command for all values. My script is below. To test this, you’ll need to save it as a file with a “.sh” file tag (e.g., scriptname.sh) and access to a bash terminal. If you’re using a linux or mac then just pop a terminal open, go to the folder with your script, and type the following:\nchmod +x scriptname.sh\n./scriptname.sh\nThere are a lot of solutions for Windows systems. Since Ubuntu was my primary OS for a while, I prefer just installing WSL (or WSL2).\n#!/bin/bash\nfor STORE_ID in {1..800}\ndo\n   curl --location 'https://www.lcbo.com/en/storepickup/selection/store/' \\\n   --header 'content-type: application/x-www-form-urlencoded; charset=UTF-8' \\\n   --header 'x-newrelic-id: VwQHU1dQCRAJU1NUAgMEUFQ=' \\\n   --header 'x-requested-with: XMLHttpRequest' \\\n   --data 'value='$STORE_ID'&st_loc_flag=true' \\\n   --output 'json/stores/store_'$STORE_ID'.json'\n   sleep 10\ndone\nHere are explanations for the bash script above.\n\nInstructs the operating system we are using bash.\n\n#!/bin/bash\n\nA for loop where the variable STORE_ID takes values between 1 and 800. After some trial and error, I did not find successful returns above 800. This number is consistent with there being nearly 700 LCBO stores.\n\nfor STORE_ID in {1..800}\ndo\n  ...\ndone\n\nI use the for loop variable here to specify the store ID. Note, if there is no data for the store ID then the return is a JSON containing {“success”: false}.\n\n   --data 'value='$STORE_ID'&st_loc_flag=true'\n\nThis argument stores the output as a file with the filename in quotations.\n\n   --output 'json/stores/store_'$STORE_ID'.json'\n\nThis is incredibly important. This is a sleep command of 10 seconds. If you do not have a delay, the for loop will send 800 requests nearly instantaneously. This can place tremendous load on the servers and cause delays or crashes for customers using the website. What will likely happen is that your IP address will be temporarily banned after the 50th or so request. In my opinion, you deserve this ban. We do not want to harass freely accessible websites!\n\nsleep 10\n\n\n\n\n\n\nScrape gently\n\n\n\nAlways scrape gently in testing and final production. If you don’t, they can easily make scraping much harder and more annoying."
  },
  {
    "objectID": "posts/2023-06-01-lcbo-scraper/index.html#footnotes",
    "href": "posts/2023-06-01-lcbo-scraper/index.html#footnotes",
    "title": "Scraping LCBO Data (Part 1: Store Information)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA little known fact about Ontario is that it is full of monopolistic business practices.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Me persuading this whip to crack during a public talk."
  },
  {
    "objectID": "about.html#academia",
    "href": "about.html#academia",
    "title": "About",
    "section": "Academia",
    "text": "Academia\nMy professional background is in math, physics, and astrophysics. Here are some projects I have worked on with truly amazing people:\n\nAnalyzed1 the optical performance of cryogenic light guides for a dark matter detector2.\nTested optical shutters3 and measured proton beam alignments4 for a particle accelerator in Japan. It shoots neutrinos under Japan and is aimed at an underground neutrino detector, Super-Kamiokande.\nSimulated galaxies merging with galaxy-groups with FLASH: a FORTRAN(!) hydro code.\nWrote Galino5: A 3D spectrographic image synthesizer that generates realistic datacubes (X, Y, wavelength) of line emission from distant galaxies. Includes realistic statistical properties from telescopes, instruments, and radiation physics.\nSolved for a lot of eigenvalue-solutions describing self-similar shocks sweeping through an atmosphere6.\nSolved7 the structures of stars with extremely dense winds. I did this to simulate how a supernova explosion could look differently.\nMy favourite: A mantis shrimp video inspired me to adapt the physics of sonoluminescence8 to describe shockwaves in stars9.\nSimulated how failed supernovae with a central black hole can still explode10.\n\n\nEducation\nBScH in Mathematical Physics | Queen’s University  PhD in Astrophysics | University of Toronto  Postdoc | Theoretical Astrophysics Center | UC Berkeley\n\n\nPapers"
  },
  {
    "objectID": "about.html#teaching-and-outreach",
    "href": "about.html#teaching-and-outreach",
    "title": "About",
    "section": "Teaching and Outreach",
    "text": "Teaching and Outreach\nI love to teach! This passion was lit in graduate school. Since then, I designed an undergraduate python course how I wished science and coding were taught to me. I then trained to become a highschool math and physics teacher in Ontario, Canada. This was an immensely rewarding path that I enjoyed thoroughly and miss. The decision to place my teaching path on hold11 was quite an emotional and challenging one. But, my passion to teach continues and will appear in all my presentations and talks to come."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nusing lasers and exorbitant amounts of liquid nitrogen↩︎\ncryogenic acrylic chamber housing 3600 kg liquid argon in a mine 2 km underground. The earth is really warm down there! DEAP3600.↩︎\nscreens that show the numbers on your childhood calculator (i.e., twisted nematic screens). Use these as remotely operated digital shutters for a camera in an irradiated area. Schematic↩︎\nand convincing researchers to not over-interpret the shoulders of gaussian fits to 5 data points.↩︎\nDistant galaxies are seen at earlier times during the “galactic bombardment phase” when galaxies are being smashed and cannibalizing smaller galaxies. It is a messy phase and a telescopic image will show a bunch of pixelated blobs. Hence, “Galino” = “little galaxy … blobs”. ↩︎\nA close analogy is a wave traveling down a whip and studying how it “cracks”. Except, the wave is replaced with a shockwave and the whip is replaced with an atmosphere.↩︎\nWrote a relaxation algorithm to solve non-linear equations describing a star with a windy surface. Proudly integrated unwieldly FORTRAN code (OPAL) to include realistic opacity calculations.↩︎\nSonoluminescence. Sono = sound; luminescence = light. If you blast two ultrasonic soundwaves at each other underwater, the constuctive interference can become non-linear and rip open a cavity. The underwater cavity is like a hole with no pressure support (i.e., it’s not a bubble) and so it implodes. The implosion of a spherical wall of water meeting at a singularity generates light with spectral characteristics indicating peak emission temperatures of 5,000 to 150,000 K. The sun’s surface is about 6,000 K…↩︎\nI got goosebumps from seeing my exploding star simulation be predicted perfectly by solutions for underwater implosions, a seemingly unrelated problem. ↩︎\nImagine if the earth’s mass suddenly decreased by 10%. You’d immediately feel lighter because gravity has suddenly decreased. This happens in a massive star (maybe 0.5% of mass rather than 10%) moments before it goes supernova. The sudden drop in gravity causes the outer envelope of gas to expand non-uniformly and non-linearly to form a shockwave that might breakout to the surface of the star. You’d never see this smaller shockwave because it’d be overrun by the bigger supernova shockwave coming from the core! This scenario was thought about because astronomers found a red supergiant to have disappeared without any explosion of light… indicating supernova can fail…?! (Also, the mass vanishes suddenly because most of the supernova’s energy is lost in the form of neutrinos. These neutrinos don’t interact with anything and can zoom out of the star at the speed of light unimpeded.)↩︎\nfor many, many reasons including the state of education and infrastructure, culture, policies, and politics, which I’m happy to talk about↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Ro  Ph.D.",
    "section": "",
    "text": "Stephen Ro  Ph.D.\nTrained in scientific computing, mathematical physics, and astrophysics, Stephen thrives on making sense of complex systems with research, data, and collaboration. He brings more than a decade of programming, project, and leadership experience into data science.\nStephen is passionate about teaching and learning. He has taught python at UC Berkeley, given numerous talks on astrophysics (for all ages!), and designed a collaborative data-driven astronomy project for high school math students."
  },
  {
    "objectID": "posts/2023-06-10-lcbo-scraper/index.html",
    "href": "posts/2023-06-10-lcbo-scraper/index.html",
    "title": "Scraping LCBO Data (Part 2: Product Inventory)",
    "section": "",
    "text": "A fundamental feature of my wine app is to only recommend bottles available right now at a store nearby.\nLCBO’s product inventory data is live and readily accessible for any given product given the unique SKU number (a 6 digit code). For instance, SKU number 328534 leads you to the inventory page for Jackson-Triggs Cabernet Sauvignon using the URL https://www.lcbo.com/en/storeinventory/?sku=328534. This URL is easy to query considering we only have to change one parameter, assuming we have a list of LCBO’s SKU numbers (see Part 3 of this series for how I got the numbers)."
  },
  {
    "objectID": "posts/2023-06-10-lcbo-scraper/index.html#bash-sed-and-regex",
    "href": "posts/2023-06-10-lcbo-scraper/index.html#bash-sed-and-regex",
    "title": "Scraping LCBO Data (Part 2: Product Inventory)",
    "section": "Bash, sed, and regex",
    "text": "Bash, sed, and regex\nThe cURL command is fairly simple:\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1\nwhere the variable “1” will be substituted by the SKU number. I need to process the HTML code delivered before we can extract the JSON. This is done using “pipes” represented by the character “|” without quotes. For example, we can add a subsequent command after cURL called “grep” using a pipe as follows:\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| tac | tac \\\n| grep -m 1 'storeList'\nThe backslash “\\” lets you continue the command on a new line. “grep” lets you search files or, in this case, HTML text piped from the cURL command. The option “-m 1” is equivalent to “–max-count=1” which says to stop searching after the first detection of the pattern ‘storeList’.\nA funny thing happens if we do not include the two piped “tac” commands before grep. The tac command simply reverses the incoming file. Executing two tac commands will return the original cURL output! Read @Kaworu’s for an explanation why this is the case.\nThe sequence of piped commands will return line 4004, which looks like this:\n\"storeList\":[[\"City\",\"Intersection\",\"Address Line 1\",\"Address Line 2\",\"Phone Number\",\"Store Number\",\"Available Inventory\"],[\"St. Catharines\",\"Vansickle & Fourth\",\"420 Vansickle Road\",\"\",\"(905) 685-8000\",\"392\",\"1\"],[\"Kitchener\",\"Fairway & Manitou\",\"655 Fairway Road South\",\"\",\"(519) 894-0710\",\"381\",\"1\"],[\"Lasalle\",\"Malden Rd & Elmdale Ave\",\"1825 Wyoming Avenue\",\"\",\"(519) 250-9847\",\"370\",\"2\"],[\"Toronto-North York\",\"Wilson & Dufferin\",\"675 Wilson Avenue\",\"\",\"(416) 636-5349\",\"360\",\"4\"],[\"Toronto-North York\",\"Bayview & Sheppard (Bayview Village)\",\"2901 Bayview Avenue - Unit 125\",\"BAYVIEW VILLAGE MALL\",\"(416) 222-7658\",\"355\",\"3\"],[\"Dundas\",\"Osler & Main W\",\"119 Osler Drive\",\"UNIT 13A\",\"(905) 628-2042\",\"25\",\"4\"],[\"Toronto-Central\",\"Dupont & Spadina\",\"232 Dupont Street\",\"\",\"(416) 922-7066\",\"15\",\"3\"],[\"Ancaster\",\"Golf Links & Hwy 403 (Meadowlands)\",\"737 Golf Links Road\",\"MEADOWLANDS CENTRE\",\"(905) 304-9608\",\"21\",\"1\"],[\"Burlington\",\"Appleby Line & New St\",\"501 Appleby Line\",\"\",\"(905) 639-0337\",\"641\",\"1\"],[\"Aurora\",\"Leslie & Wellington\",\"94 First Commerce Drive\",\"\",\"(905) 751-0684\",\"658\",\"4\"],[\"Mississauga\",\"Dundas & Winston Churchill (Woodchester)\",\"2458 Dundas Street West\",\"WOODCHESTER MALL\",\"(905) 822-1776\",\"494\",\"1\"],[\"Oakville\",\"Cornwall & Trafalgar\",\"321 Cornwall Road\",\"UNIT C120\",\"(905) 845-8100\",\"486\",\"1\"],[\"Windsor\",\"Tecumseh & Lauzon\",\"7640 Tecumseh Rd. E.\",\"\",\"(519) 944-4014\",\"490\",\"1\"],[\"Toronto-North York\",\"Avenue & Lawrence\",\"1838-1844 Avenue Road\",\"\",\"(416) 785-6389\",\"452\",\"1\"],[\"Newmarket\",\"Leslie & Davis\",\"17365 Leslie Street\",\"\",\"(905) 898-1062\",\"453\",\"1\"],[\"Markham\",\"Hwy 7 & Woodbine\",\"3075 Highway 7 East\",\"\",\"(905) 940-2768\",\"590\",\"9\"],[\"Markham\",\"Hwy 7 & Birchmount\\/village Pkwy\",\"3991 Highway 7\",\"\",\"(905) 479-9612\",\"580\",\"3\"],[\"Markham\",\"Markham Rd & Bur Oak\",\"9720 Markham Road\",\"\",\"(905) 201-9743\",\"585\",\"1\"],[\"Burlington\",\"Walkers Line & Dundas St\",\"3041 Walkers Line\",\"\",\"(905) 331-5792\",\"551\",\"1\"],[\"Kitchener\",\"Highland & Westmount (Highland Rd Plaza)\",\"324 Highland Road West Unit 6\",\"HIGHLAND ROAD PLAZA\",\"(519) 745-8781\",\"549\",\"1\"],[\"Richmond Hill\",\"Yonge & Hwy 7\",\"8783 Yonge Street\",\"\",\"(905) 886-3511\",\"623\",\"5\"],[\"Mississauga\",\"Erin Mills & Eglinton\",\"5100 Erin Mills Parkway\",\"SUITE 5035\",\"(905) 607-7900\",\"619\",\"3\"],[\"Innisfil\",\"Innisfil Beach Rd & 20th Sr\",\"1465 Innisfil Beach Road\",\"UNIT #7\",\"(705) 436-7182\",\"605\",\"1\"],[\"Pickering\",\"Brock & Kingston (Hwy 2)\",\"1899 Brock Rd Unit K3\",\"\",\"(905) 427-9830\",\"776\",\"1\"],[\"Woodbridge\",\"Hwy 27 & Innovation Dr\",\"8260 Hwy. 27\",\"\",\"(905) 264-7366\",\"632\",\"4\"],[\"Richmond Hill\",\"Major Mackenzie & Leslie\",\"1520 Major Mackenzie Dr. E.,\",\"\",\"(905) 884-4700\",\"629\",\"3\"],[\"Milton\",\"Main St & Thompson Rd\",\"830 Main Street East\",\"\",\"(905) 864-7030\",\"631\",\"1\"],[\"King City\",\"King Rd & Dufferin St\",\"1700 King Road\",\"UNIT #55 BUILDING B\",\"(905) 833-0641\",\"671\",\"1\"],[\"Toronto-Central\",\"Queens Quay & Cooper Street\",\"15 Cooper Street\",\"\",\"(416) 864-6777\",\"217\",\"1\"],[\"Aurora\",\"Yonge & Henderson\",\"1 Henderson Drive\",\"\",\"(905) 727-9722\",\"311\",\"1\"],[\"Toronto-Central\",\"Eglinton & Laird\",\"65 Wicksteed Avenue\",\"\",\"(416) 425-6282\",\"164\",\"3\"],[\"Tecumseh\",\"E.c. Row & Manning\",\"15 Amy Croft Drive\",\"\",\"(519) 735-2661\",\"278\",\"1\"],[\"Toronto-North York\",\"Keele & Lawrence (North Park Plaza)\",\"1339 Lawrence Ave. West\",\"NORTH PARK PLAZA\",\"(416) 249-1391\",\"279\",\"1\"],[\"Toronto-North York\",\"Lawrence & Don Mills\",\"195 The Donway West\",\"\",\"(416) 447-0491\",\"253\",\"1\"],[\"Ottawa\",\"Bank & Walkley\",\"1980 Bank Street\",\"\",\"(613) 523-7763\",\"243\",\"1\"],[\"Newmarket\",\"Upper Canada Mall\",\"17600 Yonge Street\",\"\",\"(905) 895-6341\",\"226\",\"3\"],[\"Waterdown\",\"Hwy 5 & Hwy 6\",\"74 Dundas Street East\",\"\",\"(905) 689-6855\",\"326\",\"2\"],[\"Oakville\",\"3rd Line & Rebecca (Hopedale Mall)\",\"1527 Rebecca Street\",\"\",\"(905) 827-5072\",\"437\",\"1\"],[\"Waterloo\",\"King N & Northfield\",\"571 King Street North\",\"\",\"(519) 884-8511\",\"417\",\"1\"],[\"Vaughan\",\"Major Mackenzie & Weston\",\"3631 Major Mackenzie Drive\",\"BUILDING B\",\"(905) 417-1102\",\"397\",\"3\"],[\"Niagara Falls\",\"Portage Rd & Colborne E\",\"3714 Portage Road\",\"\",\"(905) 356-3972\",\"401\",\"1\"],[\"Stouffville\",\"Main St & Mostar St\",\"5710 Main Street Unit 1\",\"\",\"(905) 640-3771\",\"404\",\"1\"]],\nWe could take the above output and, with a little editing, immediately create a JSON. A small snapshot of the JSON is seen below.\n\nThere are a number of practical reasons to not do this. For instance, the only data we require is the store number and available inventory. We know the store information from Part 1 of our series and can call upon it with the store number alone. Throwing away redundant data reduces the file sizes and decreases the read time of each file. Furthermore, the JSON structure can be simpler and much more readable with restructuring! Our goal is to rebuild the JSON to look like this:\n\n94433 is the SKU number. The keys and values are simply the store numbers and available inventory, respectively.\n\nsed\n“sed” is a UNIX stream editor that lets you search, and find and replace. There is a neat online editor that lets you quickly test your ideas: https://sed.js.org/.\nHere is sed piped into action:\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| grep -m 1 'storeList' \\ \n| tac | tac \\\n| sed '$s/,$//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\nOkay… sed is an alien language when you first see it. Let me break it down. To start, the output from grep is piped to sed for processing. The quoted text after sed is actually several search and replace commands concatenated together. The following text\n's/,$//; s/\"storeList\":\\[//g; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\nbreaks down into 4 sed commands separated by semicolons:\n1. 's/,$//'\n2. 's/\\]\\]//g'\n3. 's/\\],\\[/\\n/g'\n4. '/^ $/d'\nAn sed command has four components separated by forward slashes “/”:\n'[s]ubstitute / Regexp / Replacement / [g]lobal'\nThe substitute and global parameters are represented with s and g. Regexp is the regular expression you’re searching for. Replacement is the text that will substitute the regex expression. Below is an explanation for each command. You can follow along by pasting the output the contents of line 4004 to https://sed.js.org/ and applying the commands below.\n\nSubstitute the last comma at the end of the grep output with nothing.\n\n# ,$  &lt;-- the dollar sign $ says to get the last comma\n's/,$//'\n\nGlobally substitute the double closing square brackets with nothing.\n\n\n# Substitute ]] with nothing (i.e., delete ]])\n's/\\]\\]//g'\n\nGlobally substitute inner square brackets with a newline.\n\n# Substitute ],[ with \\n\n's/\\],\\[/\\n/g'\n\nDelete lines with only white spaces or newlines.\n\n'/^ $/d'\nHere is a sample output of the first three lines after running these sed commands:\n\"storeList\":[[\"City\",\"Intersection\",\"Address Line 1\",\"Address Line 2\",\"Phone Number\",\"Store Number\",\"Available Inventory\"\n\"St. Catharines\",\"Vansickle & Fourth\",\"420 Vansickle Road\",\"\",\"(905) 685-8000\",\"392\",\"1\"\n\"Kitchener\",\"Fairway & Manitou\",\"655 Fairway Road South\",\"\",\"(519) 894-0710\",\"381\",\"1\"\n...\n\n\ntail\nThe “tail” command shows the last few lines of a given file. We pipe the output to “tail” and add a +2 option to return the entire text from line 2 onwards (i.e., delete the first line).\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| grep -m 1 'storeList' \\ \n| tac | tac \\\n| sed '$s/,$//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\\\n| tail +2\nHere is the current state of our text file:\n\"St. Catharines\",\"Vansickle & Fourth\",\"420 Vansickle Road\",\"\",\"(905) 685-8000\",\"392\",\"1\"\n\"Kitchener\",\"Fairway & Manitou\",\"655 Fairway Road South\",\"\",\"(519) 894-0710\",\"381\",\"1\"\n...\n\n\nMore sed!\nOur next sed command will take the output and produce the JSON file we desire! Last sed, I promise. Note that we use the -r option with sed to permit extended regular expressions.\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| grep -m 1 'storeList' \\ \n| tac | tac \\\n| sed '$s/,$//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\\\n| tail +2 \\\n| sed -r 's/(.*,)([^,]+,[^,]+)$/\\2/g; s/\"*$//g; s/\",\"/\":/g ; 1s/^/{\"'$1'\":{/; s/$/,/ ; $s/,$/}}/' \n\nGrab the last two columns from each row.\n\ns/(.*,)([^,]+,[^,]+)$/\\2/g;\nReturn:\n\"392\",\"1\"\n\"381\",\"1\"\n...\n\nDelete the last quotation mark in each row.\n\ns/\"*$//g;\nReturn:\n\"392\",\"1\n\"381\",\"1\n...\n\nGlobally replace the regexp (^^^) with the replacement (^^). Sorry, it gets weird referencing punctuations. I’ve added some white spaces for clarity.\n\ns/   \",\"   /    \":     /g\n     ^^^        ^^\nReturn:\n\"392\":1\n\"381\":1\n...\n\nThe start of a JSON needs an open curly bracket. Append the following text with the SKU number to the beginning: {“94433”: {\n\n1s/^/{\"'$1'\":{/; \nReturn:\n{\"94433\":{\"392\":1\n\"381\":1\n...\n\nAdd a comma at the end of each row:\n\ns/$/,/\nReturn:\n{\"94433\":{\"392\":1,\n\"381\":1,\n...\n\nAdd closing brackets at the end of the file to complete the JSON.\n\n$s/,$/}}/\nReturn:\n{\"94433\":{\"392\":1,\n\"381\":1,\n...\n\"404\":1}}\n\n\nMake it pretty with jq and save!\nThe jq command reformats the JSON to look pretty. Finally, we save the final output to a JSON file.\n#!/bin/bash\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\\n| grep -m 1 'storeList' \\\n| tac | tac \\\n| sed '$s/,$//; s/\"storeList\":\\[//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^ $/d' \\\n| tail +2  \\\n| sed -r 's/(.*,)([^,]+,[^,]+)$/\\2/g; s/\"*$//g; s/\",\"/\":/g ; 1s/^/{\"'$1'\":{/; s/$/,/ ; $s/,$/}}/' \\\n| jq . \\\n&gt; json/inventory/$1.json"
  },
  {
    "objectID": "posts/2023-06-10-lcbo-scraper/index.html#loop-the-script-for-each-sku-number",
    "href": "posts/2023-06-10-lcbo-scraper/index.html#loop-the-script-for-each-sku-number",
    "title": "Scraping LCBO Data (Part 2: Product Inventory)",
    "section": "Loop the script for each SKU number:",
    "text": "Loop the script for each SKU number:\nIn Part 3, I obtain a list of SKU numbers for wines and non-wine beverages. In the script below, the script runs a loop for each SKU number in the file and executes the script made above. The “chmod” command changes permissions to permit write access. We add a sleep command of 1 second to scrape more gently.\n#!/bin/bash\nchmod +x code/inventory_request.sh\n\nwhile IFS= read -r line; do\n    sudo ./code/inventory_request.sh $line\n    sleep 1\ndone &lt; json/wine_sku_list.txt\n\nwhile IFS= read -r line; do\n    sudo ./code/inventory_request.sh $line\n    sleep 1\ndone &lt; json/nonwine_sku_list.txt"
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#what-is-github-actions",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#what-is-github-actions",
    "title": "Making my own scraper bot!",
    "section": "What is GitHub Actions?",
    "text": "What is GitHub Actions?\nGitHub Actions is a platform to launch “workflows” with your GitHub repository. These workflows can be automated to build, test, and deploy your code directly from GitHub. Workflows can have create numerous jobs that run in parallel. They can also create temporary branches for testing your build’s functionality and performance.\n\n\n\n\n\nThank you, GitHub!\n\n\nThere are great guides and documentation on GitHub Actions."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-strategy",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-strategy",
    "title": "Making my own scraper bot!",
    "section": "The strategy",
    "text": "The strategy\nIn my three-part blog series, I describe three scrapers I wrote for the LCBO website. These scrapers obtain several thousands of data products as JSON files. I have one workflow executing each scraper along with a python script for post-processing. Each workflow performs the following sequence of actions (more or less in the same order):\n\nWait for an event to trigger the workflow.\nCreate a new branch.\nGo to the new branch.\nRun the scraping script.\nInstall python modules and do python post-processing.\nCommit and push changes to the new branch.\nGo to the main branch.\nPull new branch and then delete it.\n\nCreating a temporary branch is very helpful when you are first starting to experiment with workflows. Your workflows will fail. Scripts will stop working. A temporary branch lets you run your script and inspect the results without threatening your main branch. I suggest ignoring steps 7 and 8 from the above sequence until you are confident with your workflow.\n\n\n\nSimple diagram of my workflow\n\n\n\nWorkflow solution\nBelow is the strategy written as a workflow. The workflow is sequential, and each sequence is described in the rest of the blog.\n\n\n\n\n\nYou can flatten the code.\n\n\n\n\n\n\n\n\nworkflow solution: .github/workflows/workflow_products.yml\n\n\n\n\n\nname: Product Description Scraper\n\non:\n  push:\n  workflow_dispatch:\n  schedule:\n    - cron:  '7 7,19 * * 0'\n\njobs:\n  my-job:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Check out this repo\n      uses: actions/checkout@v3\n\n    - name: Create branch\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git checkout -b workflow_products_output_singlejob\n        git push -u origin workflow_products_output_singlejob\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: workflow_products_output_singlejob\n\n    - name: Fetch latest data.\n      run: |-\n          chmod +x code/all_product_request.sh\n          sudo ./code/all_product_request.sh\n      shell: bash\n\n    - name: Install python modules. Upload to Autovino server.\n      env: \n        AUTOVINO_KEY: ${{secrets.AUTOVINO_KEY}}\n      run: |-\n        python -m pip install --upgrade pip\n        pip install -r code/requirements/requirements.txt\n        python code/create_master_json_and_skulist_txt.py\n        python code/updateTables.py prices \n      shell: bash\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git branch\n        git pull \n        git add json/*\n        timestamp=$(date -u)\n        git commit -m \"Latest data: ${timestamp}\" || exit 0\n        git push\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: main\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git config pull.rebase true\n        git pull --allow-unrelated-histories origin workflow_products_output_singlejob \n        git push origin main\n        git push origin --delete workflow_products_output_singlejob\n\n\n\n\n\n\n\n\n\nWarning 1: Workflow experimentation\n\n\n\n\n\nWorkflows can have a steep learning curve! I strongly encourage workflow experimentation and debugging to be done in a separate branch and not in main.\n\n\n\n\n\n\n\n\n\nWarning 2: Change read/write permissions\n\n\n\n\n\nMy workflow will crash because GitHub Actions does not have read/write permissions by default. Go to Settings &gt; Actions-General &gt; Workflow permissions, select Read and write permissions, select Allow GitHub Actions to create and approve pull requests, and save."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-yaml",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-yaml",
    "title": "Making my own scraper bot!",
    "section": "The YAML",
    "text": "The YAML\nI suggest creating a new private github repository to follow along. You can save the upcoming workflow snippets to your workflows directory and follow along. Workflows must must exist in this directory:\n.github/workflows/\n\nName and events (Part 1)\nWorkflows are written in YAML (*yml), begin with a name (e.g., “Product Description Scraper”), and a list of triggers:\n\n\n\n\n\n\nworkflow snippet: names and events\n\n\n\n\n\nname: Product Description Scraper\n\non:\n  push:\n  workflow_dispatch:\n  schedule:\n    - cron:  '7 7,19 * * 0'\n\n\n\nThis workflow is triggered when one of three events types occur:\n\npush: a push to the branch has been made.\nworkflow_dispatch: a manually activated trigger (see below).\nschedule: a scheduled event occurs 1.\n\nSee this list of all other event types.\nAfter saving the above workflow to your repository, go to the Actions tab. There, you can monitor your workflow. Any YAML files in your workflow directory will be displayed in this tab.\n\nUnfortunately, the workflow is not executable because it is incomplete. The YAML file is shown by its filename rather than the workflow name. We need to provide a job with at least one step. Append the following code, save, and refresh the workflow monitor.\n\n\n\n\n\n\nworkflow snippet: jobs\n\n\n\n\n\njobs:\n  my-job:\n    runs-on: ubuntu-latest #choose an OS\n    steps:\n    - name: Check out this repo\n      uses: actions/checkout@v3\n\n\n\nYou should see the label change from your filename to the name “Product Description Scraper”. \nA Run workflow button is available because the workflow_dispatch event is included as a trigger. This button does not display by default.\n\n\n\n\n\n\nworkflow_dispatch button\n\n\n\n\n\n\n\n\n\nTry clicking on the Run workflow trigger! I will talk more the display below.\n\n\nJobs (Part 2)\nWorkflows execute jobs. Jobs are sent to a queue and are picked up by “runners”. Runners are machines that will execute your job when not busy. Let’s look at the first few steps in my workflow’s job called my-job.\n\n\n\n\n\n\nworkflow snippet: jobs\n\n\n\n\n\njobs:\n  my-job:\n    runs-on: ubuntu-latest #choose an OS\n    steps:\n    - name: Check out this repo\n      uses: actions/checkout@v3\n\n\n\nHere, I specify needing a runner using ubuntu-latest (i.e., Ubuntu 22.04). A list of OSs can be found here. Runners using Windows and macOS cost 2x and 10x more computing minutes than Linux (ref)!\nA job contains a sequence of tasks called steps. Each step has a label called name. My first step uses a pre-built action called CheckoutV3 which “checks out” my respository, specifically the main branch by default. This lets me access and do things with the repository. This action is available in a GitHub Actions Marketplace where authors host free or paid actions for others to use in their workflow!\nI need CheckoutV3 so I can duplicate the main branch. This is done in the step below.\n\n\nCreate a branch and web scrape (Part 3)\n\n\n\n\n\n\nworkflow snippet: create a branch\n\n\n\n\n\n    - name: Create branch\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git checkout -b workflow_products_output_singlejob\n        git push -u origin workflow_products_output_singlejob\n\n\n\nThe first two commands git config specify the username and email, so git can report who is messing around and how to contact them. Next, I create a new branch with git checkout -b and name the branch “workflow_products_output_singlejob”. I then upload the repository I am “checking out” (main) to the newly created one (workflow_products_output_singlejob) using git push. The following -u origin workflow_products_output_singlejob option lets me later use git pull without ambiguity.\nI use CheckoutV3 again to check out the new branch I made. In this new branch, I execute the scraping script to collect new data products.\n\n\n\n\n\n\nworkflow snippet: check out the new branch\n\n\n\n\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: workflow_products_output_singlejob\n\n    - name: Fetch latest data.\n      run: |-\n          chmod +x code/all_product_request.sh\n          sudo ./code/all_product_request.sh\n      shell: bash\n\n\n\n\n\n\n\n\n\nMinimize work\n\n\n\n\n\nThe goal is to create a functioning workflow first. Do not start scraping the entire catalog while constructing and debugging your workflow. Try scraping a couple products max.\n\n\n\n\n\nGet my secret key and do python (Part 4)\nI need to process the raw data products before uploading them to my postgreSQL data base. I use a python script which needs the modules listed in requirements.txt. The first python script processes the raw JSONS and combines them into a single JSON with new features. The second script updates tables in my postgreSQL database.\n\n\n\n\n\n\nworkflow snippet: python and postgreSQL\n\n\n\n\n\n    - name: Install python modules. Upload to Autovino server.\n      env: \n        AUTOVINO_KEY: ${{secrets.AUTOVINO_KEY}}\n      run: |-\n        python -m pip install --upgrade pip\n        pip install -r code/requirements/requirements.txt\n        python code/create_master_json_and_skulist_txt.py\n        python code/updateTables.py prices \n      shell: bash\n\n\n\nThe external URL for my postgreSQL database is protected! GitHub lets you create secret keys that is hidden from the public and accessible to your environment. I define an environment variable AUTOVINO_KEY to be my secret key secrets.AUTOVINO_KEY with my database’s external URL. This key is read in my python script using\nAUTOVINO_KEY = os.environ['AUTOVINO_KEY']\n\n\nWrap it up (Part 5)\nThe remaining steps are straightforward. I add the json folder containing the data files, commit, and push to the new branch. I switch to (or “check out”) the main branch and pull the new branch before deleting it.\n\n\n\n\n\n\nworkflow snippet: commit and push, pull and delete new branch\n\n\n\n\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git branch\n        git pull \n        git add json/*\n        timestamp=$(date -u)\n        git commit -m \"Latest data: ${timestamp}\" || exit 0\n        git push\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: main\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git config pull.rebase true\n        git pull --allow-unrelated-histories origin workflow_products_output_singlejob \n        git push origin main\n        git push origin --delete workflow_products_output_singlejob\n\n\n\nThat’s it! That’s the workflow behind my scraper bot.\n\nTips:\n\nIf a workflow fails after a new branch is created, the new branch will continue to exist. You must delete this branch before running another workflow.\nIf you do NOT want to save the raw or processed data (because they are uploaded to your database) then you simply delete the branch after you have what you need."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#workflow-monitor",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#workflow-monitor",
    "title": "Making my own scraper bot!",
    "section": "Workflow Monitor",
    "text": "Workflow Monitor\nThe monitor is vital to debugging your workflow. Try dispatching the workflow now. You can see it added to queue under your Actions tab. The orange circle indicates the workflow is in queue or running.\n\nWhen a workflow fails, the indicator becomes a crossed red circle. You can setup notifications settings to receive an email when this occurs in case things fail unexpectedly.\n\nWe can inspect the failure by clicking on the workflow to show a GUI diagram of the jobs requested. Here, “job_0” is my-job in the workflow script. The GUI can have a graph of connecting jobs and lets you zoom in and out. My script only has one job!\n\nClicking on the job shows a list of steps with names we defined using name. We can inspect each step and see whether they performed as desired. The failure here is “remote: Write access to repository not granted.” This is because I skipped my warning about giving GitHub Actions read and write permissions. As a result, a new branch was not created and the workflow shut down."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-matrix",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-matrix",
    "title": "Making my own scraper bot!",
    "section": "The Matrix",
    "text": "The Matrix\nMy original ambition for my workflow was to make it fast. Create dozens of jobs. Execute a cluster of scrape commands for each job. Go forth, my minions! Pillage the servers!\n\n\n\nAzog and his job requests. Source: The Hobbit: The Battle of the Five Armies.\n\n\nNot only was this strategy greedy and unnecessary, it took spectacular effort and cost 50x more minutes and computational resources! Here is why.\nIn GitHub Actions, you can execute a multitude of identical jobs where each job has a different parameter or initial value. These parameters come from a “matrix” (i.e., a list) of values. In my case, I wanted to assign a scraping job for each parameter in my matrix2. Feel free to read more about matrices here. You can specify the maximum number of jobs that run simultaneously too (ref).\nAfter experimenting with the matrix strategy, it was clearly a very bad solution for several reasons:\n\nDefining a matrix that is more than one dimension causes hair loss.\nConstantly referring to these as matrices hurts me.\nIf you have a list of unequal lists to query then too bad. A matrix must be a list of equal length lists. Elements that do not fit in the matrix must bad added as individual special scenarios. smile\nThe minutes used for each job is rounded up and summed. That means 10 jobs that take 1 second each will cost 10 minutes. Meanwhile, 1 job that performs ten 1-second tasks will cost 1 minute.\n\n\n\n\nThe last reason was a complete shock to me. With all my scrapers, I could only afford one or two scrapings per month each! (You can see your usage in the workflows monitor or your GitHub settings under “Billing and plans”.)\nI rewrote my entire workflow and wrote another bash script to loop over my list of queries. This led to a factor of 50x fewer minutes spent. The savings affords me to run more frequent workflows and have more up-to-date data.\nYes, the total time for each workflow is longer in series than in parallel. The headaches avoided from understanding and implementing matrices makes this completely worth it. Also, the scraping is much gentler on the servers too."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-main-branch",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-main-branch",
    "title": "Making my own scraper bot!",
    "section": "The Main Branch",
    "text": "The Main Branch\nIn my mad dash for power, it had not occurred to me to create a separate branch where my scraping could be done. My original workflow ran in the main branch. Another silly decision was to commit and push after each scraping job when I implemented the matrix strategy. As a result, my main branch had a flurry of updates that prevented me from making separate commits of my own.\n\n\n\n\n\nDon’t code where you eat.\n\n\nSo, don’t do that. Create a temporary branch to play in and delete it when you’re done."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#footnotes",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#footnotes",
    "title": "Making my own scraper bot!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI use “crontab guru” to make a cron schedule. For example, ‘7 7,19 * * 0’ corresponds to Sundays at 7:07 am and pm. ↩︎\nI am not a fan of the term “matrix”. Not one bit.↩︎"
  }
]