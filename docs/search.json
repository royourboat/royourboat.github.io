[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Judging Wine Prices: Selecting a Model (Part 2)\n\n\n\n\n\n\n\nPython\n\n\nJSON\n\n\nBERT\n\n\nNLP\n\n\nmachine-learning\n\n\nsklearn\n\n\n\n\nUsing both context (SBERT) and keywords (TF-IDF) with XGBoost\n\n\n\n\n\n\nJul 11, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nJudging Wine Prices: Training Data (Part 1)\n\n\n\n\n\n\n\nPython\n\n\nJSON\n\n\nBERT\n\n\nNLP\n\n\n\n\nIs $20 a good price for this bottle?\n\n\n\n\n\n\nJul 10, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\n31 Million Wine Reviews\n\n\n\n\n\n\n\nPython\n\n\nJSON\n\n\nNLP\n\n\nPostgreSQL\n\n\n\n\nI scraped a LOT of reviews. Found neat distributions and bizarre reviewers.\n\n\n\n\n\n\nJul 6, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nDIY Wine Database with postgreSQL\n\n\n\n\n\n\n\nbash\n\n\nYAML\n\n\nGitHub Actions\n\n\nPostgreSQL\n\n\n\n\nImagine the LCBO website without ads, overlays, cookies, and clunky UI.\n\n\n\n\n\n\nJul 1, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nMaking my own scraper bot!\n\n\n\n\n\n\n\nbash\n\n\nYAML\n\n\nGitHub Actions\n\n\n\n\nI made a bot that runs on GitHub to regularly collect LCBO data.\n\n\n\n\n\n\nJun 22, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nScraping LCBO Data (Part 3: Product Descriptions)\n\n\n\n\n\n\n\nPython\n\n\nHTML\n\n\nbash\n\n\nJSON\n\n\n\n\nLearn how to find and download data products from the LCBO webpage.\n\n\n\n\n\n\nJun 17, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nScraping LCBO Data (Part 2: Product Inventory)\n\n\n\n\n\n\n\nPython\n\n\nHTML\n\n\nbash\n\n\nJSON\n\n\n\n\nScraping with regex.\n\n\n\n\n\n\nJun 10, 2023\n\n\nStephen Ro\n\n\n\n\n\n\n  \n\n\n\n\nScraping LCBO Data (Part 1: Store Information)\n\n\n\n\n\n\n\nPython\n\n\nHTML\n\n\nbash\n\n\nJSON\n\n\n\n\nLearn how to find and download data products from the LCBO webpage.\n\n\n\n\n\n\nJun 1, 2023\n\n\nStephen Ro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#table-details",
    "href": "posts/2023-07-01-lcbo-psql/index.html#table-details",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Table details",
    "text": "Table details\nThe LCBO assigns a unique identifier for each store and product, store_id and sku number. These identifiers are the primary keys for each row of data. Below is a snapshot of the column names, data types, and nullability (?) in both tables.\n\n\n\n\n\n\n\n(a) products\n\n\n\n\n\n\n\n(b) products (cont’d)\n\n\n\n\n\n\n\n(c) stores\n\n\n\n\n\n\n\n\n\n(d) notes\n\n\n\n\n\n\n\n(e) prices\n\n\n\n\n\n\n\n(f) inventory\n\n\n\n\nFigure 1: List of column names, data types, and nullability for five tables. prices and inventory tables have composite (two) primary keys shown.\n\n\nI want to keep track of the price history of a product and only make updates when a price change has been noticed. The prices table uses a composite primary key of two columns, sku and checktime. checktime is the UNIX time for when the row was inserted into the table.\nUnlike the prices table, the inventory history is not recorded. This could be changed by creating a composite primary key of three columns. However, this could generate a lot of data that I do not currently need for my app."
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#table-relationships",
    "href": "posts/2023-07-01-lcbo-psql/index.html#table-relationships",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Table relationships",
    "text": "Table relationships\n\nThe lines connecting the tables indicate the relationships between them. The prices nad notes use composite primary keys derived from products and stores. The inventory has no primary keys but does require the sku value must exist in products.\nThese tables were designed using DrawSQL. You can freely download the PSQL commands to create these tables in your database. Here is a copy of the script:\n\n\n\n\n\n\nPSQL commands to CREATE and ALTER tables.\n\n\n\n\n\n-- 1. STORES\nCREATE TABLE \"stores\"(\n    \"store_id\" INTEGER NOT NULL,\n    \"name\" TEXT NULL,\n    \"lat\" DECIMAL(8, 2) NULL,\n    \"lng\" DECIMAL(8, 2) NULL,\n    \"address\" TEXT NULL,\n    \"city\" TEXT NULL,\n    \"zipcode\" TEXT NULL,\n    \"intersection\" TEXT NULL,\n    \"phone\" TEXT NULL,\n    \"url\" TEXT NULL\n);\nALTER TABLE\n    \"stores\" ADD PRIMARY KEY(\"store_id\");\n\n-- 2. products\nCREATE TABLE \"products\"(\n    \"sku\" INTEGER NOT NULL,\n    \"name\" TEXT NULL,\n    \"is_available\" BOOLEAN NOT NULL,\n    \"abv\" DECIMAL(8, 2) NULL,\n    \"volume\" INTEGER NULL,\n    \"quantity_per_package\" INTEGER NULL DEFAULT '1',\n    \"package_type\" TEXT NULL,\n    \"category\" TEXT NULL,\n    \"region\" TEXT NULL,\n    \"country\" TEXT NULL,\n    \"brand\" TEXT NULL,\n    \"description\" TEXT NULL DEFAULT 'No description available.',\n    \"url\" TEXT NULL,\n    \"url_thumbnail\" TEXT NULL,\n    \"notes\" TEXT NULL,\n    \"sugar_gm_per_ltr\" INTEGER NULL,\n    \"calories\" INTEGER NULL,\n    \"varietal\" TEXT NULL,\n    \"sweetness\" INTEGER NULL,\n    \"body\" INTEGER NULL,\n    \"flavor\" INTEGER NULL,\n    \"tannins\" INTEGER NULL,\n    \"acidity\" INTEGER NULL\n);\nALTER TABLE\n    \"products\" ADD PRIMARY KEY(\"sku\");\n\n-- 3. PRICES\nCREATE TABLE \"prices\"(\n    \"sku\" INTEGER NOT NULL,\n    \"price_cents\" INTEGER NULL,\n    \"promo_price_cents\" INTEGER NULL,\n    \"checktime\" BIGINT NOT NULL\n);\nALTER TABLE\n    \"prices\" ADD PRIMARY KEY(\"sku\", \"checktime\");\nALTER TABLE\n    \"prices\" ADD CONSTRAINT \"prices_sku_foreign\" FOREIGN KEY(\"sku\") REFERENCES \"products\"(\"sku\");\n    \n-- 4. INVENTORY\nCREATE TABLE \"inventory\"(\n    \"sku\" INTEGER NOT NULL,\n    \"store_id\" INTEGER NOT NULL,\n    \"quantity\" INTEGER NOT NULL,\n    \"checktime\" BIGINT NOT NULL\n);\nALTER TABLE\n    \"inventory\" ADD PRIMARY KEY(\"store_id\", \"sku\");\nALTER TABLE\n    \"inventory\" ADD CONSTRAINT \"inventory_store_id_foreign\" FOREIGN KEY(\"store_id\") REFERENCES \"stores\"(\"store_id\");\nALTER TABLE\n    \"inventory\" ADD CONSTRAINT \"inventory_sku_foreign\" FOREIGN KEY(\"sku\") REFERENCES \"products\"(\"sku\");\n\n-- 5. NOTES\nCREATE TABLE \"notes\"(\n    \"sku\" INTEGER NOT NULL,\n    \"note\" TEXT NOT NULL\n);\nALTER TABLE\n    \"notes\" ADD CONSTRAINT \"notes_sku_foreign\" FOREIGN KEY(\"sku\") REFERENCES \"products\"(\"sku\");\n\n\n\nNote, you can delete a table with the simple command:\nDROP TABLE tablename;\nMake sure to first delete tables that have keys or indices that are derivative of other tables (i.e., tables with foreign keys)."
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#basic-python-functions-for-psql",
    "href": "posts/2023-07-01-lcbo-psql/index.html#basic-python-functions-for-psql",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Basic Python Functions for PSQL",
    "text": "Basic Python Functions for PSQL\n\nOne-way Query:\nThe following is a barebones one-way query tool that requires the PSQL address and the query to send. I use this to drop and re-create a table, for example. Do not forget the .commit() to execute your query.\n\n\n\n\n\n\nOne-way Query Function\n\n\n\n\n\nimport psycopg2\ndef query(sql_address, query):\n    connection = psycopg2.connect(sql_address)\n    cur = connection.cursor()\n    cur.execute(query)\n    connection.commit()\n    connection.close()\n\n\n\n\n\nTable Query:\nThe following can return table queries such as “SELECT * FROM products LIMIT 10;”. The return is a list of tuples and the column name for each element in the tuple.\n\n\n\n\n\n\nTable Query Function\n\n\n\n\n\nimport psycopg2\ndef query_table(sql_address, query):\n    connection = psycopg2.connect(sql_address)\n    cur = connection.cursor()\n    cur.execute(query)\n    columnName = [desc[0] for desc in cur.description]\n    tbl = cur.fetchall()\n    connection.close()\n\n    return tbl, columnName  \n\n\n\n\n\nInsert Table Rows:\nHere is a function that inserts rows of data to my PSQL database (ref). It has a nice catch for SQL exceptions.\nThe arguments require the PSQL address, the table name, and column names. records is the data represented as a list of tuples where each tuple’s elements are defined and ordered identically to the list of column names, columns.\nNote, psycopg2 has multiple solutions for inserting rows and some are much faster than others (by orders of magnitude). I found psycopg2.extras.execute_values to be the fastest.\n\n\n\n\n\n\nInsert Table Rows Function\n\n\n\n\n\nimport psycopg2\nimport psycopg2.extras \ndef bulkInsert(sql_address, tableName, records, columns):\n    try:\n        connection = psycopg2.connect(sql_address)        \n        cursor = connection.cursor()\n\n        tableColumns = list(columns)\n        \n        sql_insert_query = f\"\"\"\n        INSERT INTO {tableName} ({', '.join(tableColumns)})\n        VALUES %s\n        \"\"\"\n        result = psycopg2.extras.execute_values(cursor, sql_insert_query, records)\n         \n        connection.commit()\n        print(cursor.rowcount, f\"Record inserted successfully into {tableName} table\")\n\n    except (Exception, psycopg2.Error) as error:\n        print(\"Error while updating PostgreSQL table\", error)\n\n    finally:\n        # closing database connection.\n        if connection:\n            cursor.close()\n            connection.close()\n            print(\"PostgreSQL connection is closed\")\n\n\n\nHere is an example of how to insert multiple rows with the above code.\nsql_address = \"postgres://USERNAME:PASSWORD@SERVER/DATABASE\"\ntablename = \"products\"\ndata = [\n        ('La Pamplemousse', 750, 14.5, 19.99),\n        ('Bargain Red Wine', 750, 14.0, 26.99),\n        ]\ncolumns = ['name', 'volume', 'abv', 'price']\n\nbulkInsert(sql_address, tablename, data, columns)\nThe bulkInsert function should suffice for inserting rows of data into a table. If I obtain a new product to insert into the table, I can use query_table to obtain the existing list to ensure the product is indeed new. A simple python solution for products is:\nskuOldList = query_table(sql_address, \"select sku from products;\")\nskuList = read_from_data_files()\nskuNewList = set(skuList) - set(skuOldList)"
  },
  {
    "objectID": "posts/2023-07-01-lcbo-psql/index.html#creating-a-price-history-table",
    "href": "posts/2023-07-01-lcbo-psql/index.html#creating-a-price-history-table",
    "title": "DIY Wine Database with postgreSQL",
    "section": "Creating a Price History Table",
    "text": "Creating a Price History Table\nI only insert the price if it has changed in value. I need to create a table from our prices table the most recent prices to make a comparison. This could be done using two approaches: temporary tables and views. Choosing which depends on the context and query complexity. The SQL commands are identical except for TEMP TABLE and VIEW.\n--CREATE VIEW price_temp AS\nCREATE TEMP TABLE temp_prices AS\nselect sku, price_cents, promo_price_cents, checktime from prices\ninner join (\n    select sku as sku2, max(checktime) as checktime2 from prices\n    group by sku\n) as max_checktime \non sku = sku2\nand checktime = checktime2\n;\nTemporary tables exist until the connection to the database is closed. The data added to the table can be updated or changed. It does not depend on original table(s) after creation. This is a more memory intensive process.\nViews exist only for one query! Each time the table is called upon, it is regenerated and can differ if the original tables change values (i.e., the view table is always current). This is a more computing intensive process.\nThe views solution is not suitable for my task because:\n\nIt can be completed in one query\nA single product is considered only once (so, regeneration is not necessary)\nMy server has low CPU speeds but plenty of memory\nThe task is executed on GitHub Actions. So, using views with low database CPU speeds with views would waste computing time on GitHub actions\nMy data is relatively small in memory (~45 MB).\n\nAfter creating a temporary table, the price data is inserted using the following sample code. Suppose I checked a product and find $15.00 and $10.00 are the regular and promotional prices. If either of these values differ from the last price entry then we need to insert the change.\nINSERT into prices (sku, price_cents, promo_price_cents, checktime)\nselect 10101, 1499, 999, 1686005186\nwhere not exists  (\n    select checktime from temp_prices where\n    1499 = price_cents and\n    999 = promo_price_cents and\n    1686005186 &gt;= checktime and\n    10101 = sku\n)\n;"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html",
    "href": "posts/2023-06-17-lcbo-scraper/index.html",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "",
    "text": "Designing a web-scraping strategy depends on your constraints (e.g.,deadlines, uncertainties, awareness), objectives (e.g., features, speed, reproducability), and experience! In this post, I talk about my experience of trying two different strategies to collect the product description data. Trying both strategies revealed valuable insights into the strengths and weaknesses of each. I share these insights below."
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#brute-forcing-strategy",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#brute-forcing-strategy",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Brute-forcing strategy:",
    "text": "Brute-forcing strategy:\nI noticed the product page contains a link “Check Availability in All Stores” that sends you to a well organized inventory page for this bottle! (We will get back to this inventory data later.) The inventory page has a “View Details” button that sends you back to the product page.\n\n\n\n\n\n\nSample inventory page\n\n\n\n\n\n\n\n\nInventory for Jackson-Triggs Cabernet Sauvignon\n\n\n\n\n\nUnlike the product page URL, the product inventory URL is structured simply as https://www.lcbo.com/en/storeinventory/?sku=32853. It looks like I can query inventory pages by selecting SKU numbers. If I have the SKU number then I can use the URL behind the “View Details” button to get the product descriptions URL. From there, I can use the techniques in Part 1 of the blog series to obtain JSON data containing product descriptions.\nThis technique works. The solution is a minor adaptation to the script shown in Part 1 of the blog series. I found approximately 13,000 SKU numbers have products associated with them. However, this brute-force approach took several days to perform. The reason is LCBO’s SKU number is up to 6 digits long which has 1 million possible combinations. So, only 1.3% of the requests were successful."
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Pros & Cons",
    "text": "Pros & Cons\nThis strategy feels quite embarassing to share. But, its valuable to make a poorly performing but “successful” solution. It made me appreciate how thoughtful and clever web scraping can be. A few small mistakes or accounting errors can lead to restarting the entire scraping process. Even worse is to patch together two poorly executed scraping results and hope things are okay. The frustrations compel me to consider more than one solution before deciding to commit and build.\nHere are my observations of the pros and cons for brute-forcing.\n\nPros to brute-forcing\n\nEasy to implement\nEasy to debug\nCan do other work while waiting\nFeels like you are working\n\n\n\nCons to brute-forcing\n\nExtremely slow and inefficient\nVulnerable to computer and network instabilities\nRequires babysitting\nRequires two steps (product inventory -&gt; product page)\nVery likely to have IP banned\nSuspicious and straining to servers\nGenerates enormous inertia to run again\nGives a false impression that you are working"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#query-strategy",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#query-strategy",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Query strategy:",
    "text": "Query strategy:\n\nPart 1: Finding the data\nUsing the techniques in Part 1 of the blog series, I found a relatively large JSON file is transferred when querying the LCBO product list. See the image below.\n\n\n\n\n\n\nJSON for product exploration page\n\n\n\n\n\n\n\n\n\nThe primary variables we care about are the first and last variables, “totalCount” and “results”.\n“totalCount” is the number of products resulting from my query. Since the query had no filters, there were 13,186 results products to be found.\n“results” is the list of wine products on display. It contains all of the product descriptions we are interested in and more. There is a LOT of data here that we can unpack. Here are some snapshots for one product:\n\nJSON part 1\nJSON part 2\nJSON part 3.\n\n\n\nPart 2: Requesting the data\nThis is where things become a little tricky. In Part 1, we found the cURL command to request store information data. The command had a single integer as an argument corresponding to the store ID. Here, we will have several parameters due to the numerous filters available. The parameters also contain hidden limits. For example, I cannot simply request a single JSON with 13,186 products (I tried, of course). As a result, we need to procede carefully with designing a scraping strategy.\nAfter importing the cURL command in Postman, this is what I see:\n\n\n\n\n\n\nJSON for product exploration page\n\n\n\n\n\n\n\n\n\nThe code snippet on the right is this:\n\n\n\n\n\n\ncURL command\n\n\n\n\n\ncurl –location ‘https://platform.cloud.coveo.com/rest/search/v2?organizationId=lcboproductionx2kwygnc’\n–header ‘User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0’\n–header ‘Accept: /’\n–header ‘Accept-Language: en-CA,en-US;q=0.7,en;q=0.3’\n–header ‘Accept-Encoding: gzip, deflate, br’\n–header ‘Authorization: Bearer xx883b5583-07fb-416b-874b-77cce565d927’\n–header ‘Content-Type: application/x-www-form-urlencoded; charset=UTF-8’\n–header ‘Origin: https://www.lcbo.com’\n–header ‘DNT: 1’\n–header ‘Connection: keep-alive’\n–header ‘Referer: https://www.lcbo.com/’\n–header ‘Sec-Fetch-Dest: empty’\n–header ‘Sec-Fetch-Mode: cors’\n–header ‘Sec-Fetch-Site: cross-site’\n–data ‘actionsHistory=%5B%5D&referrer=&analytics=%7B%22clientId%22%3A%22%22%2C%22documentLocation%22%3A%22https%3A%2F%2Fwww.lcbo.com%2Fen%2Fproducts%23t%3Dclp-products%26sort%3Drelevancy%26layout%3Dcard%22%2C%22documentReferrer%22%3A%22%22%2C%22pageId%22%3A%22%22%7D&isGuestUser=false&aq=%40ec_category%3D%3DProducts&searchHub=Web_Listing_EN&tab=clp-products&locale=en&firstResult=0&numberOfResults=48&excerptLength=200&enableDidYouMean=true&sortCriteria=relevancy&queryFunctions=%5B%5D&rankingFunctions=%5B%5D&groupBy=%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_sweetness%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_sweetness%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_sweetness%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_body%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_body%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_body%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_flavor_intensity%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_flavor_intensity%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_flavor_intensity%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_acidity%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_acidity%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_acidity%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_tannins%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_bintag_wine_tannins%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_bintag_wine_tannins%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40ec_price%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40ec_price%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40ec_price%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_total_volume%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_total_volume%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_total_volume%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_alcohol_percent%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_alcohol_percent%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_alcohol_percent%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%2C%7B%22field%22%3A%22%40lcbo_sugar_gm_per_ltr%22%2C%22injectionDepth%22%3A10000%2C%22computedFields%22%3A%5B%7B%22field%22%3A%22%40lcbo_sugar_gm_per_ltr%22%2C%22operation%22%3A%22minimum%22%7D%2C%7B%22field%22%3A%22%40lcbo_sugar_gm_per_ltr%22%2C%22operation%22%3A%22maximum%22%7D%5D%2C%22maximumNumberOfValues%22%3A1%7D%5D&facets=%5B%7B%22facetId%22%3A%22%40ec_category%22%2C%22field%22%3A%22ec_category%22%2C%22type%22%3A%22hierarchical%22%2C%22injectionDepth%22%3A1000%2C%22delimitingCharacter%22%3A%22%7C%22%2C%22filterFacetCount%22%3Atrue%2C%22basePath%22%3A%5B%22Products%22%5D%2C%22filterByBasePath%22%3Afalse%2C%22currentValues%22%3A%5B%5D%2C%22preventAutoSelect%22%3Afalse%2C%22numberOfValues%22%3A5%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_varietal_name%22%2C%22field%22%3A%22lcbo_varietal_name%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_vqa_code%22%2C%22field%22%3A%22lcbo_vqa_code%22%2C%22type%22%3A%22specific%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A8%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40country_of_manufacture%22%2C%22field%22%3A%22country_of_manufacture%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_region_name%22%2C%22field%22%3A%22lcbo_region_name%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_program%22%2C%22field%22%3A%22lcbo_program%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40lcbo_current_offer%22%2C%22field%22%3A%22lcbo_current_offer%22%2C%22type%22%3A%22specific%22%2C%22sortCriteria%22%3A%22occurrences%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A8%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40stores_stock%22%2C%22field%22%3A%22stores_stock%22%2C%22type%22%3A%22specific%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%5D%2C%22numberOfValues%22%3A8%2C%22freezeCurrentValues%22%3Afalse%2C%22preventAutoSelect%22%3Afalse%2C%22isFieldExpanded%22%3Afalse%7D%2C%7B%22facetId%22%3A%22%40ec_rating%22%2C%22field%22%3A%22ec_rating%22%2C%22type%22%3A%22numericalRange%22%2C%22sortCriteria%22%3A%22descending%22%2C%22injectionDepth%22%3A1000%2C%22filterFacetCount%22%3Atrue%2C%22currentValues%22%3A%5B%7B%22start%22%3A1%2C%22end%22%3A1.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A2%2C%22end%22%3A2.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A3%2C%22end%22%3A3.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A4%2C%22end%22%3A4.9%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%2C%7B%22start%22%3A5%2C%22end%22%3A5%2C%22endInclusive%22%3Atrue%2C%22state%22%3A%22idle%22%7D%5D%2C%22numberOfValues%22%3A5%2C%22freezeCurrentValues%22%3Afalse%2C%22generateAutomaticRanges%22%3Afalse%7D%5D&facetOptions=%7B%7D&categoryFacets=%5B%5D&retrieveFirstSentences=true&timezone=America%2FNew_York&enableQuerySyntax=false&enableDuplicateFiltering=false&enableCollaborativeRating=false&debug=false&allowQueriesWithoutKeywords=true&dictionaryFieldContext=%7B%22stores_stock%22%3A%22%22%2C%22stores_inventory%22%3A%22217%22%2C%22stores_stock_combined%22%3A%22217%22%2C%22stores_low_stock_combined%22%3A%22217%22%7D’\n\n\n\nYIKES! The big block of text in the “–data” argument is URL encoded. We can decode this using online converters such as https://meyerweb.com/eric/tools/dencoder/ and prettify with http://urlprettyprint.com/. Here’s what I find:\nactionsHistory              = []\nreferrer                    = \nanalytics                   = {\"clientId\":\"\",\"documentLocation\":\"https://www.lcbo.com/en/products#t=clp-products&sort=relevancy&layout=card\",\"documentReferrer\":\"\",\"pageId\":\"\"}\nisGuestUser                 = false\naq                          = @ec_category==Products\nsearchHub                   = Web_Listing_EN\ntab                         = clp-products\nlocale                      = en\nfirstResult                 = 0\nnumberOfResults             = 48\nexcerptLength               = 200\nenableDidYouMean            = true\nsortCriteria                = relevancy\nqueryFunctions              = []\nrankingFunctions            = []\ngroupBy                     = [{\"field\":\"@lcbo_bintag_wine_sweetness\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_sweetness\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_sweetness\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_body\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_body\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_body\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_flavor_intensity\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_flavor_intensity\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_flavor_intensity\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_acidity\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_acidity\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_acidity\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_bintag_wine_tannins\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_bintag_wine_tannins\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_bintag_wine_tannins\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@ec_price\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@ec_price\",\"operation\":\"minimum\"},{\"field\":\"@ec_price\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_total_volume\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_total_volume\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_total_volume\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_alcohol_percent\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_alcohol_percent\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_alcohol_percent\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1},{\"field\":\"@lcbo_sugar_gm_per_ltr\",\"injectionDepth\":10000,\"computedFields\":[{\"field\":\"@lcbo_sugar_gm_per_ltr\",\"operation\":\"minimum\"},{\"field\":\"@lcbo_sugar_gm_per_ltr\",\"operation\":\"maximum\"}],\"maximumNumberOfValues\":1}]\nfacets                      = [{\"facetId\":\"@ec_category\",\"field\":\"ec_category\",\"type\":\"hierarchical\",\"injectionDepth\":1000,\"delimitingCharacter\":\"|\",\"filterFacetCount\":true,\"basePath\":[\"Products\"],\"filterByBasePath\":false,\"currentValues\":[],\"preventAutoSelect\":false,\"numberOfValues\":5,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_varietal_name\",\"field\":\"lcbo_varietal_name\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_vqa_code\",\"field\":\"lcbo_vqa_code\",\"type\":\"specific\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":8,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@country_of_manufacture\",\"field\":\"country_of_manufacture\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_region_name\",\"field\":\"lcbo_region_name\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_program\",\"field\":\"lcbo_program\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@lcbo_current_offer\",\"field\":\"lcbo_current_offer\",\"type\":\"specific\",\"sortCriteria\":\"occurrences\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":8,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@stores_stock\",\"field\":\"stores_stock\",\"type\":\"specific\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[],\"numberOfValues\":8,\"freezeCurrentValues\":false,\"preventAutoSelect\":false,\"isFieldExpanded\":false},{\"facetId\":\"@ec_rating\",\"field\":\"ec_rating\",\"type\":\"numericalRange\",\"sortCriteria\":\"descending\",\"injectionDepth\":1000,\"filterFacetCount\":true,\"currentValues\":[{\"start\":1,\"end\":1.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":2,\"end\":2.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":3,\"end\":3.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":4,\"end\":4.9,\"endInclusive\":true,\"state\":\"idle\"},{\"start\":5,\"end\":5,\"endInclusive\":true,\"state\":\"idle\"}],\"numberOfValues\":5,\"freezeCurrentValues\":false,\"generateAutomaticRanges\":false}]\nfacetOptions                = {}\ncategoryFacets              = []\nretrieveFirstSentences      = true\ntimezone                    = America/New_York\nenableQuerySyntax           = false\nenableDuplicateFiltering    = false\nenableCollaborativeRating   = false\ndebug                       = false\nallowQueriesWithoutKeywords = true\ndictionaryFieldContext      = {\"stores_stock\":\"\",\"stores_inventory\":\"217\",\"stores_stock_combined\":\"217\",\"stores_low_stock_combined\":\"217\"}\nWe can start throwing away some arguments that we do not need such as “groupBy” and “analytics”. With some experimentation1, I was able to widdle it down to this:\nlocale                      = en\nfirstResult                 = '\"$1\"'\nnumberOfResults             = '\"$2\"'\nexcerptLength               = 200\nsortCriteria                = @ec_price '\"$3\"'\nfacets                      = [{\"facetId\":\"@ec_category\",\"field\":\"ec_category\",\"type\":\"hierarchical\",\"injectionDepth\":1000,\"delimitingCharacter\":\"|\",\"filterFacetCount\":true,\"basePath\":[\"Products\"],\"filterByBasePath\":false,\"currentValues\":[{\"value\":\"wine\",\"state\":\"idle\",\"children\":[{\"value\":\"'\"$4\"'\",\"state\":\"selected\",\"children\":[],\"retrieveChildren\":false,\"retrieveCount\":0}],\"retrieveChildren\":false,\"retrieveCount\":0}],\"preventAutoSelect\":true,\"numberOfValues\":1,\"isFieldExpanded\":false}]\nfacetOptions                = {\"freezeFacetOrder\":true}\ncategoryFacets              = []\nretrieveFirstSentences      = true\ntimezone                    = America/New_York\nenableQuerySyntax           = false\nenableDuplicateFiltering    = false\nenableCollaborativeRating   = false\ndebug                       = false\nallowQueriesWithoutKeywords = true\nA few additional modifications have been made to the above.\n\nAdded a sort criterion “ec_price” for price.\nModified “facets” to return only wine-related products. See the end of this section for non-wine products.\nAdded 4 variable placeholders in particular locations with the following meanings:\n\n$1: page number (starts at 0)\n$2: number of requests per page number (max 10002)\n$3: sort order; use “ascending” or “descending” text without quotation marks.\n$4: wine-category; must be URL encoded. E.g., red$20wine\n\n\nWhen we re-encode the above, we can rebuild our cURL command:\ncurl --location 'https://platform.cloud.coveo.com/rest/search/v2?organizationId=lcboproductionx2kwygnc' \\\n--header 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0' \\\n--header 'Accept: */*' \\\n--header 'Accept-Language: en-CA,en-US;q=0.7,en;q=0.3' \\\n--header 'Accept-Encoding: gzip, deflate, br' \\\n--header 'Authorization: Bearer xx883b5583-07fb-416b-874b-77cce565d927' \\\n--header 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \\\n--header 'Origin: https://www.lcbo.com' \\\n--header 'DNT: 1' \\\n--header 'Connection: keep-alive' \\\n--header 'Referer: https://www.lcbo.com/' \\\n--header 'Sec-Fetch-Dest: empty' \\\n--header 'Sec-Fetch-Mode: cors' \\\n--header 'Sec-Fetch-Site: cross-site' \\\n--data 'locale=en&firstResult='\"$1\"'&numberOfResults='\"$2\"'&excerptLength=2000&sortCriteria=%40ec_price%20'\"$3\"'&facets=%5B%7B%22facetId%22%3A%22%40ec_category%22%2C%22field%22%3A%22ec_category%22%2C%22type%22%3A%22hierarchical%22%2C%22injectionDepth%22%3A1000%2C%22delimitingCharacter%22%3A%22%7C%22%2C%22filterFacetCount%22%3Atrue%2C%22basePath%22%3A%5B%22Products%22%5D%2C%22filterByBasePath%22%3Afalse%2C%22currentValues%22%3A%5B%7B%22value%22%3A%22wine%22%2C%22state%22%3A%22idle%22%2C%22children%22%3A%5B%7B%22value%22%3A%22'\"$4\"'%22%2C%22state%22%3A%22selected%22%2C%22children%22%3A%5B%5D%2C%22retrieveChildren%22%3Afalse%2C%22retrieveCount%22%3A0%7D%5D%2C%22retrieveChildren%22%3Afalse%2C%22retrieveCount%22%3A0%7D%5D%2C%22preventAutoSelect%22%3Atrue%2C%22numberOfValues%22%3A1%2C%22isFieldExpanded%22%3Afalse%7D%5D&facetOptions=%7B%22freezeFacetOrder%22%3Atrue%7D&categoryFacets=%5B%5D&retrieveFirstSentences=true&timezone=America%2FNew_York&enableQuerySyntax=false&enableDuplicateFiltering=false&enableCollaborativeRating=false&debug=false&allowQueriesWithoutKeywords=true' \\\n--output ''\"$5\"'' \nI added the “–output” command and a 5th variable placeholder which represents the output filename. Save the script and run it in a terminal with the following commands:\nchmod +x wine_request.sh\n./wine_request.sh 0 1000 ascending red%20wine redwine.json\nThe arguments trailing the script “./wine_request.sh” are passed to the numbered arguments $1 through $5, respectively. Congratulations! We have downloaded and saved the JSON file as ‘redwine.json’. Better yet, we can download 1,000 products per query!\nThere are non-wine beverages as well (e.g., beer, spirits, cider, icewine). Due to the query structure, we cannot use the same script above for non-wine products. We can create a second script that is nearly identical with the same arguments.\ncurl --location 'https://platform.cloud.coveo.com/rest/search/v2?organizationId=lcboproductionx2kwygnc' \\\n--header 'User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0' \\\n--header 'Accept: */*' \\\n--header 'Accept-Language: en-CA,en-US;q=0.7,en;q=0.3' \\\n--header 'Accept-Encoding: gzip, deflate, br' \\\n--header 'Authorization: Bearer xx883b5583-07fb-416b-874b-77cce565d927' \\\n--header 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \\\n--header 'Origin: https://www.lcbo.com' \\\n--header 'DNT: 1' \\\n--header 'Connection: keep-alive' \\\n--header 'Referer: https://www.lcbo.com/' \\\n--header 'Sec-Fetch-Dest: empty' \\\n--header 'Sec-Fetch-Mode: cors' \\\n--header 'Sec-Fetch-Site: cross-site' \\\n--data 'locale=en&firstResult='\"$1\"'&numberOfResults='\"$2\"'&excerptLength=2000&sortCriteria=%40ec_price%20'\"$3\"'&facets=%5B%7B%22facetId%22%3A%22%40ec_category%22%2C%22field%22%3A%22ec_category%22%2C%22type%22%3A%22hierarchical%22%2C%22injectionDepth%22%3A1000%2C%22delimitingCharacter%22%3A%22%7C%22%2C%22filterFacetCount%22%3Atrue%2C%22basePath%22%3A%5B%22Products%22%5D%2C%22filterByBasePath%22%3Afalse%2C%22currentValues%22%3A%5B%7B%22value%22%3A%22'\"$4\"'%22%2C%22state%22%3A%22selected%22%2C%22children%22%3A%5B%5D%2C%22retrieveChildren%22%3Afalse%2C%22retrieveCount%22%3A0%7D%5D%2C%22preventAutoSelect%22%3Atrue%2C%22numberOfValues%22%3A1%2C%22isFieldExpanded%22%3Afalse%7D%5D&facetOptions=%7B%22freezeFacetOrder%22%3Atrue%7D&categoryFacets=%5B%5D&retrieveFirstSentences=true&timezone=America%2FNew_York&enableQuerySyntax=false&enableDuplicateFiltering=false&enableCollaborativeRating=false&debug=false&allowQueriesWithoutKeywords=true' \\\n--output ''\"$5\"''  \n\n\nPart 3: Collecting all the data\nOkay… so how do I get all the products?\nWe need to create another script that executes multiple requests. Below, I will describe pieces of a script that we will combine at the end.\n\nA. Run a loop over each category of wine\nI have a text file containing wine categories in each row. The first column is the URL encoded string to pass into the cURL command. The second column is the prefix for the filenames.\n\n\n\n\n\n\ncode/wine_names.txt\n\n\n\n\n\nred%20wine redwine\nwhite%20wine whitewine\nsparkling%20wine sparklingwine\nrose%20wine rosewine\nfortified%20wine fortifiedwine\nchampagne champagne\nsake%20%26%20rice%20wine sakericewine\nspecialty%20wine specialtywine\ngifts%20and%20samplers giftsandsamplers\nicewine icewine\n\n\n\nThe following while loop iterates over each row in the text file above. The first and second columns are represented as variables “htag” and “ftag”. These variables are into a function called “scrape_request_loop()”.\nwhile IFS= read -r htag ftag; do\n    scrape_request_loop $htag $ftag\ndone &lt; ./code/wine_names.txt\n\n\nB. Loop over page number requests\nThe function “scrape_request_loop()” has multiple steps and can be seen in its entirety at the end of this subsection. I first describe the function in chunks before combining the chunks together.\nWe request the first 1,000 products for the specified category with the command below. You can assume the function “request_ascending()” executes the cURL command and saves the JSON file.\n# $1: $htag; wine-category URL substring; e.g., red$20wine\n# $2: $ftag; wine-category filename prefix; e.g., redwine\n# E.g., request_ascending pageNumber numProducts htag ftag\nrequest_ascending 0 1000 $1 $2  \nThe JSON file contains a variable “totalCount” which is the number of products found in the category specified. This is important since it determines whether there are more than 1,000 wines in this category. The following commands read the newly made JSON file (using “jq”) and determines how many more query requests are needed. It then makes these query requests with the for loop. The for loop is skipped if “totalCount” is less than 1000.\ntotalCount=$(jq '.totalCount' $fname) # Read newly made JSON file with jq. Get totalCount.\ncountPer1000=$((totalCount/1000)) # Number of 1000-batches needed.\nmaxPages=$countPer1000 # Maximum number of pages (redundant for now...)\n\nfor pageNumber in $(seq 1000 1000 $(($maxPages*1000)))\ndo\n  request_ascending $pageNumber 1000 $1 $2\ndone\nUnfortunately, the solution above does not work if “totalCount” exceeds 5,000. If you request “pageNumber” 5 and beyond (the index starts at 0), the server will return results for “pageNumber = 4”. There appears to be a hidden limit set by the server. While this is mildly annoying, the simple fix is to reverse the sort order from ascending to descending and query again. Of course, this solution cannot work immediately since there are 13,186 products and only 10,000 will be delivered. This is why I filtered by product category to reduce the maximum results. This solution will also fail for red wines since there are 9,400 product descriptions available. That will not happen any time soon and is a future Stephen problem3.\nWe need to change the “maxPages” definition to take the minimum between (“countPer1000”, 4).\nmaxPages=$(($countPer1000&lt;4?$countPer1000:4)) # Minimum value b/w (countPer1000, 4)\nAfter the for loop executing the function “request_ascending()” is complete, we need to append the following for loop to obtain the remaining solutions:\nif [ $countPer1000 -ge 5 ]; then\n    for pageNumber in $(seq 0 1000 $(($countPer1000*1000-4000)))\n    do\n        request_descending $pageNumber 1000 $1 $2\n    done\nfi\n\n\nC. The entire script\nThe entire script is shown below. After setting permissions (i.e., “chmod +x”), the script starts at the bottom where the while loops begin.\nThe functions “request_ascending()” and “request_descending()” contain the command to execute the cURL scripts.\n#!/bin/bash\nchmod +x code/wine_request.sh\nchmod +x code/nonwine_request.sh\n\nrequest_ascending () {\n    # $1: start index\n    # $2: number of requests\n    # $3: wine-category URL substring; e.g., red$20wine\n    # $4: wine-category filename prefix; e.g., redwine\n    fname=\"json/products/$4.a.$1.json\"\n    sudo $SCRAPE_SH $1 $2 ascending $3 $fname\n    sleep 1\n}\nrequest_descending () {\n    fname=\"json/products/$4.d.$1.json\"\n    sudo $SCRAPE_SH $1 $2 descending $3 $fname\n    sleep 1\n}\n\nscrape_request_loop (){\n    # $1: $htag; wine-category URL substring; e.g., red$20wine\n    # $2: $ftag; wine-category filename prefix; e.g., redwine\n    request_ascending 0 1000 $1 $2\n    totalCount=$(jq '.totalCount' $fname)\n    countPer1000=$((totalCount/1000))\n    maxPages=$(($countPer1000&lt;4?$countPer1000:4))  \n\n    for i in $(seq 1000 1000 $(($maxPages*1000)))\n    do\n        request_ascending $i 1000 $1 $2\n    done\n\n    if [ $countPer1000 -ge 5 ]; then\n        for i in $(seq 0 1000 $(($countPer1000*1000-4000)))\n        do\n            request_descending $i 1000 $1 $2\n        done\n    fi\n\n}\n\nSCRAPE_SH=\"./code/wine_request.sh\"\nwhile IFS= read -r htag ftag; do\n    scrape_request_loop $htag $ftag\ndone &lt; ./code/wine_names.txt\n\nSCRAPE_SH=\"./code/nonwine_request.sh\"\nwhile IFS= read -r htag ftag; do\n    scrape_request_loop $htag $ftag\ndone &lt; ./code/nonwine_names.txt"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons-1",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#pros-cons-1",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Pros & Cons",
    "text": "Pros & Cons\n\nPros\n\nExtremely fast (1 min!) and no babysitting\nVery cheap\nAutomatable for Github Actions (a big deal)\nGentle on servers\nCan execute more frequently for up to date price changes\nRequests existing products only (unlike the brute-force method)\n\n\n\nCons\n\nRead and manage URL-encoded variables carefully\nTime to understand cURL parameters\nClose attention to hidden limits"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#data-product-summary",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#data-product-summary",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Data product summary",
    "text": "Data product summary\nOnce I have all of the product description data, I can generate a list of SKU numbers that I can query to obtain the live inventory (see Part 2 of this blog series)! There are approximately registered products: 9,400 wine and 4,100 non-wine. “Non-wine” products can include beer, liquor, and reusuable bags. Only 2,800 wines and 2,100 non-wines are purchasable as of 5/14/2023.\nThe files described above are located in the repository: https://github.com/royourboat/lcbo-wine-scraper. Specifically,\ncode/all_product_request.sh\n\ncode/wine_request.sh\n\ncode/nonwine_request.sh\n\ncode/wine_names.txt\n\ncode/nonwine_names.txt"
  },
  {
    "objectID": "posts/2023-06-17-lcbo-scraper/index.html#footnotes",
    "href": "posts/2023-06-17-lcbo-scraper/index.html#footnotes",
    "title": "Scraping LCBO Data (Part 3: Product Descriptions)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Postman, you can delete lines of code in the Code Snippet panel and see its impact by clicking “Send”. Super convenient.↩︎\nfound through trial and error↩︎\nI can partition the query using different categories.↩︎"
  },
  {
    "objectID": "posts/2023-06-01-lcbo-scraper/index.html",
    "href": "posts/2023-06-01-lcbo-scraper/index.html",
    "title": "Scraping LCBO Data (Part 1: Store Information)",
    "section": "",
    "text": "In Canada, the government of Ontario owns a Crown corporation called the Liquor Control Board of Ontario (LCBO). The LCBO distributes practically all alcoholic drinks in the province of Ontario and is one of the largest purchasers of alcohol in the world1."
  },
  {
    "objectID": "posts/2023-06-01-lcbo-scraper/index.html#data-product-store-information",
    "href": "posts/2023-06-01-lcbo-scraper/index.html#data-product-store-information",
    "title": "Scraping LCBO Data (Part 1: Store Information)",
    "section": "Data Product: Store Information",
    "text": "Data Product: Store Information\n\nPart 1: Finding the data\nEach LCBO store location has a webpage containing its details such as hours, address, and phone number:\n\n\n\n\n\n\nhttps://www.lcbo.com/en/stores/queens-quay-cooper-street-217\n\n\n\n\n\n\n\n\n\nhttps://www.lcbo.com/en/stores/queens-quay-cooper-street-217\nWhen clicking the above link, we can monitor the traffic of information arriving to your browser and search for the store information. Often times, its sent as a JSON file.\nTo find the JSON, do the following:\n\nRight-Click on the page\nClick Inspect\nSelect the “Network” tab\nSelect the “XHR” sub-tab (on Chrome it is “Fetch/XHR”)\nReload the page and wait.\n\nYou’ll see a table being filled as data is being sent to your browser.\n\nSort the table by file size (optional)\nClick on the “Response” sub-tab located at the bottom.\n\nHere’s how it looks on my screen so far.\n\n\n\n\n\n\nStore Information (JSON)\n\n\n\n\n\n\n\n\n\nClick on a table row JSON file type. You will see the contents of the JSON file beneath the “Response” sub-tab. Keep clicking until you find the store information.\nCongratulations! You’ve found the data. It contains the store address, longitude and latitude, city, area (zip) code, phone number, URL, store hours, and store ID.\nNow lets grab it.\n\n\nPart 2: Requesting the data\ncURL is a command line tool used to transfer data. It was made by a dev who wanted to automatically fetch currency exchange rates! We use the cURL command by…\n\nRight-clicking on the row with the data product\nSelect “Copy Value”\nSelect “Copy as cURL (Windows)” or (POSIX)\n\n\n\n\n\n\n\ncURL command location\n\n\n\n\n\n\n\n\n\nThe cURL command is now on your clipboard. If you paste the cURL command into an editor, you’ll find a wall of intimidating text. I found Postman incredibly useful to parse the text. Make a free account, agree to things, and you’ll arrive at a workspace. Click on “Import” in the top-left (see note below).\n\n\n\n\n\n\nPostman - Import button\n\n\n\n\n\n\n\n\n\nA little window will pop up. Paste the copied cURL command and Postman will automatically setup a workspace. Click the blue button “Send” to execute the cURL command. The JSON is retrieved and displayed at the bottom. Next, select the “Header” tab and then select the code snippet button “&lt;/&gt;” on the very right margin. See image below.\n\n\n\n\n\n\nPostman - Import button\n\n\n\n\n\n\n\n\n\nEach row in the “Header” tab is an argument in the cURL command. If you uncheck an option, the cURL command will update on the right. Most of these arguments are not important for us. You can uncheck them and click “Send” again to see whether the JSON product is delivered. I found the bare minimum needed is “Content-type” and “X-Requested-With”. So, we should include them.\nLooking at the cURL command (see my reduced version below), you’ll find an argument that requests the specific store corresponding to “Queen’s Quay”.\ncurl --location 'https://www.lcbo.com/en/storepickup/selection/store/' \\\n--header 'Accept-Encoding: gzip, deflate, br' \\\n--header 'Content-Type: application/x-www-form-urlencoded; charset=UTF-8' \\\n--header 'X-Requested-With: XMLHttpRequest' \\\n--data 'value=217&st_loc_flag=true'\nCopy your cURL command and paste it into a notepad/editor. Change the value from 217 to 218. Copy the new command, click “Import” again in Postman, paste the command, and click “Send”. You’ll obtain a different store location’s details corresponding to 218.\nNow we know what to change to start collecting all store information.\n\n\nPart 3: Collecting all the data\nOur next goal is to write a bash script that will loop cURL command for all values. My script is below. To test this, you’ll need to save it as a file with a “.sh” file tag (e.g., scriptname.sh) and access to a bash terminal. If you’re using a linux or mac then just pop a terminal open, go to the folder with your script, and type the following:\nchmod +x scriptname.sh\n./scriptname.sh\nThere are a lot of solutions for Windows systems. Since Ubuntu was my primary OS for a while, I prefer just installing WSL (or WSL2).\n#!/bin/bash\nfor STORE_ID in {1..800}\ndo\n   curl --location 'https://www.lcbo.com/en/storepickup/selection/store/' \\\n   --header 'content-type: application/x-www-form-urlencoded; charset=UTF-8' \\\n   --header 'x-newrelic-id: VwQHU1dQCRAJU1NUAgMEUFQ=' \\\n   --header 'x-requested-with: XMLHttpRequest' \\\n   --data 'value='$STORE_ID'&st_loc_flag=true' \\\n   --output 'json/stores/store_'$STORE_ID'.json'\n   sleep 10\ndone\nHere are explanations for the bash script above.\n\nInstructs the operating system we are using bash.\n\n#!/bin/bash\n\nA for loop where the variable STORE_ID takes values between 1 and 800. After some trial and error, I did not find successful returns above 800. This number is consistent with there being nearly 700 LCBO stores.\n\nfor STORE_ID in {1..800}\ndo\n  ...\ndone\n\nI use the for loop variable here to specify the store ID. Note, if there is no data for the store ID then the return is a JSON containing {“success”: false}.\n\n   --data 'value='$STORE_ID'&st_loc_flag=true'\n\nThis argument stores the output as a file with the filename in quotations.\n\n   --output 'json/stores/store_'$STORE_ID'.json'\n\nThis is incredibly important. This is a sleep command of 10 seconds. If you do not have a delay, the for loop will send 800 requests nearly instantaneously. This can place tremendous load on the servers and cause delays or crashes for customers using the website. What will likely happen is that your IP address will be temporarily banned after the 50th or so request. In my opinion, you deserve this ban. We do not want to harass freely accessible websites!\n\nsleep 10\n\n\n\n\n\n\nScrape gently\n\n\n\nAlways scrape gently in testing and final production. If you don’t, they can easily make scraping much harder and more annoying."
  },
  {
    "objectID": "posts/2023-06-01-lcbo-scraper/index.html#footnotes",
    "href": "posts/2023-06-01-lcbo-scraper/index.html#footnotes",
    "title": "Scraping LCBO Data (Part 1: Store Information)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA little known fact about Ontario is that it is full of monopolistic business practices.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Me persuading this whip to crack during a public talk."
  },
  {
    "objectID": "about.html#academia",
    "href": "about.html#academia",
    "title": "About",
    "section": "Academia",
    "text": "Academia\nMy professional background is in math, physics, and astrophysics. Here are some projects I have worked on with truly amazing people:\n\nAnalyzed1 the optical performance of cryogenic light guides for a dark matter detector2.\nTested optical shutters3 and measured proton beam alignments4 for a particle accelerator in Japan. It shoots neutrinos under Japan and is aimed at an underground neutrino detector, Super-Kamiokande.\nSimulated galaxies merging with galaxy-groups with FLASH: a FORTRAN(!) hydro code.\nWrote Galino5: A 3D spectrographic image synthesizer that generates realistic datacubes (X, Y, wavelength) of line emission from distant galaxies. Includes realistic statistical properties from telescopes, instruments, and radiation physics.\nSolved for a lot of eigenvalue-solutions describing self-similar shocks sweeping through an atmosphere6.\nSolved7 the structures of stars with extremely dense winds. I did this to simulate how a supernova explosion could look differently.\nMy favourite: A mantis shrimp video inspired me to adapt the physics of sonoluminescence8 to describe shockwaves in stars9.\nSimulated how failed supernovae with a central black hole can still explode10.\n\n\nEducation\nBScH in Mathematical Physics | Queen’s University  PhD in Astrophysics | University of Toronto  Postdoc | Theoretical Astrophysics Center | UC Berkeley\n\n\nPapers"
  },
  {
    "objectID": "about.html#teaching-and-outreach",
    "href": "about.html#teaching-and-outreach",
    "title": "About",
    "section": "Teaching and Outreach",
    "text": "Teaching and Outreach\nI love to teach! This passion was lit in graduate school. Since then, I designed an undergraduate python course how I wished science and coding were taught to me. I then trained to become a highschool math and physics teacher in Ontario, Canada. This was an immensely rewarding path that I enjoyed thoroughly and miss. The decision to place my teaching path on hold11 was quite an emotional and challenging one. But, my passion to teach continues and will appear in all my presentations and talks to come."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nusing lasers and exorbitant amounts of liquid nitrogen↩︎\ncryogenic acrylic chamber housing 3600 kg liquid argon in a mine 2 km underground. The earth is really warm down there! DEAP3600.↩︎\nscreens that show the numbers on your childhood calculator (i.e., twisted nematic screens). Use these as remotely operated digital shutters for a camera in an irradiated area. Schematic↩︎\nand convincing researchers to not over-interpret the shoulders of gaussian fits to 5 data points.↩︎\nDistant galaxies are seen at earlier times during the “galactic bombardment phase” when galaxies are being smashed and cannibalizing smaller galaxies. It is a messy phase and a telescopic image will show a bunch of pixelated blobs. Hence, “Galino” = “little galaxy … blobs”. ↩︎\nA close analogy is a wave traveling down a whip and studying how it “cracks”. Except, the wave is replaced with a shockwave and the whip is replaced with an atmosphere.↩︎\nWrote a relaxation algorithm to solve non-linear equations describing a star with a windy surface. Proudly integrated unwieldly FORTRAN code (OPAL) to include realistic opacity calculations.↩︎\nSonoluminescence. Sono = sound; luminescence = light. If you blast two ultrasonic soundwaves at each other underwater, the constuctive interference can become non-linear and rip open a cavity. The underwater cavity is like a hole with no pressure support (i.e., it’s not a bubble) and so it implodes. The implosion of a spherical wall of water meeting at a singularity generates light with spectral characteristics indicating peak emission temperatures of 5,000 to 150,000 K. The sun’s surface is about 6,000 K…↩︎\nI got goosebumps from seeing my exploding star simulation be predicted perfectly by solutions for underwater implosions, a seemingly unrelated problem. ↩︎\nImagine if the earth’s mass suddenly decreased by 10%. You’d immediately feel lighter because gravity has suddenly decreased. This happens in a massive star (maybe 0.5% of mass rather than 10%) moments before it goes supernova. The sudden drop in gravity causes the outer envelope of gas to expand non-uniformly and non-linearly to form a shockwave that might breakout to the surface of the star. You’d never see this smaller shockwave because it’d be overrun by the bigger supernova shockwave coming from the core! This scenario was thought about because astronomers found a red supergiant to have disappeared without any explosion of light… indicating supernova can fail…?! (Also, the mass vanishes suddenly because most of the supernova’s energy is lost in the form of neutrinos. These neutrinos don’t interact with anything and can zoom out of the star at the speed of light unimpeded.)↩︎\nfor many, many reasons including the state of education and infrastructure, culture, policies, and politics, which I’m happy to talk about↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Ro  Ph.D.",
    "section": "",
    "text": "Stephen Ro  Ph.D.\nTrained in scientific computing, mathematical physics, and astrophysics, Stephen thrives on making sense of complex systems with research, data, and collaboration. He brings more than a decade of programming, project, and leadership experience into data science.\nStephen is passionate about teaching and learning. He has taught python at UC Berkeley, given numerous talks on astrophysics (for all ages!), and designed a collaborative data-driven astronomy project for high school math students."
  },
  {
    "objectID": "posts/2023-06-10-lcbo-scraper/index.html",
    "href": "posts/2023-06-10-lcbo-scraper/index.html",
    "title": "Scraping LCBO Data (Part 2: Product Inventory)",
    "section": "",
    "text": "A fundamental feature of my wine app is to only recommend bottles available right now at a store nearby.\nLCBO’s product inventory data is live and readily accessible for any given product given the unique SKU number (a 6 digit code). For instance, SKU number 328534 leads you to the inventory page for Jackson-Triggs Cabernet Sauvignon using the URL https://www.lcbo.com/en/storeinventory/?sku=328534. This URL is easy to query considering we only have to change one parameter, assuming we have a list of LCBO’s SKU numbers (see Part 3 of this series for how I got the numbers)."
  },
  {
    "objectID": "posts/2023-06-10-lcbo-scraper/index.html#bash-sed-and-regex",
    "href": "posts/2023-06-10-lcbo-scraper/index.html#bash-sed-and-regex",
    "title": "Scraping LCBO Data (Part 2: Product Inventory)",
    "section": "Bash, sed, and regex",
    "text": "Bash, sed, and regex\nThe cURL command is fairly simple:\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1\nwhere the variable “1” will be substituted by the SKU number. I need to process the HTML code delivered before we can extract the JSON. This is done using “pipes” represented by the character “|” without quotes. For example, we can add a subsequent command after cURL called “grep” using a pipe as follows:\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| tac | tac \\\n| grep -m 1 'storeList'\nThe backslash “\\” lets you continue the command on a new line. “grep” lets you search files or, in this case, HTML text piped from the cURL command. The option “-m 1” is equivalent to “–max-count=1” which says to stop searching after the first detection of the pattern ‘storeList’.\nA funny thing happens if we do not include the two piped “tac” commands before grep. The tac command simply reverses the incoming file. Executing two tac commands will return the original cURL output! Read @Kaworu’s for an explanation why this is the case.\nThe sequence of piped commands will return line 4004, which looks like this:\n\"storeList\":[[\"City\",\"Intersection\",\"Address Line 1\",\"Address Line 2\",\"Phone Number\",\"Store Number\",\"Available Inventory\"],[\"St. Catharines\",\"Vansickle & Fourth\",\"420 Vansickle Road\",\"\",\"(905) 685-8000\",\"392\",\"1\"],[\"Kitchener\",\"Fairway & Manitou\",\"655 Fairway Road South\",\"\",\"(519) 894-0710\",\"381\",\"1\"],[\"Lasalle\",\"Malden Rd & Elmdale Ave\",\"1825 Wyoming Avenue\",\"\",\"(519) 250-9847\",\"370\",\"2\"],[\"Toronto-North York\",\"Wilson & Dufferin\",\"675 Wilson Avenue\",\"\",\"(416) 636-5349\",\"360\",\"4\"],[\"Toronto-North York\",\"Bayview & Sheppard (Bayview Village)\",\"2901 Bayview Avenue - Unit 125\",\"BAYVIEW VILLAGE MALL\",\"(416) 222-7658\",\"355\",\"3\"],[\"Dundas\",\"Osler & Main W\",\"119 Osler Drive\",\"UNIT 13A\",\"(905) 628-2042\",\"25\",\"4\"],[\"Toronto-Central\",\"Dupont & Spadina\",\"232 Dupont Street\",\"\",\"(416) 922-7066\",\"15\",\"3\"],[\"Ancaster\",\"Golf Links & Hwy 403 (Meadowlands)\",\"737 Golf Links Road\",\"MEADOWLANDS CENTRE\",\"(905) 304-9608\",\"21\",\"1\"],[\"Burlington\",\"Appleby Line & New St\",\"501 Appleby Line\",\"\",\"(905) 639-0337\",\"641\",\"1\"],[\"Aurora\",\"Leslie & Wellington\",\"94 First Commerce Drive\",\"\",\"(905) 751-0684\",\"658\",\"4\"],[\"Mississauga\",\"Dundas & Winston Churchill (Woodchester)\",\"2458 Dundas Street West\",\"WOODCHESTER MALL\",\"(905) 822-1776\",\"494\",\"1\"],[\"Oakville\",\"Cornwall & Trafalgar\",\"321 Cornwall Road\",\"UNIT C120\",\"(905) 845-8100\",\"486\",\"1\"],[\"Windsor\",\"Tecumseh & Lauzon\",\"7640 Tecumseh Rd. E.\",\"\",\"(519) 944-4014\",\"490\",\"1\"],[\"Toronto-North York\",\"Avenue & Lawrence\",\"1838-1844 Avenue Road\",\"\",\"(416) 785-6389\",\"452\",\"1\"],[\"Newmarket\",\"Leslie & Davis\",\"17365 Leslie Street\",\"\",\"(905) 898-1062\",\"453\",\"1\"],[\"Markham\",\"Hwy 7 & Woodbine\",\"3075 Highway 7 East\",\"\",\"(905) 940-2768\",\"590\",\"9\"],[\"Markham\",\"Hwy 7 & Birchmount\\/village Pkwy\",\"3991 Highway 7\",\"\",\"(905) 479-9612\",\"580\",\"3\"],[\"Markham\",\"Markham Rd & Bur Oak\",\"9720 Markham Road\",\"\",\"(905) 201-9743\",\"585\",\"1\"],[\"Burlington\",\"Walkers Line & Dundas St\",\"3041 Walkers Line\",\"\",\"(905) 331-5792\",\"551\",\"1\"],[\"Kitchener\",\"Highland & Westmount (Highland Rd Plaza)\",\"324 Highland Road West Unit 6\",\"HIGHLAND ROAD PLAZA\",\"(519) 745-8781\",\"549\",\"1\"],[\"Richmond Hill\",\"Yonge & Hwy 7\",\"8783 Yonge Street\",\"\",\"(905) 886-3511\",\"623\",\"5\"],[\"Mississauga\",\"Erin Mills & Eglinton\",\"5100 Erin Mills Parkway\",\"SUITE 5035\",\"(905) 607-7900\",\"619\",\"3\"],[\"Innisfil\",\"Innisfil Beach Rd & 20th Sr\",\"1465 Innisfil Beach Road\",\"UNIT #7\",\"(705) 436-7182\",\"605\",\"1\"],[\"Pickering\",\"Brock & Kingston (Hwy 2)\",\"1899 Brock Rd Unit K3\",\"\",\"(905) 427-9830\",\"776\",\"1\"],[\"Woodbridge\",\"Hwy 27 & Innovation Dr\",\"8260 Hwy. 27\",\"\",\"(905) 264-7366\",\"632\",\"4\"],[\"Richmond Hill\",\"Major Mackenzie & Leslie\",\"1520 Major Mackenzie Dr. E.,\",\"\",\"(905) 884-4700\",\"629\",\"3\"],[\"Milton\",\"Main St & Thompson Rd\",\"830 Main Street East\",\"\",\"(905) 864-7030\",\"631\",\"1\"],[\"King City\",\"King Rd & Dufferin St\",\"1700 King Road\",\"UNIT #55 BUILDING B\",\"(905) 833-0641\",\"671\",\"1\"],[\"Toronto-Central\",\"Queens Quay & Cooper Street\",\"15 Cooper Street\",\"\",\"(416) 864-6777\",\"217\",\"1\"],[\"Aurora\",\"Yonge & Henderson\",\"1 Henderson Drive\",\"\",\"(905) 727-9722\",\"311\",\"1\"],[\"Toronto-Central\",\"Eglinton & Laird\",\"65 Wicksteed Avenue\",\"\",\"(416) 425-6282\",\"164\",\"3\"],[\"Tecumseh\",\"E.c. Row & Manning\",\"15 Amy Croft Drive\",\"\",\"(519) 735-2661\",\"278\",\"1\"],[\"Toronto-North York\",\"Keele & Lawrence (North Park Plaza)\",\"1339 Lawrence Ave. West\",\"NORTH PARK PLAZA\",\"(416) 249-1391\",\"279\",\"1\"],[\"Toronto-North York\",\"Lawrence & Don Mills\",\"195 The Donway West\",\"\",\"(416) 447-0491\",\"253\",\"1\"],[\"Ottawa\",\"Bank & Walkley\",\"1980 Bank Street\",\"\",\"(613) 523-7763\",\"243\",\"1\"],[\"Newmarket\",\"Upper Canada Mall\",\"17600 Yonge Street\",\"\",\"(905) 895-6341\",\"226\",\"3\"],[\"Waterdown\",\"Hwy 5 & Hwy 6\",\"74 Dundas Street East\",\"\",\"(905) 689-6855\",\"326\",\"2\"],[\"Oakville\",\"3rd Line & Rebecca (Hopedale Mall)\",\"1527 Rebecca Street\",\"\",\"(905) 827-5072\",\"437\",\"1\"],[\"Waterloo\",\"King N & Northfield\",\"571 King Street North\",\"\",\"(519) 884-8511\",\"417\",\"1\"],[\"Vaughan\",\"Major Mackenzie & Weston\",\"3631 Major Mackenzie Drive\",\"BUILDING B\",\"(905) 417-1102\",\"397\",\"3\"],[\"Niagara Falls\",\"Portage Rd & Colborne E\",\"3714 Portage Road\",\"\",\"(905) 356-3972\",\"401\",\"1\"],[\"Stouffville\",\"Main St & Mostar St\",\"5710 Main Street Unit 1\",\"\",\"(905) 640-3771\",\"404\",\"1\"]],\nWe could take the above output and, with a little editing, immediately create a JSON. A small snapshot of the JSON is seen below.\n\nThere are a number of practical reasons to not do this. For instance, the only data we require is the store number and available inventory. We know the store information from Part 1 of our series and can call upon it with the store number alone. Throwing away redundant data reduces the file sizes and decreases the read time of each file. Furthermore, the JSON structure can be simpler and much more readable with restructuring! Our goal is to rebuild the JSON to look like this:\n\n94433 is the SKU number. The keys and values are simply the store numbers and available inventory, respectively.\n\nsed\n“sed” is a UNIX stream editor that lets you search, and find and replace. There is a neat online editor that lets you quickly test your ideas: https://sed.js.org/.\nHere is sed piped into action:\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| grep -m 1 'storeList' \\ \n| tac | tac \\\n| sed '$s/,$//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\nOkay… sed is an alien language when you first see it. Let me break it down. To start, the output from grep is piped to sed for processing. The quoted text after sed is actually several search and replace commands concatenated together. The following text\n's/,$//; s/\"storeList\":\\[//g; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\nbreaks down into 4 sed commands separated by semicolons:\n1. 's/,$//'\n2. 's/\\]\\]//g'\n3. 's/\\],\\[/\\n/g'\n4. '/^ $/d'\nAn sed command has four components separated by forward slashes “/”:\n'[s]ubstitute / Regexp / Replacement / [g]lobal'\nThe substitute and global parameters are represented with s and g. Regexp is the regular expression you’re searching for. Replacement is the text that will substitute the regex expression. Below is an explanation for each command. You can follow along by pasting the output the contents of line 4004 to https://sed.js.org/ and applying the commands below.\n\nSubstitute the last comma at the end of the grep output with nothing.\n\n# ,$  &lt;-- the dollar sign $ says to get the last comma\n's/,$//'\n\nGlobally substitute the double closing square brackets with nothing.\n\n\n# Substitute ]] with nothing (i.e., delete ]])\n's/\\]\\]//g'\n\nGlobally substitute inner square brackets with a newline.\n\n# Substitute ],[ with \\n\n's/\\],\\[/\\n/g'\n\nDelete lines with only white spaces or newlines.\n\n'/^ $/d'\nHere is a sample output of the first three lines after running these sed commands:\n\"storeList\":[[\"City\",\"Intersection\",\"Address Line 1\",\"Address Line 2\",\"Phone Number\",\"Store Number\",\"Available Inventory\"\n\"St. Catharines\",\"Vansickle & Fourth\",\"420 Vansickle Road\",\"\",\"(905) 685-8000\",\"392\",\"1\"\n\"Kitchener\",\"Fairway & Manitou\",\"655 Fairway Road South\",\"\",\"(519) 894-0710\",\"381\",\"1\"\n...\n\n\ntail\nThe “tail” command shows the last few lines of a given file. We pipe the output to “tail” and add a +2 option to return the entire text from line 2 onwards (i.e., delete the first line).\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| grep -m 1 'storeList' \\ \n| tac | tac \\\n| sed '$s/,$//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\\\n| tail +2\nHere is the current state of our text file:\n\"St. Catharines\",\"Vansickle & Fourth\",\"420 Vansickle Road\",\"\",\"(905) 685-8000\",\"392\",\"1\"\n\"Kitchener\",\"Fairway & Manitou\",\"655 Fairway Road South\",\"\",\"(519) 894-0710\",\"381\",\"1\"\n...\n\n\nMore sed!\nOur next sed command will take the output and produce the JSON file we desire! Last sed, I promise. Note that we use the -r option with sed to permit extended regular expressions.\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\ \n| grep -m 1 'storeList' \\ \n| tac | tac \\\n| sed '$s/,$//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^$/d'\\\n| tail +2 \\\n| sed -r 's/(.*,)([^,]+,[^,]+)$/\\2/g; s/\"*$//g; s/\",\"/\":/g ; 1s/^/{\"'$1'\":{/; s/$/,/ ; $s/,$/}}/' \n\nGrab the last two columns from each row.\n\ns/(.*,)([^,]+,[^,]+)$/\\2/g;\nReturn:\n\"392\",\"1\"\n\"381\",\"1\"\n...\n\nDelete the last quotation mark in each row.\n\ns/\"*$//g;\nReturn:\n\"392\",\"1\n\"381\",\"1\n...\n\nGlobally replace the regexp (^^^) with the replacement (^^). Sorry, it gets weird referencing punctuations. I’ve added some white spaces for clarity.\n\ns/   \",\"   /    \":     /g\n     ^^^        ^^\nReturn:\n\"392\":1\n\"381\":1\n...\n\nThe start of a JSON needs an open curly bracket. Append the following text with the SKU number to the beginning: {“94433”: {\n\n1s/^/{\"'$1'\":{/; \nReturn:\n{\"94433\":{\"392\":1\n\"381\":1\n...\n\nAdd a comma at the end of each row:\n\ns/$/,/\nReturn:\n{\"94433\":{\"392\":1,\n\"381\":1,\n...\n\nAdd closing brackets at the end of the file to complete the JSON.\n\n$s/,$/}}/\nReturn:\n{\"94433\":{\"392\":1,\n\"381\":1,\n...\n\"404\":1}}\n\n\nMake it pretty with jq and save!\nThe jq command reformats the JSON to look pretty. Finally, we save the final output to a JSON file.\n#!/bin/bash\ncurl --location \"https://www.lcbo.com/en/storeinventory/?sku=\"$1 \\\n| grep -m 1 'storeList' \\\n| tac | tac \\\n| sed '$s/,$//; s/\"storeList\":\\[//; s/\\]\\]//g; s/\\],\\[/\\n/g; /^ $/d' \\\n| tail +2  \\\n| sed -r 's/(.*,)([^,]+,[^,]+)$/\\2/g; s/\"*$//g; s/\",\"/\":/g ; 1s/^/{\"'$1'\":{/; s/$/,/ ; $s/,$/}}/' \\\n| jq . \\\n&gt; json/inventory/$1.json"
  },
  {
    "objectID": "posts/2023-06-10-lcbo-scraper/index.html#loop-the-script-for-each-sku-number",
    "href": "posts/2023-06-10-lcbo-scraper/index.html#loop-the-script-for-each-sku-number",
    "title": "Scraping LCBO Data (Part 2: Product Inventory)",
    "section": "Loop the script for each SKU number:",
    "text": "Loop the script for each SKU number:\nIn Part 3, I obtain a list of SKU numbers for wines and non-wine beverages. In the script below, the script runs a loop for each SKU number in the file and executes the script made above. The “chmod” command changes permissions to permit write access. We add a sleep command of 1 second to scrape more gently.\n#!/bin/bash\nchmod +x code/inventory_request.sh\n\nwhile IFS= read -r line; do\n    sudo ./code/inventory_request.sh $line\n    sleep 1\ndone &lt; json/wine_sku_list.txt\n\nwhile IFS= read -r line; do\n    sudo ./code/inventory_request.sh $line\n    sleep 1\ndone &lt; json/nonwine_sku_list.txt"
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#what-is-github-actions",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#what-is-github-actions",
    "title": "Making my own scraper bot!",
    "section": "What is GitHub Actions?",
    "text": "What is GitHub Actions?\nGitHub Actions is a platform to launch “workflows” with your GitHub repository. These workflows can be automated to build, test, and deploy your code directly from GitHub. Workflows can have create numerous jobs that run in parallel. They can also create temporary branches for testing your build’s functionality and performance.\n\n\n\n\n\nThank you, GitHub!\n\n\nThere are great guides and documentation on GitHub Actions."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-strategy",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-strategy",
    "title": "Making my own scraper bot!",
    "section": "The strategy",
    "text": "The strategy\nIn my three-part blog series, I describe three scrapers I wrote for the LCBO website. These scrapers obtain several thousands of data products as JSON files. I have one workflow executing each scraper along with a python script for post-processing. Each workflow performs the following sequence of actions (more or less in the same order):\n\nWait for an event to trigger the workflow.\nCreate a new branch.\nGo to the new branch.\nRun the scraping script.\nInstall python modules and do python post-processing.\nCommit and push changes to the new branch.\nGo to the main branch.\nPull new branch and then delete it.\n\nCreating a temporary branch is very helpful when you are first starting to experiment with workflows. Your workflows will fail. Scripts will stop working. A temporary branch lets you run your script and inspect the results without threatening your main branch. I suggest ignoring steps 7 and 8 from the above sequence until you are confident with your workflow.\n\n\n\nSimple diagram of my workflow\n\n\n\nWorkflow solution\nBelow is the strategy written as a workflow. The workflow is sequential, and each sequence is described in the rest of the blog.\n\n\n\n\n\nYou can flatten the code.\n\n\n\n\n\n\n\n\nworkflow solution: .github/workflows/workflow_products.yml\n\n\n\n\n\nname: Product Description Scraper\n\non:\n  push:\n  workflow_dispatch:\n  schedule:\n    - cron:  '7 7,19 * * 0'\n\njobs:\n  my-job:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Check out this repo\n      uses: actions/checkout@v3\n\n    - name: Create branch\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git checkout -b workflow_products_output_singlejob\n        git push -u origin workflow_products_output_singlejob\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: workflow_products_output_singlejob\n\n    - name: Fetch latest data.\n      run: |-\n          chmod +x code/all_product_request.sh\n          sudo ./code/all_product_request.sh\n      shell: bash\n\n    - name: Install python modules. Upload to Autovino server.\n      env: \n        AUTOVINO_KEY: ${{secrets.AUTOVINO_KEY}}\n      run: |-\n        python -m pip install --upgrade pip\n        pip install -r code/requirements/requirements.txt\n        python code/create_master_json_and_skulist_txt.py\n        python code/updateTables.py prices \n      shell: bash\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git branch\n        git pull \n        git add json/*\n        timestamp=$(date -u)\n        git commit -m \"Latest data: ${timestamp}\" || exit 0\n        git push\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: main\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git config pull.rebase true\n        git pull --allow-unrelated-histories origin workflow_products_output_singlejob \n        git push origin main\n        git push origin --delete workflow_products_output_singlejob\n\n\n\n\n\n\n\n\n\nWarning 1: Workflow experimentation\n\n\n\n\n\nWorkflows can have a steep learning curve! I strongly encourage workflow experimentation and debugging to be done in a separate branch and not in main.\n\n\n\n\n\n\n\n\n\nWarning 2: Change read/write permissions\n\n\n\n\n\nMy workflow will crash because GitHub Actions does not have read/write permissions by default. Go to Settings &gt; Actions-General &gt; Workflow permissions, select Read and write permissions, select Allow GitHub Actions to create and approve pull requests, and save."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-yaml",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-yaml",
    "title": "Making my own scraper bot!",
    "section": "The YAML",
    "text": "The YAML\nI suggest creating a new private github repository to follow along. You can save the upcoming workflow snippets to your workflows directory and follow along. Workflows must must exist in this directory:\n.github/workflows/\n\nName and events (Part 1)\nWorkflows are written in YAML (*yml), begin with a name (e.g., “Product Description Scraper”), and a list of triggers:\n\n\n\n\n\n\nworkflow snippet: names and events\n\n\n\n\n\nname: Product Description Scraper\n\non:\n  push:\n  workflow_dispatch:\n  schedule:\n    - cron:  '7 7,19 * * 0'\n\n\n\nThis workflow is triggered when one of three events types occur:\n\npush: a push to the branch has been made.\nworkflow_dispatch: a manually activated trigger (see below).\nschedule: a scheduled event occurs 1.\n\nSee this list of all other event types.\nAfter saving the above workflow to your repository, go to the Actions tab. There, you can monitor your workflow. Any YAML files in your workflow directory will be displayed in this tab.\n\nUnfortunately, the workflow is not executable because it is incomplete. The YAML file is shown by its filename rather than the workflow name. We need to provide a job with at least one step. Append the following code, save, and refresh the workflow monitor.\n\n\n\n\n\n\nworkflow snippet: jobs\n\n\n\n\n\njobs:\n  my-job:\n    runs-on: ubuntu-latest #choose an OS\n    steps:\n    - name: Check out this repo\n      uses: actions/checkout@v3\n\n\n\nYou should see the label change from your filename to the name “Product Description Scraper”. \nA Run workflow button is available because the workflow_dispatch event is included as a trigger. This button does not display by default.\n\n\n\n\n\n\nworkflow_dispatch button\n\n\n\n\n\n\n\n\n\nTry clicking on the Run workflow trigger! I will talk more the display below.\n\n\nJobs (Part 2)\nWorkflows execute jobs. Jobs are sent to a queue and are picked up by “runners”. Runners are machines that will execute your job when not busy. Let’s look at the first few steps in my workflow’s job called my-job.\n\n\n\n\n\n\nworkflow snippet: jobs\n\n\n\n\n\njobs:\n  my-job:\n    runs-on: ubuntu-latest #choose an OS\n    steps:\n    - name: Check out this repo\n      uses: actions/checkout@v3\n\n\n\nHere, I specify needing a runner using ubuntu-latest (i.e., Ubuntu 22.04). A list of OSs can be found here. Runners using Windows and macOS cost 2x and 10x more computing minutes than Linux (ref)!\nA job contains a sequence of tasks called steps. Each step has a label called name. My first step uses a pre-built action called CheckoutV3 which “checks out” my respository, specifically the main branch by default. This lets me access and do things with the repository. This action is available in a GitHub Actions Marketplace where authors host free or paid actions for others to use in their workflow!\nI need CheckoutV3 so I can duplicate the main branch. This is done in the step below.\n\n\nCreate a branch and web scrape (Part 3)\n\n\n\n\n\n\nworkflow snippet: create a branch\n\n\n\n\n\n    - name: Create branch\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git checkout -b workflow_products_output_singlejob\n        git push -u origin workflow_products_output_singlejob\n\n\n\nThe first two commands git config specify the username and email, so git can report who is messing around and how to contact them. Next, I create a new branch with git checkout -b and name the branch “workflow_products_output_singlejob”. I then upload the repository I am “checking out” (main) to the newly created one (workflow_products_output_singlejob) using git push. The following -u origin workflow_products_output_singlejob option lets me later use git pull without ambiguity.\nI use CheckoutV3 again to check out the new branch I made. In this new branch, I execute the scraping script to collect new data products.\n\n\n\n\n\n\nworkflow snippet: check out the new branch\n\n\n\n\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: workflow_products_output_singlejob\n\n    - name: Fetch latest data.\n      run: |-\n          chmod +x code/all_product_request.sh\n          sudo ./code/all_product_request.sh\n      shell: bash\n\n\n\n\n\n\n\n\n\nMinimize work\n\n\n\n\n\nThe goal is to create a functioning workflow first. Do not start scraping the entire catalog while constructing and debugging your workflow. Try scraping a couple products max.\n\n\n\n\n\nGet my secret key and do python (Part 4)\nI need to process the raw data products before uploading them to my postgreSQL data base. I use a python script which needs the modules listed in requirements.txt. The first python script processes the raw JSONS and combines them into a single JSON with new features. The second script updates tables in my postgreSQL database.\n\n\n\n\n\n\nworkflow snippet: python and postgreSQL\n\n\n\n\n\n    - name: Install python modules. Upload to Autovino server.\n      env: \n        AUTOVINO_KEY: ${{secrets.AUTOVINO_KEY}}\n      run: |-\n        python -m pip install --upgrade pip\n        pip install -r code/requirements/requirements.txt\n        python code/create_master_json_and_skulist_txt.py\n        python code/updateTables.py prices \n      shell: bash\n\n\n\nThe external URL for my postgreSQL database is protected! GitHub lets you create secret keys that is hidden from the public and accessible to your environment. I define an environment variable AUTOVINO_KEY to be my secret key secrets.AUTOVINO_KEY with my database’s external URL. This key is read in my python script using\nAUTOVINO_KEY = os.environ['AUTOVINO_KEY']\n\n\nWrap it up (Part 5)\nThe remaining steps are straightforward. I add the json folder containing the data files, commit, and push to the new branch. I switch to (or “check out”) the main branch and pull the new branch before deleting it.\n\n\n\n\n\n\nworkflow snippet: commit and push, pull and delete new branch\n\n\n\n\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git branch\n        git pull \n        git add json/*\n        timestamp=$(date -u)\n        git commit -m \"Latest data: ${timestamp}\" || exit 0\n        git push\n\n    - name: Check out this repo\n      uses: actions/checkout@v3\n      with:\n        ref: main\n\n    - name: Commit and push if it changed\n      run: |-\n        git config user.name \"Automated\"\n        git config user.email \"actions@users.noreply.github.com\"\n        git config pull.rebase true\n        git pull --allow-unrelated-histories origin workflow_products_output_singlejob \n        git push origin main\n        git push origin --delete workflow_products_output_singlejob\n\n\n\nThat’s it! That’s the workflow behind my scraper bot.\n\nTips:\n\nIf a workflow fails after a new branch is created, the new branch will continue to exist. You must delete this branch before running another workflow.\nIf you do NOT want to save the raw or processed data (because they are uploaded to your database) then you simply delete the branch after you have what you need."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#workflow-monitor",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#workflow-monitor",
    "title": "Making my own scraper bot!",
    "section": "Workflow Monitor",
    "text": "Workflow Monitor\nThe monitor is vital to debugging your workflow. Try dispatching the workflow now. You can see it added to queue under your Actions tab. The orange circle indicates the workflow is in queue or running.\n\nWhen a workflow fails, the indicator becomes a crossed red circle. You can setup notifications settings to receive an email when this occurs in case things fail unexpectedly.\n\nWe can inspect the failure by clicking on the workflow to show a GUI diagram of the jobs requested. Here, “job_0” is my-job in the workflow script. The GUI can have a graph of connecting jobs and lets you zoom in and out. My script only has one job!\n\nClicking on the job shows a list of steps with names we defined using name. We can inspect each step and see whether they performed as desired. The failure here is “remote: Write access to repository not granted.” This is because I skipped my warning about giving GitHub Actions read and write permissions. As a result, a new branch was not created and the workflow shut down."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-matrix",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-matrix",
    "title": "Making my own scraper bot!",
    "section": "The Matrix",
    "text": "The Matrix\nMy original ambition for my workflow was to make it fast. Create dozens of jobs. Execute a cluster of scrape commands for each job. Go forth, my minions! Pillage the servers!\n\n\n\nAzog and his job requests. Source: The Hobbit: The Battle of the Five Armies.\n\n\nNot only was this strategy greedy and unnecessary, it took spectacular effort and cost 50x more minutes and computational resources! Here is why.\nIn GitHub Actions, you can execute a multitude of identical jobs where each job has a different parameter or initial value. These parameters come from a “matrix” (i.e., a list) of values. In my case, I wanted to assign a scraping job for each parameter in my matrix2. Feel free to read more about matrices here. You can specify the maximum number of jobs that run simultaneously too (ref).\nAfter experimenting with the matrix strategy, it was clearly a very bad solution for several reasons:\n\nDefining a matrix that is more than one dimension causes hair loss.\nConstantly referring to these as matrices hurts me.\nIf you have a list of unequal lists to query then too bad. A matrix must be a list of equal length lists. Elements that do not fit in the matrix must bad added as individual special scenarios. smile\nThe minutes used for each job is rounded up and summed. That means 10 jobs that take 1 second each will cost 10 minutes. Meanwhile, 1 job that performs ten 1-second tasks will cost 1 minute.\n\n\n\n\nThe last reason was a complete shock to me. With all my scrapers, I could only afford one or two scrapings per month each! (You can see your usage in the workflows monitor or your GitHub settings under “Billing and plans”.)\nI rewrote my entire workflow and wrote another bash script to loop over my list of queries. This led to a factor of 50x fewer minutes spent. The savings affords me to run more frequent workflows and have more up-to-date data.\nYes, the total time for each workflow is longer in series than in parallel. The headaches avoided from understanding and implementing matrices makes this completely worth it. Also, the scraping is much gentler on the servers too."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-main-branch",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#the-main-branch",
    "title": "Making my own scraper bot!",
    "section": "The Main Branch",
    "text": "The Main Branch\nIn my mad dash for power, it had not occurred to me to create a separate branch where my scraping could be done. My original workflow ran in the main branch. Another silly decision was to commit and push after each scraping job when I implemented the matrix strategy. As a result, my main branch had a flurry of updates that prevented me from making separate commits of my own.\n\n\n\n\n\nDon’t code where you eat.\n\n\nSo, don’t do that. Create a temporary branch to play in and delete it when you’re done."
  },
  {
    "objectID": "posts/2023-06-22-lcbo-github-actions-bot/index.html#footnotes",
    "href": "posts/2023-06-22-lcbo-github-actions-bot/index.html#footnotes",
    "title": "Making my own scraper bot!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI use “crontab guru” to make a cron schedule. For example, ‘7 7,19 * * 0’ corresponds to Sundays at 7:07 am and pm. ↩︎\nI am not a fan of the term “matrix”. Not one bit.↩︎"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html",
    "href": "posts/2023-07-05-test/sentimentModels.html",
    "title": "my bert thing",
    "section": "",
    "text": "Code\n\n\nimport json\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom itertools import chain\nimport re\nimport numpy as np\nfrom glob import glob\n\nimport pandas as pd\nCode\nimport emoji\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef remove_emoji(text):\n    return emoji.replace_emoji(text)\n\n\ndef refine_text(s):\n    s = re.sub(r\"[^a-zA-Z\\d\\s'’$]\", ' ', s)\n    return s\n\n\ndef replace_exclaimation(s):\n    s = re.sub(r\"!\", '.', s)\n    return s\n\n\ndef remove_fullstops_and_vline(s):\n    s = re.sub(r\"\\.+\", '.', s)\n    s = re.sub(r\"\\|\", ' ', s)\n    s = re.sub(r\"\\n\", ' ', s)\n    return s\n\n\ndef tokenize_lemma(text):\n    text = refine_text(text)\n    return [\n        re.sub('[^A-Za-z0-9]+', '',\n               w.lemma_.lower().strip()).strip() for w in nlp(text)\n    ]\n\n\ndef decontracted(phrase):\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n\ndef tokenize_price(s, token='price'):\n\n    #Find \"$10, replace it with token\"\n    dollar_word = re.search(r'\\w*\\$\\d+(?:\\.\\d+)?\\w*',  s)  # \"not bad for $10AUD.\" --&gt; \"$10AUD\"\n    dollar_word2 = re.search(r'\\w*\\d+(?:\\.\\d+)?\\$\\w*', s)  # \"not bad for $10AUD.\" --&gt; \"$10AUD\"\n\n    if not dollar_word:\n        dollar_word = dollar_word2\n\n    if not dollar_word:\n        return s\n\n    dollar_word = dollar_word.group(0)\n    s = s.replace(dollar_word, token)  #replace price with token\n\n    #Return price-tokenized review.\n    return s\n\n\nrewriteDict = {\n    'Positive': 'Positive',\n    'Neutral': 'Negative',\n    'Negative': 'Negative',\n    \"Don't know\": \"Don't know\"\n}\nCode\ndirIn = \"D:\\\\vivino_reviews\\\\survey_results\\\\\"\nfilename = dirIn + \"labelled_surveys_no_ratings.csv\"\ndf_old = pd.read_csv(filename, encoding='utf-8-sig')\ndf_old = df_old[~df_old['sentiment'].isna()]\nCode\ndirIn = \"D:\\\\vivino_reviews\\\\survey_results\\\\\"\nfilename = dirIn + \"labelled_surveys.csv\"\ndf_new = pd.read_csv(filename, encoding='utf-8-sig')\ndf_new = df_new[~df_new['sentiment'].isna()]\nCode\ndf = pd.concat([df_new,df_old],ignore_index=True)[['reviews','sentiment']]\n\nmaxCharLength = 350\nminCharLength = 0\ndf['length'] = df['reviews'].apply(len)\ndf_long = df[df['length'] &gt; maxCharLength]\ndf = df[df['length'] &lt;= maxCharLength]\ndf = df[df['length'] &gt;= minCharLength]\n\nfrom sklearn.preprocessing import LabelEncoder, label_binarize\nfrom sklearn.model_selection import train_test_split\n\ndf['sentiment'] = df['sentiment'].apply(lambda x: rewriteDict[x])\ndf['labels'] = LabelEncoder().fit_transform(df['sentiment'])\n\n\ndf['reviews'] = df['reviews'].apply(remove_emoji)\n#df['reviews'] = df['reviews'].apply(remove_fullstops_and_vline)\n#df['reviews'] = df['reviews'].apply(decontracted)\n#df['reviews'] = df['reviews'].apply(tokenize_price)\n\ndf['sentiment'].value_counts()\n#df = df[df['sentiment'] != \"Don't know\"]\n\n\nsentiment\nPositive      3417\nNegative      1558\nDon't know     658\nName: count, dtype: int64\nCode\nseed = 4223123\nX_train, X_test, y_train, y_test = train_test_split(df, df['sentiment'], train_size=0.8, random_state=seed)\nprint(f'Training set size: {len(X_train)}')\nprint(f'Testing set size: {len(X_test)}')\nX_train['sentiment'].value_counts()\n\n\nTraining set size: 1689\nTesting set size: 3944\n\n\nsentiment\nPositive      1001\nNegative       499\nDon't know     189\nName: count, dtype: int64\nCode\ndirIn = \"D:/vivino_reviews/\"\nfname = dirIn + \"allEnglishReviews.json\"\nwineReviews = json.load(open(fname,'r'))\nallReviews = [list(r.values()) for r in wineReviews.values()]\nallReviews = list(chain(*allReviews))\nallReviews = [r for r in allReviews if r and len(r.strip())&gt;0]\n\n\nKeyboardInterrupt:\nCode\nimport random\nnumSample = X_train['sentiment'].value_counts()['Positive'] - X_train['sentiment'].value_counts()[\"Don't know\"]\nnumSample = numSample\nreviewsDK = random.sample(allReviews, numSample)\nreviewsDK = [r+ \" $50\" for r in reviewsDK]\ndfDK = pd.DataFrame({'reviews':reviewsDK, 'sentiment': [\"Don't know\"]*numSample})\nCode\nX_train = pd.concat([X_train, dfDK],ignore_index=True)[['reviews','sentiment']]\ny_train = pd.concat([y_train, dfDK['sentiment']],ignore_index=True)\nCode\nX_train['sentiment'].value_counts()"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html#step-1-pre-processing-text",
    "href": "posts/2023-07-05-test/sentimentModels.html#step-1-pre-processing-text",
    "title": "my bert thing",
    "section": "Step 1: Pre-processing text",
    "text": "Step 1: Pre-processing text"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html#undersample-and-oversample",
    "href": "posts/2023-07-05-test/sentimentModels.html#undersample-and-oversample",
    "title": "my bert thing",
    "section": "Undersample and oversample",
    "text": "Undersample and oversample\n\n\nCode\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nUS = RandomUnderSampler(random_state=0)\nusx, usy = US.fit_resample(X_train, y_train)\n\nOS = RandomOverSampler(sampling_strategy='auto', random_state=0)\nosx, osy = OS.fit_resample(X_train, y_train)\n\n\n\n\nCode\n#knowDict = {'Positive': 'Know', 'Negative': 'Know', \"Don't know\": \"Don't know\"}\n#df['sentiment'] = df['sentiment'].apply(lambda x: knowDict[x])\n#df['sentiment'].value_counts()"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html#sentence-augmentation",
    "href": "posts/2023-07-05-test/sentimentModels.html#sentence-augmentation",
    "title": "my bert thing",
    "section": "Sentence Augmentation",
    "text": "Sentence Augmentation\n\n\nCode\nfrom nltk import sent_tokenize\nimport random\ndef sentence_shuffle(text, n = None):\n    \n    #Add a period if missing finalstop. \n    text = text.strip()\n    if text[-1] != \".\" and text[-1] != \"!\":\n        text = text + '.'\n        \n    d = sent_tokenize(text)\n    numSentences = len(d)\n    augSentences = [' '.join(d)]\n \n    if numSentences == 1:\n        return augSentences\n\n    if not n:\n        n = numSentences\n        \n    for i in range(n):\n        newText = ' '.join(random.sample(d, numSentences))\n        if newText not in augSentences:\n            augSentences.append(newText)\n    \n    return augSentences\n\n\n\n\nCode\nreviews = X_train['reviews']\nsentiment = X_train['sentiment']\n\n\n\n\nCode\naugReviews = []\naugSentiment = []\n\nfor i, review in enumerate(X_train['reviews']):\n    if X_train['sentiment'].iloc[i] == \"Positive\":\n        augSentences = [review]\n    else:\n        augSentences = sentence_shuffle(review, 2)\n        \n    augReviews.append(augSentences)\n    augSentiment.append([X_train['sentiment'].iloc[i]]*len(augSentences))\n    \n    \n\n\n\n\nCode\nprint(len(augReviews), len(augSentiment))\naugReviews = list(chain(*augReviews))\naugSentiment = list(chain(*augSentiment))\nprint(len(augReviews), len(augSentiment))\n\nX_train  = pd.DataFrame({'reviews': augReviews})\ny_train  = pd.Series(augSentiment)\ny_train.value_counts()\n\n\n\n\nCode\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nUS = RandomUnderSampler(random_state=0)\nusx, usy = US.fit_resample(X_train, y_train)\n\nOS = RandomOverSampler(sampling_strategy='auto', random_state=0)\nosx, osy = OS.fit_resample(X_train, y_train)\n\nlen(osx)"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html#star-rating-sentiment-guide",
    "href": "posts/2023-07-05-test/sentimentModels.html#star-rating-sentiment-guide",
    "title": "my bert thing",
    "section": "Star rating sentiment guide",
    "text": "Star rating sentiment guide\n\n\nCode\ndirIn = \"E:/vivino_reviews/\"\nfname = dirIn + \"allEnglishReviews_PricesRatingsOnly_unique.json\"\nwineReviews = json.load(open(fname,'r'))\n\n\n\n\nCode\nreviews = [r['note'].strip() for r in wineReviews.values()]\nratings = [r['rating'] for r in wineReviews.values()]\n\n\n\n\nCode\ndf_all = pd.DataFrame({'reviews':reviews, 'ratings':ratings})\ndf_all = df_all[(df_all['ratings']&lt;=2) | (df_all['ratings']&gt;=4.9) ]\ndf_all['sentiment'] = df_all['ratings'].apply(lambda x: 0 if x&lt;3 else 1)\ndf_all['reviews_edit'] = df_all['reviews']\n\n\n\n\nCode\ndf_all\n\n\n\n\nCode\ndf_all['sentiment'] = df_all['sentiment'].apply(lambda x: [\"Negative\",\"Positive\"][x])\ndf_all['sentiment'].value_counts()\n\n\n\n\nCode\n['d','a'][0]\n\n\n\n\nCode\n#X_train, X_test, y_train, y_test = train_test_split(df_all, df_all['sentiment'], train_size=0.8, random_state=seed)"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html#bag-of-words-results",
    "href": "posts/2023-07-05-test/sentimentModels.html#bag-of-words-results",
    "title": "my bert thing",
    "section": "Bag of words Results",
    "text": "Bag of words Results\n\n\nCode\nsvm_pipe = Pipeline([\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2), \n                         tokenizer=tokenize_lemma,  )),\n                    ('model',   SVC(kernel='linear',probability=True, class_weight='balanced'))\n                    ])\n\nest = svm_pipe\n\nX = df_old['reviews']\ny = df_old['sentiment']\n\nseed = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=seed)\n\nprint(f'Training on {len(X_train)} data.')\nest.fit(X_train, y_train)\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\n\n\n\n\nCode\nsvm_pipe = Pipeline([\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2), \n                         tokenizer=tokenize_lemma,  )),\n                    ('model',   SVC(kernel='linear',probability=True, class_weight='balanced'))\n                    ])\n\nest = svm_pipe\n\nX = df_new['reviews']\ny = df_new['sentiment']\n\nseed = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=seed)\n\nprint(f'Training on {len(X_train)} data.')\nest.fit(X_train, y_train)\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\n\n\n\n\nCode\nsvm_pipe = Pipeline([\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2), \n                         tokenizer=tokenize_lemma,  )),\n                    ('model',   SVC(kernel='linear',probability=True, class_weight='balanced'))\n                    ])\n\nest = svm_pipe\n\nX = df['reviews']\ny = df['sentiment']\n\nseed = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=seed)\n\nprint(f'Training on {len(X_train)} data.')\nest.fit(X_train, y_train)\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\n\n\n\n\nCode\nsvm_pipe = Pipeline([\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2), \n                         tokenizer=tokenize_lemma,  )),\n                    ('model',   SVC(kernel='linear',probability=True, class_weight='balanced'))\n                    ])\n\nest = svm_pipe\n\nX = df['reviews']\ny = df['sentiment']\n\nseed = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=seed)\n\nprint(f'Training on {len(X_train)} data.')\nest.fit(X_train, y_train)\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\n\n\n\n\nCode\nsvm_pipe = Pipeline([\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2), \n                         tokenizer=tokenize_lemma,  )),\n                    ('model',   SVC(kernel='linear',probability=True, class_weight='balanced'))\n                    ])\n\nest = svm_pipe\n\nX = df['reviews']\ny = df['sentiment']\n\nseed = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=seed)\n\nprint(f'Training on {len(X_train)} data.')\nest.fit(X_train, y_train)\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\n\n\n\n\nCode\n#print(confusion_matrix(y_test, y_pred))\n\n\n\nOld survey results. Delete neutral and don’t know.\n\n\n\nimage.png\n\n\n\n\nOld survey results. Negative = {Negative, Neutral}\n\n\nCode\ndf['reviews_edit'].lower()\n\n\n\n\nCode\ns = \"4.2. While young, it’delicious! 😋🙌🏻. Had 2 bottles- first went down smooth and saft, I mean fast! 😋🥳! 2nd: Wonderful oak, mildly bitter tannins; jammy, dark berry prominent at first, soon vanilla and spice (everything nice later 👍🏻🙃🤣). Got spicier (baking to pepper), less bitter, chewier, and a longer finish with decant. Later, powder cocoa and 😋 cola! 🙌🏻 @1hr: 5😋’s! - vanilla pops! .. Artemis worth BJ’s $53. Will get better with age!  Happy NY! 🥂🍾🥂! Here’s to your BEST year ever! 🤜🏻💥🤛🏼 Cheers friends! 🎊🍷😋!\"\nprint(s)\nprint()\nprint(decontracted(remove_fullstops_and_vline(remove_emoji(s))))\n\n\n\n\n\nimage.png\n\n\n\n\nNew survey. Negative = {Negative, Neutral}\n\n\nCode\nsentence_model.encode(s), sentence_model.encode(s.lower())\n\n\n\n\n\nimage.png\n\n\n\n\nCode\nimport pandas as pd\nfrom sklearn.base import TransformerMixin\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model_name = \"all-MiniLM-L6-v2\"\nsentence_model = SentenceTransformer(embedding_model_name)\n\n\n\n\nCode\nsentence_model.encode([\"hello dogs\", \"hello cats\"], show_progress_bar=True)"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html#setup",
    "href": "posts/2023-07-05-test/sentimentModels.html#setup",
    "title": "my bert thing",
    "section": "Setup",
    "text": "Setup\n\n\nCode\nfrom sentence_transformers import SentenceTransformer\nembedding_model_name = \"all-MiniLM-L6-v2\"\n#embedding_model_name = \"all-distilroberta-v1\"\n#embedding_model_name = \"all-mpnet-base-v2\"\nembedding_model_name = \"all-roberta-large-v1\"\n#embedding_model_name = \"sentence-t5-xxl\" #too big\nsentence_model = SentenceTransformer(embedding_model_name)\nsentence_model.get_sentence_embedding_dimension()\n\n\n1024\n\n\n\n\nCode\n#from spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, precision_score, classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom imblearn.over_sampling  import RandomOverSampler\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.decomposition import PCA\n\n\n\nclass ToDataFrame(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return pd.DataFrame(X)\n\n\nclass PCAEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        embeddings = PCA().fit_transform(X)\n        return embeddings\n\nclass BERTEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        embeddings = sentence_model.encode(list(X.iloc[:,0]), \n                                          normalize_embeddings = True)\n        return embeddings\n     \ndef my_kernel(X, Y):\n    return np.dot(X, Y.T)  \n\nBRTransformer = ColumnTransformer([\n    ('BERTEncoder_transformer', BERTEncoder(), ['reviews'] ),\n    #('LengthEncoder', ToDataFrame(), ['length'] ),\n    #('ratings_transformer', ToDataFrame(), ['rating']),\n    ])\n\nsvm_pipe = Pipeline([\n                    ('BRTransformer', BRTransformer),\n                    #('SVC',   SVC(kernel=my_kernel, probability=True, class_weight='balanced',cache_size=4000)),\n                    ('SVC',   SVC(kernel='poly', \n                                  degree=3, \n                                  probability=True, \n                                  class_weight='balanced',\n                                  cache_size=4000,\n                                 )),\n                    ])\n\nknc_pipe = Pipeline([\n                    ('BRTransformer', BRTransformer),\n                    #('ToDataFrame', ToDataFrame()),\n                    ('KNeighborsClassifier', KNeighborsClassifier(n_jobs=8)),\n                    #('SVC',   SVC(kernel='poly',probability=True, class_weight='balanced',cache_size=4000)),\n                    ])\n\nsgd_pipe = Pipeline([\n                    ('BRTransformer', BRTransformer),\n                    ('SGD classifier', SGDClassifier(max_iter=50, loss='modified_huber', alpha=0.0005)),\n                    ])\n\nmnb_pipe = Pipeline([\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n                    ('classifier', MultinomialNB()),\n                    ])\nrf_pipe = Pipeline([\n                    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n                    ('classifier', RandomForestClassifier(n_estimators=300, min_samples_leaf= 5)),\n                    ])"
  },
  {
    "objectID": "posts/2023-07-05-test/sentimentModels.html#models",
    "href": "posts/2023-07-05-test/sentimentModels.html#models",
    "title": "my bert thing",
    "section": "Models",
    "text": "Models\n\nMultinomial NB\n\n\nCode\nest = mnb_pipe\n\nprint(f'Training on {len(osx)} data.')\n#est.fit(usx['reviews_edit'], usy)\nest.fit(osx['reviews'], osy)\nprint('SGD score:',est.score(X_test['reviews'], y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test['reviews'])\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test['reviews'], y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\nNameError: name 'osx' is not defined\n\n\n\n\nStochastic Gradient Descent\n\n\nCode\nest = sgd_pipe\n\n\nprint(f'Training on {len(osx)} data.')\n#est.fit(X_train, y_train)\n#est.fit(usx, usy)\nest.fit(osx, osy)\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nK nearest neighbors\n\n\nCode\nest = knc_pipe\n\nprint(f'Training on {len(osx)} data.')\n#est.fit(usx, usy)\nest.fit(osx, osy)\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nRandom Forest Classifier\n\n\nCode\nest = rf_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {len(osy)} data.')\nest.fit(osx, osy)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nSVM\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\nTraining on 1689 data.\nSGD score: 0.7743407707910751\n::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::\n              precision    recall  f1-score   support\n\n  Don't know       0.44      0.54      0.49       469\n    Negative       0.76      0.73      0.75      1059\n    Positive       0.86      0.84      0.85      2416\n\n    accuracy                           0.77      3944\n   macro avg       0.69      0.70      0.69      3944\nweighted avg       0.78      0.77      0.78      3944\n\n\n\n\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\ncount = 0\nfor i,r in enumerate(X_test['reviews']):\n    if y_test.iloc[i] != y_pred[i]:\n        print(y_test.iloc[i], y_pred[i])\n        print(r)\n        print('-----')\n        print()\n        count += 1\nprint(count, len(X_test))\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\nfor i,r in enumerate(X_test['reviews']):\n    if y_test.iloc[i] != y_pred[i]:\n        print(y_test.iloc[i], y_pred[i])\n        print(r)\n        print('-----')\n        print()\n        \n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\n#est.fit(X_train, y_train)\nest.fit(osx, osy)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n# two categories only. pos, neg\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n#new seed\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - 350 max characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\n\n\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - all characters - emoji+fullstop+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\n\n\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - all characters - no tokenization :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\n\n\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint('::::::: poly3 - all characters - emoji+fullstop+decontract+price :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train, y_train)\n\n\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::BEST DEGREE-3 POLY results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\nSVM results - OVR (default is one vs rest)\n\n\nCode\nprint('::::::: Kernel Linear, 0.8 training,  :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nprint('::::::: Kernel Poly(deg=3), 0.8 training,  :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nprint('::::::: Kernel SIGMOID, 0.8 training,  :::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nsep = 200\nX_test_short = X_test[X_test['length'] &lt;= sep]\ny_test_short = y_test[X_test['length'] &lt;= sep]\nX_test_long = X_test[X_test['length'] &gt; sep]\ny_test_long = y_test[X_test['length'] &gt; sep]\nlen(X_test_short), len(X_test_long)\n\n\n\n\nCode\nprint(len(X_test_short))\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test_short)\nprint(classification_report(y_test_short, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test_short, y_test_short,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nprint(len(X_test_long))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test_long)\nprint(classification_report(y_test_long, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test_long, y_test_long,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nfor i,r in enumerate(X_test):\n    if y_test.iloc[i] != y_pred[i]:\n        print(y_test.iloc[i], y_pred[i])\n        print(r)\n        print('-----')\n        print()\n        \n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train[0:index_limit], y_train[0:index_limit])\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train[0:index_limit], y_train[0:index_limit])\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\nX_train\n\n\n\n\nCode\ntfidfTransformer.fit_transform(X_train)\n\n\n\n\nCode\nest = svm_pipe\n\npercent_cut = 1\nindex_limit = int(percent_cut*len(X_train))\n\nprint(f'Training on {index_limit} data.')\nest.fit(X_train[0:index_limit], y_train[0:index_limit])\nprint('SGD score:',est.score(X_test, y_test))\n\nprint(':::::::SGD Classifier results:::::::')\ny_pred = est.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=est.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(est, X_test, y_test,\n    display_labels=est.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\n\nDummy classifier\n\n\nCode\nX_test['reviews_edit']\n\n\n\n\nCode\nfrom sklearn.dummy import DummyClassifier\nprint(':::::::Dummy Classifier results:::::::')\ndummy_clf = DummyClassifier(strategy=\"stratified\")\ndummy_clf.fit(X_train['reviews'], y_train)\nprint('Dummy score:',dummy_clf.score(X_test['reviews'], y_test))\nprint(classification_report(y_test, dummy_clf.predict(X_test['reviews']), target_names=dummy_clf.classes_))\n\ndisp = ConfusionMatrixDisplay.from_estimator(dummy_clf, X_test['reviews'], y_test,\n    display_labels=dummy_clf.classes_, cmap=plt.cm.Blues, normalize=\"true\",\n)\ndisp.ax_.get_images()[0].set_clim(0, 1)\n\n\n\n\nCode\n(X_train, y_train)"
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html",
    "href": "posts/2023-07-06-vivino-part1/index.html",
    "title": "31 Million Wine Reviews",
    "section": "",
    "text": "Vivino is a very popular wine app containing millions of wines. Many wines have over 100k+ reviews!\nI noticed approximately 0.5% (~1 in 200) of reviews contain the price a user paid and their sentiment for that price. An example of reviews with “price-sentiment” are shown in the diagram below.\n\n\n\nAn overview of my price-sentiment algorithm.\n\n\nMy idea is to crowdsource price-sentiments to judge whether the price of wine set by my liquor store is good or not.\nFor example, suppose the LCBO (my liquor store) sells a particular bottle for $13.99. I look at all of Vivino’s reviews for the bottle and find users who:\n\npaid more than $13.99 and liked the price\npaid less than $13.99 and disliked the price\n\nThis requires a large number of reviews to:\n\nTrain a machine learning algorithm\nDecide whether a bottle has a “good” price\n\nSo, I scraped thousands of wine products from the LCBO website! I matched them to bottles on Vivino and gently downloaded about 6 million corresponding reviews. Both the LCBO and Vivino datasets are hosted on my local PostgreSQL database for fast querying.\nI continued to gently amass 31 million reviews. This blog looks at the first partition of tables consisting of 11M reviews."
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#features",
    "href": "posts/2023-07-06-vivino-part1/index.html#features",
    "title": "What do 6.4 Million Wine Reviews Think about Wine?",
    "section": "Features",
    "text": "Features\nThere are several features for each Vivino review.\n\n\nCode: get SQL table features\nimport pandas as pd\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\nq = \"\"\"\nSELECT column_name\nFROM information_schema.columns\nWHERE table_schema = 'public'\nAND table_name   = 'vivino_lcbo'\n;\n\"\"\"\ncolNames = pd.read_sql(q, sql_address)['column_name']\nncolHalf = int(len(colNames)/2)\n\nMarkdown(tabulate(\n  list(zip(colNames[0:ncolHalf], colNames[ncolHalf:])), \n  headers=[\"Feature names\",\"Feature names (cont'd)\"]\n))\n\n\n\nFeatures per Vivino review \n\n\nFeature names\nFeature names (cont’d)\n\n\n\n\nreview_id\nuser_id\n\n\nvivino_id\nuser_seo_name\n\n\nrating\nis_featured\n\n\nreview\nis_premium\n\n\nlanguage\nuser_followers_count\n\n\ncreated_at\nuser_followings_count\n\n\nlikes_count\nuser_ratings_count\n\n\ncomments_count\nuser_ratings_sum\n\n\nflavors\nuser_reviews_count\n\n\n\n\n\n\n\nCode: get SQL table\nimport matplotlib.pyplot as plt\n\nq = \"\"\"\nSELECT rating, likes_count, comments_count\nFROM vivino_lcbo\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\n\n\n\nCode: plot ratings histogram\nplt.hist(tbl['rating'], bins=81, log=True)\nplt.xlabel('Star Ratings')\nplt.ylabel('Log Count')\nplt.title('Distribution of Star Ratings')\nplt.ylim(0.9, 1e7)\nplt.xlim(0.95,5.05)\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.hist(tbl['likes_count'], bins=100, log=True)\nplt.hist(tbl['comments_count'], bins=100, log=True, alpha=0.5)\nplt.xlabel('Star Ratings')\nplt.ylabel('Log Count')\nplt.title('Distribution of Star Ratings')\nplt.show()\n\n\n\n\n\n\n\nCode\nq = \"\"\"\nSELECT review_id, review, likes_count, comments_count, user_seo_name, user_followers_count, vivino_id\nFROM vivino_lcbo\n--WHERE comments_count &gt; 250\nWHERE likes_count &gt; 1000\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\ntbl\n\n\n\n\n\n\n\n\n\nreview_id\nreview\nlikes_count\ncomments_count\nuser_seo_name\nuser_followers_count\nvivino_id"
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#header",
    "href": "posts/2023-07-06-vivino-part1/index.html#header",
    "title": "31 Million Wine Reviews",
    "section": "1. Header",
    "text": "1. Header\n\n\nCode: get number of reviews\nimport pandas as pd\n\nq = \"\"\"\nSELECT COUNT(*) as count\nFROM vivino\nGROUP BY vivino_id\n;\n\"\"\"\nnumReviews = sum(pd.read_sql(q, sql_address)['count'])\nnumWines = len(pd.read_sql(q, sql_address)['count'])\n\nprint(f\"Total number of reviews: {numReviews}\")\nprint(f\"Total number of wines: {numWines}\")\n\n\nTotal number of reviews: 11214793\nTotal number of wines: 12845\n\n\n\n\nCode: get number of reviews from extras table\nimport pandas as pd\n\nq = \"\"\"\nSELECT COUNT(*) as count\nFROM vivino_extra\nGROUP BY vivino_id\n;\n\"\"\"\nnumReviews = sum(pd.read_sql(q, sql_address)['count'])\nnumWines = len(pd.read_sql(q, sql_address)['count'])\n\nprint(\"Note: From extra table not used in this analysis:\")\nprint(f\"Total number of reviews: {numReviews}\")\nprint(f\"Total number of wines: {numWines}\")\n\n\nNote: From extra table not used in this analysis:\nTotal number of reviews: 20134847\nTotal number of wines: 49517\n\n\n\n\nCode: get SQL table features\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\nq = \"\"\"\nSELECT column_name\nFROM information_schema.columns\nWHERE table_schema = 'public'\nAND table_name   = 'vivino'\n;\n\"\"\"\ncolNames = pd.read_sql(q, sql_address)['column_name']\nncolHalf = int(len(colNames)/2)\n\nMarkdown(tabulate(\n  list(zip(colNames[0:ncolHalf], colNames[ncolHalf:])), \n  headers=[\"Feature names\",\"Feature names (cont'd)\"]\n))\n\n\n\nFeatures per Vivino review \n\n\nFeature names\nFeature names (cont’d)\n\n\n\n\nreview_id\nuser_id\n\n\nvivino_id\nuser_seo_name\n\n\nrating\nis_featured\n\n\nreview\nis_premium\n\n\nlanguage\nuser_followers_count\n\n\ncreated_at\nuser_followings_count\n\n\nlikes_count\nuser_ratings_count\n\n\ncomments_count\nuser_ratings_sum\n\n\nflavors\nuser_reviews_count"
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#ratings-likes-and-comments",
    "href": "posts/2023-07-06-vivino-part1/index.html#ratings-likes-and-comments",
    "title": "What do 6.4 Million Wine Reviews Think about Wine?",
    "section": "2. Ratings, Likes, and Comments",
    "text": "2. Ratings, Likes, and Comments\n\n\nCode: get SQL table\nimport matplotlib.pyplot as plt\n\nq = \"\"\"\nSELECT rating, likes_count, comments_count\nFROM vivino_lcbo\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\n\nHere is a log-histogram of the star ratings. Most users vote in 0.5 increments (e.g., 3.5, 4.0, 4.5) rather than the finest 0.1 increments.\n\n\nCode: plot ratings histogram\nplt.hist(tbl['rating'], bins=81, log=True)\nplt.xlabel('Star Ratings')\nplt.ylabel('Log Count')\nplt.title('Distribution of Star Ratings')\nplt.ylim(0.9, 1e7)\nplt.xlim(0.95,5.05)\nplt.show()\n\n\n\n\n\nTo demonstrate this more clearly, here is a normal histogram.\n\n\nCode: plot ratings histogram\nplt.hist(tbl['rating'], bins=81)\nplt.xlabel('Star Ratings')\nplt.ylabel('Count (millions)')\nplt.title('Distribution of Star Ratings')\nplt.xlim(0.95,5.05)\nplt.show()\n\n\n\n\n\nLet’s integrate the histogram using the cumulative option.\n\n\nCode: plot ratings cumulative histogram\nplt.hist(tbl['rating'], bins=81, cumulative=True, density=True)\nplt.xlabel('Star Ratings')\nplt.ylabel('Normalized Density')\nplt.title('Cumulative Distribution of Star Ratings')\nplt.xlim(0.95,5.05)\nplt.show()\n\n\n\n\n\nLooks like less than 5% of bottles are rated below 3. Why?\n\nTheory 1: Most bottles are decent or awesome.\nTheory 2: Low ratings are hidden… (I don’t think so)\nTheory 3: 3 stars is the new bad.\nTheory 4: There are more theories but I’m not a wine expert.\n\nThe answer is likely some combination of theories 1, 3, and 4. For instance, I would not expect many true 1-star wines to make it to market due to severe costs of production and destructive name branding. Rather, 1-star ratings are likely from drinking preferences, faulty/damaged bottling or storage, or splitters1.\nWine Spectator’s 100-point Scale gives some credence to theory 3. Anything below 50-pts or 3-stars is “Not recommended” and would be a devastating review.\n\n\nCode\nplt.hist(tbl['likes_count'], bins=100, log=True)\nplt.hist(tbl['comments_count'], bins=100, log=True, alpha=0.5)\nplt.xlabel('Star Ratings')\nplt.ylabel('Log Count')\nplt.title('Distribution of Star Ratings')\nplt.show()\n\n\n\n\n\n\n\nCode\nq = \"\"\"\nSELECT review_id, review, likes_count, comments_count, user_seo_name, user_followers_count, vivino_id\nFROM vivino_lcbo\n--WHERE comments_count &gt; 250\nWHERE likes_count &gt; 1000\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\ntbl\n\n\n\n\n\n\n\n\n\nreview_id\nreview\nlikes_count\ncomments_count\nuser_seo_name\nuser_followers_count\nvivino_id"
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#footnotes",
    "href": "posts/2023-07-06-vivino-part1/index.html#footnotes",
    "title": "31 Million Wine Reviews",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npeople who think black and white, or 1- and 5-stars only.↩︎\nbecause effort↩︎\nVivino needs to use a different word for followings.↩︎"
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#ratings",
    "href": "posts/2023-07-06-vivino-part1/index.html#ratings",
    "title": "31 Million Wine Reviews",
    "section": "2. Ratings",
    "text": "2. Ratings\n\nAll ratings (with reviews)\nHere is a histogram of all star ratings. Most users vote in 0.5 increments (e.g., 3.5, 4.0, 4.5) rather than the finest 0.1 increments.\n\n\nCode: get SQL table and plot ratings\nimport matplotlib.pyplot as plt\n\nq = \"\"\"\nSELECT rating\nFROM vivino\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\nplt.hist(tbl['rating'], bins=81)\nplt.xlabel('Star Rating')\nplt.ylabel('Count (millions)')\nplt.title('Distribution of Star Ratings')\nplt.xlim(0.95,5.05)\nplt.show()\n\n\n\n\n\nThere are reviews with low ratings! Here is a log-y plot.\n\n\nCode: plot histogram of ratings\nplt.hist(tbl['rating'], bins=81, log=True)\nplt.xlabel('Star Rating')\nplt.ylabel('Count')\nplt.title('Distribution of Star Ratings')\nplt.ylim(0.9, 1e7)\nplt.xlim(0.95,5.05)\nplt.show()\n\n\n\n\n\nLet’s integrate the histogram to get a cumulative distribution.\n\n\nCode: plot cumulative distribution of ratings\nplt.hist(tbl['rating'], bins=81, cumulative=True, density=True)\nplt.xlabel('Star Rating')\nplt.ylabel('Normalized Density')\nplt.title('Cumulative Distribution of Star Ratings')\nplt.xlim(0.95,5.05)\nplt.show()\n\n\n\n\n\nThe above plots clearly show the ratings are distributed in a highly irregular fashion.\nLooks like less than 5% of bottles are rated below 3-stars. Why?\n\nTheory 1: Most bottles are decent or awesome.\nTheory 2: Low ratings are hidden… (unlikely)\nTheory 3: 3 stars is considered bad.\nTheory 4: Bots.\nTheory 5: There are more theories but I’m not a wine expert.\n\nThe answer is likely some combination of theories 1, 3, and 5. For instance, I would not expect many true 1-star wines to make it to market due to severe costs of production and self-destructive branding. Rather, 1-star ratings are likely from faulty/damaged bottling or storage, peculiar drinking preferences, or splitting1.\nWine Spectator’s 100-point Scale gives credence to Theory 3. Anything below 50-pts or 3-stars is “Not recommended” and would be a devastating review.\nNote: I had no impression of Theory 4 until I dug a tiny bit deeper. There are a LOT of inexplicable behaviour that would surprise me if they were human.\n\n\nAverage Rating per Wine\nThe distribution of the average rating per wine shows a nice demonstration of the Central Limit Theorem.\n\n\nCode: plot histogram of average rating per wine\nfrom scipy.optimize import curve_fit\nimport numpy as np\ndef gaussian(x, mu, sigma, a):\n    return a*np.exp(-0.5*(x-mu)**2/sigma**2)\n\nq = \"\"\"\nSELECT AVG(rating) as avg_rating\nFROM vivino\nGROUP BY vivino_id\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\nh = plt.hist(tbl['avg_rating'], bins=81)\n\nxdata = h[1][1:] - 0.5*np.diff(h[1])[0] #get center of edges.\nydata = h[0]\npopt, pcov = curve_fit(gaussian, xdata, ydata, p0=(4, 0.5, 100))\nplt.plot(xdata, gaussian(xdata, *popt), \n         'r-', \n         label=r'Gaussian: $\\mu$=%5.3f, $\\sigma$=%5.3f' % tuple(popt[0:2]))\n\nplt.xlabel('Star Rating')\nplt.ylabel('Count')\nplt.title('Cumulative Distribution of Star Ratings')\nplt.xlim(0.95,5.05)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#reviews",
    "href": "posts/2023-07-06-vivino-part1/index.html#reviews",
    "title": "31 Million Wine Reviews",
    "section": "3. Reviews",
    "text": "3. Reviews\nLet’s look at the distribution of reviews per wine bottle. I use log-binning to capture the granularity of wines with few reviews.\nA log-normal distribution with a bit of skewness is a good description. These distributions appear normal when the x-axis is logscale.\n\n\nCode: plot histogram of reviews per bottle\nq = \"\"\"\nSELECT count(*) as count\nFROM vivino\nGROUP BY vivino_id\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\ndef lognormal(x, mu, sigma, a):\n    return a*np.exp(-0.5*(np.log(x)-mu)**2/sigma**2) / (x * sigma)\n\n\nh = plt.hist(tbl['count'], bins=np.logspace(np.log10(1),5, 100))\n\nxdata = 10**(np.log10(h[1])[1:] - 0.5*np.diff(np.log10(h[1]))[0])\nydata = h[0]\npopt, pcov = curve_fit(lognormal, xdata, ydata, p0=(250, 50, 100))\nplt.plot(xdata, lognormal(xdata, *popt), \n         'r-', \n         label=r'Log-Normal: $\\mu$=%5.3f, $\\sigma$=%5.3f' % tuple(popt[0:2]))\n\n\nplt.xlabel('Number of reviews per wine')\nplt.ylabel('Count')\nplt.xscale('log')\nplt.title(f'Distribution of Reviews per Wine')\nplt.show()\n\n\n\n\n\nMost wines have between 50 to 500 reviews each. The peak is at ~250 reviews per bottle.\nEarlier, I stated approximately 0.5% or 1-in-200 of reviews have price-sentiments (i.e., the reviews that I want). Let’s say we need 10 price-sentiments to have an interesting signal. Then only bottles with 2,000+ reviews will likely the signal I desire. From the cumulative distribution below, that is approximately 15% of bottles.\n\n\nCode: plot cumulative distribution of reviews per bottle\nh = plt.hist(tbl['count'], bins=np.logspace(np.log10(1),5, 100), cumulative=True, density=True)\nplt.xlabel('Number of reviews per wine')\nplt.ylabel('Cumulative number of wines')\nplt.xscale('log')\nplt.title(f'Cumulative Distribution of Reviews per Wine')\nplt.axvline(2000, c = 'r')\nplt.show()\n\n\n\n\n\nThe above figure is for the entire dataset which includes wines I am not able to purchase at my liquor store. If I exclude those wines, I find approximately 450 bottles purchasable at our liquor stores have over 2,000 reviews. This suggests there are plenty of purchasable wines to build a wine recommendation app based on price-sentiment.\n\n\nCode: plot cumulative distribution of reviews per bottle from LCBO\nq = \"\"\"\nSELECT count(*)  FROM (\n    SELECT vivid2 FROM (\n        SELECT sku, vivid2 FROM index_matches\n        WHERE sku IS NOT null\n        AND vivid1 IS NOT null\n        AND vivid2 IS NOT null\n        ) AS matches\n    INNER JOIN (\n        SELECT sku FROM inventory\n        GROUP BY sku\n    ) AS buyable \n    ON matches.sku = buyable.sku\n) AS buyable\nLEFT JOIN (\n    SELECT vivino_id FROM vivino\n) AS vivino_reviews\nON vivid2 = vivino_id\nGROUP BY vivino_id\n\"\"\"\n\ntbl = pd.read_sql(q, sql_address)\n\nh = plt.hist(tbl['count'], bins=np.logspace(np.log10(1),5, 100), cumulative=True)\nplt.xlabel('Number of reviews per wine')\nplt.ylabel('Cumulative number of wines')\nplt.xscale('log')\nplt.title(f'Cumulative Distribution of Reviews per Wine (Purchasable at the LCBO)')\nplt.axvline(2000, c = 'r')\nplt.show()\n\nprint(f'There are {len(tbl)} bottles available at the LCBO.')\n\n\n\n\n\nThere are 1455 bottles available at the LCBO.\n\n\n\nText\nOur next goal is to study the review strings such as the word and character lengths. Let’s grab a random set of 1 million reviews.\n\n\nCode: get 1M random reviews\nimport re\nq = \"\"\"\nSELECT review_id, review, language FROM vivino\nORDER BY RANDOM()\nLIMIT 1000000\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n#delete more than one whitespace\ntbl['review'] = tbl['review'].apply(lambda x: re.sub(' +', ' ', x))\n\n\n\n\nCode: plot histogram of character lengths\nimport matplotlib.pyplot as plt\ntbl['charLength'] = tbl['review'].apply(len)\nplt.hist(tbl['charLength'], bins=int(max(tbl['charLength'])/2), log = True)\nplt.title('Distribution of Character Lengths per Review')\nplt.ylabel('Number of reviews')\nplt.xlabel('Number of characters')\nplt.ylim(1e2, 5e4)\nplt.show()\nprint('Example of a 500+ character review:')\nprint(tbl[tbl['charLength']&gt;=500]['review'].iloc[10])\n\n\n\n\n\nExample of a 500+ character review:\n👁 Garnet red\n👃 Cherry, raspberries and oak, the freshness is also very intense.\n👅 Light body, well balanced, medium finish and very soft aftertaste, I didn’t feel the acidity which is characteristic of the Pinot Noirs, which gave me a good impresion of this wine! \n\n**Esp\n\n👁 Rojo granate\n👃 Cereza, frambuesa y roble, tambien senti una frescura intensa agradable\n👅 Cuerpo ligero, bien equilibrado, final medio y retrogusto suave. No senti la acidez caracteristica de los Pinot Noir, lo cual me agrado bastante!\n\n\n\n\n\nMost users express write fewer than 100 characters. The users who express more appear as a spike in the distribution due to the 520 character limit. Very long reviews have a number of sentences and points. They often contain a story, lists of taste, smell, and sight descriptors, or common wine nomenclature (e.g., tannins, body, acidity).\n\n\nCode: plot histogram of word lengths\ntbl['wordLength'] = tbl['review'].apply(lambda x: len(x.split(' ')))\nplt.hist(tbl['wordLength'], bins=int(max(tbl['wordLength'] )), log = True)\nplt.title('Frequency of words per Review')\nplt.ylabel('Number of reviews')\nplt.xlabel('Number of words')\nplt.axvline(520/5.79, c = 'k', lw=1, label = \"4.79 char. per word\")\nplt.legend()\nplt.show()\n\n\n\n\n\nFigure 1: Frequency of words in reviews\n\n\n\n\nA study of google books found that the average number of characters per word is 4.79 (Norvig). White spaces are counted in the character limit of reviews, so the average word approximately occupies 5.79 characters. The maximum number of average words in 520 characters is then 520 / 5.79 ~ 90 words, which is located at the knee in the distribution!\nLet’s plot see how the character and word lengths are distributed together.\n\n\nCode: plot character and word lengths\nplt.scatter(tbl['charLength'], tbl['wordLength'], s= 1, alpha=0.5, label='language: any')\nplt.scatter(tbl[tbl['language']==ja]['charLength'], tbl[tbl['language']==ja]['wordLength'], c = 'r', s= 1, alpha=0.5, label='language: ja')\nplt.title('Scatter Plot of Word and Character Lengths')\nplt.xlabel('Number of characters')\nplt.ylabel('Number of words')\nplt.plot([0,520], [0, 520/5.79], c = 'k', label = \"4.79 char. per word\")\nplt.legend()\nplt.show()\n\n\n\n\n\nThere are clearly two sets of distributions. The dominant diagonol distribution shows most reviews contain 4.79 characters (plus 1 white space) per word, no matter the review length!\nThe smaller distribution of reviews with a very high character-word density are either written in kanji, have numerous emojis, or are just riiiiiiiiiiiiiiiiiiidiculously long:\n\n\n28362     ソライア2009\\nTENUTA TIGNANELLO (ANTINORI)\\nテヌータ・テ...\n34597     Para comprar a cajas....\\n\\n\\n👏👏👏👏👏👏👏👏👏👏👏👏👏\\n🍷...\n146622    ブレッドアンドバター、ピノノワール2021年\\n\\nカリフォルニア、ナパヴァレーの人気ワイナ...\n166770    Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n222111    Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n234564    Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n245968    SONOMA COAST LES NOISETIERS CHARDONNAY 2019\\n\\...\n271508    3,75*\\n\\nBuena RCP\\n\\n\\n👏👏👏👏👏👏👏👏👏👏👏👏👏\\n🍷🍷🍷🍷🍷🍷🍷...\n306546    Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n320458    Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n324681    サヴォワ・アプルモン・キュヴェ・ガストロノミー 2019\\n\\n淡いライトグリーン。ゆったり...\n335071    持って来てくれた人曰くオレンジワインらしい。\\n\\n外観はオレンジ色がかった強目の黄色からゴ...\n394920    ユニオン・デ・グラン・クリュ・ド・ボルドーにて。\\n2019ヴィンテージ。\\n\\n去年の20...\n400628    グラハム・ベック・ブリュット・ロゼ NV\\n\\n少し紫がかった濃いピンク。決め細かな泡立ち。...\n401848    シャトー アンジェリュス\\n2018.2017.2014\\n\\n2018、濃い紫寄りのガーネ...\n504967    自宅にて開栓。\\n2015ヴィンテージ。\\n\\n家で2016ヴィンテージを飲んで気に入ったシ...\n542056    T持ち込みワイン会にて。\\n2015ヴィンテージ。\\n\\n自分が持ち込みブラインドで提供した...\n559185    Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n610415    Bic Camera¥3680\\nワインショップの店員さんのお話を聞いて、ネットや雑誌では知...\n614611    Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\nName: review, dtype: object\n\n\nHmm… There’s one user who enjoys writing reviews with very low information density.\n\n\nCode: get number of duplicate reviews from francisco_silva6\ntmpID = 355128697\nq = f\"\"\" \nSELECT count(*)\nFROM vivino \nWHERE review = (\n    SELECT review\n    FROM vivino\n    WHERE review_id = {tmpID}  \n)\nAND user_seo_name='francisco_silva6' \n--the name condition makes no difference\n\"\"\"\ntbl_gra = pd.read_sql(q, sql_address)\nprint(f'Number of duplicates: {tbl_gra[\"count\"].iloc[0]}')\ntbl[tbl['review_id']==tmpID]['review'].iloc[0]\n\n\nNumber of duplicates: 131\n\n\n'Graaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaande vinho'\n\n\nWow. 131 duplicates! Lots of questions. This is a good opportunity to talk about Vivino’s users."
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#likes-and-comments",
    "href": "posts/2023-07-06-vivino-part1/index.html#likes-and-comments",
    "title": "What do 6.4 Million Wine Reviews Think about Wine?",
    "section": "Likes and Comments",
    "text": "Likes and Comments\nThe number of likes and comments for a given review is recorded at the time of download. Here are histograms for the likes and comments of reviews.\n\n\nCode\nimport math\n\nq = \"\"\"\nSELECT likes_count, comments_count\nFROM vivino\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\nbinWidth = 10\nmaxBinsLikes = int(math.ceil(max(tbl['likes_count'])/binWidth)+1)*binWidth\nmaxBinsComments = int(math.ceil(max(tbl['comments_count'])/binWidth)+1)*binWidth\nrangeBinsLikes = range(0,maxBinsLikes, binWidth)\nrangeBinsComments = range(0,maxBinsComments, binWidth)\n\nplt.hist(tbl['likes_count'], bins = rangeBinsLikes, log=True, label=\"Likes\")\nplt.hist(tbl['comments_count'], bins = rangeBinsComments, log=True, alpha=0.5, color = 'r', label=\"Comments\")\nplt.xlabel('Number of Likes and Comments')\nplt.ylabel('Log Count')\nplt.title('Distribution of a Review Likes and Comments')\nplt.legend()\nplt.show()\n\n\n\n\n\nFrom experience, it makes sense that there are fewer comments than likes[^2] and that most reviews do not have many likes. There are some ~100 out reviews that have over 500 likes.\nIt is shocking that there are reviews with more than a few hundred comments! That sounds busier than a controversial tweet. How can one review have over 1750 comments?? Let’s take a peek.\n\n\nCode\nq = \"\"\"\nSELECT comments_count as num_comments, likes_count as num_likes, review, user_seo_name as name, user_followers_count as num_followers, user_followings_count as num_followings\nFROM vivino_lcbo\nWHERE comments_count &gt; 300\nORDER BY comments_count\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\ntbl\n\n\n\n\n\n\n\n\n\nnum_comments\nnum_likes\nreview\nname\nnum_followers\nnum_followings\n\n\n\n\n0\n315\n687\n95 My 5K and last review on Vivino. What a jou...\nkim-lund_j\n3924\n574\n\n\n1\n405\n205\nThey say you will always remember your first, ...\nkelley_do\n5997\n2042\n\n\n2\n433\n3\nWawww\nademar.ju\n57\n57\n\n\n3\n547\n514\nA fantastic Syrah, deep black color, deep anim...\nscorewine\n16646\n242\n\n\n4\n1848\n0\nVery very Nice amarone. Fruitful and full with...\nm58cb20b86fe892ae6f5543f9e9b2c8c\n0\n0\n\n\n\n\n\n\n\nWeird! It looks like m58cb20b86fe892ae6f5543f9e9b2c8c is a bot or experiment. Their profile shows some reviews with over 5,000 comments and zero likes. Here are some comments for one of them:\n\nUser ademar.ju suspiciously has 57 followers and 57 followings (profile). They have 24 bottles rated with single-digit likes, hundreds of comments each and they’re all written by Leonel de Paula. Leonel de Paula appears to spam their own wine reviews with emojis and tag other users.\n\nThere is a lot more I can say about these bots on Vivino. I’ll make a dedicated post soon!\nThe remaining users in the table kim-lund_j and kelley_do appear to be real people with strong passions for wine. scorewine’s account no longer exists, unfortunately."
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#reviews-1",
    "href": "posts/2023-07-06-vivino-part1/index.html#reviews-1",
    "title": "What do 6.4 Million Wine Reviews Think about Wine?",
    "section": "3. Reviews",
    "text": "3. Reviews"
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#users",
    "href": "posts/2023-07-06-vivino-part1/index.html#users",
    "title": "31 Million Wine Reviews",
    "section": "4. Users",
    "text": "4. Users\n\nLikes and Comments\nHere are histograms for the number of likes and comments for a review.\n\n\nCode: plot histogram of likes and comments per review\nimport math\n\nq = \"\"\"\nSELECT likes_count, comments_count\nFROM vivino\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\nbinWidth = 20\nmaxBinsLikes = int(math.ceil(max(tbl['likes_count'])/binWidth)+1)*binWidth\nmaxBinsComments = int(math.ceil(max(tbl['comments_count'])/binWidth)+1)*binWidth\nrangeBinsLikes = range(0,maxBinsLikes, binWidth)\nrangeBinsComments = range(0,maxBinsComments, binWidth)\n\nplt.hist(tbl['likes_count'], bins=np.logspace(np.log10(1),4, 100), log=True, label=\"Likes\")\nplt.hist(tbl['comments_count'], bins=np.logspace(np.log10(1),4, 100), log=True, alpha=0.5, color = 'r', label=\"Comments\")\nplt.xlabel('Number of Likes, and Comments')\nplt.ylabel('Log Count')\nplt.title('Distribution of a Review Likes and Comments')\nplt.xscale('log')\nplt.legend()\nplt.show()\n\n\n\n\n\nFrom experience, it makes sense that there are fewer comments than likes2 and that most reviews do not have many likes. There are ~100 reviews that have over 500 likes, which is impressive. Here are a few samples:\n\n\nSplendid Umbrian blend of 70% Merlot, 20% CS and 10% CF, originating from vineyards located on the Spello and Assisi hills, aged for 12m in French barriques and another 24m in the bottle.  \nInky dark red, displaying scents of blueberries, blackberries, laurel, sweet almonds, smoky cedar and bitter dark chocolate refined with a whiff of mint.  \nDespite its age still very grippy, showing firm tannins that structure a dense core of  fresh and juicy dark fruit with clear hints of pepper in the finisch.  \n4.0*\n--------------------------------\n⭐️ 90 P/ 4.0* Very nice aged Tawny Port. Typical for the style, with a good complexity and concentration. Can stand alone or with food (e.g. cheese).\n \n👁 Pale tawny.\n \n👃 Intense nose of tertiary flavors – raisins, tobacco, leather, cedar, roasted nuts, milk chocolate, butterscotch, rum infused berries, dates, cooked red and black berries.\n \n👅 Half-sweet with a med+ acidity. High in alcohol with med and ripe tannins. The body is med+. The finish is dominated by fruity-tertiary notes and is med+ long.\n--------------------------------\nMerlot 55%+ syrah 35%+sangiovese 10%\nTonneau 12 months\nAt nose is quite complex: blackberry, blueberry, vanilla, cocoa, red flowers.\nRich tannins and good texture in mouth.\nGood persistency with licorice notes. \n--------------------------------\n\n\n\n\nBots?\nIt is shocking that there are reviews with more than a few hundred comments! That sounds busier than a controversial tweet. How can a review have over 2000 or 5000 comments?? Let’s take a peek.\n\n\nCode: select reviews with over 400 comments\nq = \"\"\"\nSELECT comments_count as comments, likes_count as likes, review, user_seo_name as name, user_followers_count as followers, user_followings_count as followings\nFROM vivino\nWHERE comments_count &gt; 400\nORDER BY comments_count\n;\n\"\"\"\ntbl = pd.read_sql(q, sql_address)\ntbl\n\n\n\n\n\n\n\n\n\ncomments\nlikes\nreview\nname\nfollowers\nfollowings\n\n\n\n\n0\n405\n205\nThey say you will always remember your first, ...\nkelley_do\n5997\n2042\n\n\n1\n411\n13\nSchäumig wie‘s sein soll. Ansonsten sehr üppig...\nluigi.pietronello\n102\n254\n\n\n2\n433\n3\nWawww\nademar.ju\n57\n57\n\n\n3\n446\n234\nRich modern style that incorporates classic Ri...\nakarter\n5886\n147\n\n\n4\n520\n1\nGood wine!\nalessandra.fag\n14\n20\n\n\n5\n547\n514\nA fantastic Syrah, deep black color, deep anim...\nscorewine\n16646\n242\n\n\n6\n676\n4\nVin blanc très sucré, véritablement de l'or en...\nalexandre.yi\n37\n31\n\n\n7\n1848\n0\nVery very Nice amarone. Fruitful and full with...\nm58cb20b86fe892ae6f5543f9e9b2c8c\n0\n0\n\n\n8\n5073\n0\nVery very nice sauternes.\nm58cb20b86fe892ae6f5543f9e9b2c8c\n0\n0\n\n\n\n\n\n\n\nWeird! It looks like m58cb20b86fe892ae6f5543f9e9b2c8c is a bot or experiment. Their profile shows some reviews with over 5,000 comments and zero likes. Here are comments for one of them:\n\nUser ademar.ju has reviewed 24 bottles which have single-digit likes, hundreds of comments each, all of which are written by Leonel de Paula. Leonel de Paula also comments on their own wine reviews with emojis, and tags other users.\n\nSome users like kelley_do and scorewine appear to be real people with strong passions for wine.\nThere is a lot more I can say about these bots on Vivino that I will say in my next blog post. For now, I do not find any reviews from suspected bots with prices in them. This suggests that my work on price-sentiments is unlikely to suffer from bots generating fake reviews.\n\n\nVivino Users\nThere are five numeric features for each user:\n\nFollowers: People who follow the user.\nFollowings3: People who the user follows.\nRatings count: Total number of ratings from the user.\nRatings sum: Sum of all of user’s ratings.\nReviews count: Total number of reviews authored from the user.\n\n\nUser connections:\nLet’s see how reviewers are connected to their communities.\n\n\nCode: plot followers vs. participation\nq = \"\"\" \nSELECT user_followers_count, user_followings_count\nFROM vivino \n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\nplt.scatter(tbl['user_followers_count'], tbl['user_followings_count'], s= 1, alpha=0.5)\nplt.title('Reviewer Connections')\nplt.xlabel('Number of followers    (celebrity status --&gt;)')\nplt.ylabel('Number of followings    (watcher status --&gt;)')\nplt.xscale('log')\nplt.show()\n\n\n\n\n\nWow! There’s a peculiar vase distribution. First, there’s some sort of uneven cap on the maximum number of people a reviewer can follow. The massive void on the left is striking and the reason is unclear just from this data. At the very least, it shows that you might be able to make connections just by following people. There is a lot of structure here that I will try to dig deeper in another post.\n\n\nParticipation\nLet’s define a crude participation metric that equals the number of reviews authored plus ratings given. Then, plot this against the number of followers. (Yes, the results inherently ignore all Vivino users who do not write reviews).\n\n\nCode: plot followers vs. participation\nq = \"\"\" \nSELECT user_followers_count, user_followings_count, user_reviews_count, user_ratings_count, user_seo_name\nFROM vivino \n\"\"\"\ntbl = pd.read_sql(q, sql_address)\n\nplt.scatter(tbl['user_followers_count'], tbl['user_reviews_count']+tbl['user_reviews_count'], s= 1, alpha=0.5)\nplt.title('Writer Followers')\nplt.xlabel('Number of followers')\nplt.ylabel('Number of authored reviews + ratings')\nplt.xscale('log')\nplt.yscale('log')\nplt.show()\n\n\n\n\n\nThere is a loose correlation between a user’s participation in reviews and ratings, and their follower count. My attention is drawn to the bottom where users have huge followings and participate very little. Let’s look at some of them:\n\n\nCode\nq = \"\"\" \nSELECT user_followers_count as followers, user_followings_count as followings, user_ratings_count as ratings, user_reviews_count as reviews, user_seo_name as username\nFROM vivino \nWHERE user_reviews_count + user_ratings_count &lt;= 4\nAND user_followers_count &gt; 200\nORDER BY user_followers_count\n\"\"\"\ntbl_bot = pd.read_sql(q, sql_address)\ntbl_bot\n\n\n\n\n\n\n\n\n\nfollowers\nfollowings\nratings\nreviews\nusername\n\n\n\n\n0\n210\n409\n1\n1\nroberto.campane\n\n\n1\n218\n224\n2\n2\nm7748f668dbdc131b7330298a3f91cb3\n\n\n2\n228\n63\n2\n1\neduardo_silves\n\n\n3\n267\n0\n1\n1\ngabriel-moo\n\n\n4\n287\n257\n2\n2\njunior-ma\n\n\n5\n292\n517\n1\n1\nm778d4a83c4adc3c41786c1a8f45d28a\n\n\n6\n300\n293\n2\n2\neva_bel\n\n\n7\n528\n85\n1\n1\ntino.ma\n\n\n\n\n\n\n\nAh ha! The mysterious m7748f668dbdc131b7330298a3f91cb3 users.\n\n\nUsernames\nI suspect the number of bots to be quite high. But, this is quite difficult to determine definitively without access to a user’s detailed logs. A potential gauge is number of characters in a username.\n\n\n\n\n\nWoah! I wasn’t expecting a sizable number of users have the max length for usernames. To be fair, I’ve never asked myself that before. Let’s take a look at some of the names.\n\n\n\n\n\n\n\n\n\nfollowers\nfollowings\nnum_reviews\nnum_ratings\nusername\nlen_name\n\n\n\n\n2074247\n0\n0\n11\n12\nedu.gaytan.eduardo101-gmail.com\n31\n\n\n1177568\n74\n20\n874\n919\nuniversity-of-bath.wine-society\n31\n\n\n8325228\n0\n5\n13\n13\njael.patricia-granados-kraulles\n31\n\n\n8796594\n7\n7\n261\n262\ndanton-cavalcanti.franco-junior\n31\n\n\n3811532\n1\n0\n36\n38\nmarc-antoine.mailloux-labrousse\n31\n\n\n1067813\n5\n5\n64\n64\nb7941e714811716b34fec5f91046cae2\n32\n\n\n6142637\n4\n2\n6\n7\nd0a0d35afcfa00b39219856348c1cb12\n32\n\n\n10306389\n52\n28\n1274\n1303\nm276c28ed28bdf63ac70599392739db6\n32\n\n\n340985\n6\n8\n19\n23\nm4a610a084aeb6643b624f19501e5dbc\n32\n\n\n8049959\n393\n105\n312\n312\nm2210f2e3d21d5f526d8119fae5b94bd\n32\n\n\n1833623\n0\n1\n5\n5\nm9b16842b2ac0920a9bd932d634ae788\n32\n\n\n3694206\n0\n0\n15\n29\nm68f6cf0bd32342dd78045a1f291df1d\n32\n\n\n208288\n0\n0\n25\n32\nc225723314b54f84b589ef58383ba9e8\n32\n\n\n7033437\n2\n0\n86\n86\nsylvain-est-bon-.il-faut-le-boir\n32\n\n\n3228141\n2\n12\n82\n102\nm4fdc413aa3597af25f1fd807f596cca\n32\n\n\n\n\n\n\n\nAh. The 32-char names appear to be hashes. Perhaps they are deleted usernames or bots. It might be a coincidence that the user with 5,000 comments on their reviews has the hash-like username."
  },
  {
    "objectID": "posts/2023-07-06-vivino-part1/index.html#wine-notes",
    "href": "posts/2023-07-06-vivino-part1/index.html#wine-notes",
    "title": "31 Million Wine Reviews",
    "section": "Wine Notes",
    "text": "Wine Notes\nReviewers list the ‘notes’ of wine in their reviews. A nice surprise found in this data is a list of notes identified by Vivino. There are many, many notes.\n\n\nNumber of distinct notes: 456"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html",
    "href": "posts/2023-07-10-sentence-embedding/index.html",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "",
    "text": "Image by LCBO\nThis is a picture of an LCBO store in Toronto, Canada. All of the wines are exciting to look at and I want to try some! But, how will I know what’s good? There’s thousands to choose from! Looking on their app, I found two things:\nI gave up and asked an LCBO wine guide for recommendations. Both of the bottles recommended were horrendous. The bite to my wallet hurt too.\nAn incredibly popular wine app, Vivino, is great at identifying wines using their image recognition software. But, I’m not going to stand around all day taking pictures of wine. It is good at filtering out which bottles have under 4-stars. But, I would also like a good deal after my expensive experience.\n“You have to spend a little to discover.”\nMy goal is to increase the odds of finding well-priced discoveries. Using Natural Language Processing and machine learning techniques, I identify wines at the LCBO with high price-sentiments based on Vivino reviews. Here’s how I did it."
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#partisan-sentiment-neutral-is-negative",
    "href": "posts/2023-07-10-sentence-embedding/index.html#partisan-sentiment-neutral-is-negative",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Partisan Sentiment! Neutral is negative",
    "text": "Partisan Sentiment! Neutral is negative\nAfter running a couple experiments, it became immediately clear that most neutral sentiments are falsely classified as positive or negative. This is because distinguishing “not bad” from “good” requires a lot more samples to capture context and language 1. I decided to combine the neutral and negative categories as negative because:\n\nI am less likely to purchase a bottle considered “not bad”, let alone a bottle that is “not good”. This is my bias.\nThe neutral and negative categories generally overlap with respect to star-ratings.\nThe pool of negative sentiment data is doubled.\nEliminating a category eliminates a set of potential false classifications."
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#modelling-strategy",
    "href": "posts/2023-07-10-sentence-embedding/index.html#modelling-strategy",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Modelling Strategy",
    "text": "Modelling Strategy\n\nNLP-route: Bag of words\nThe most common sentiment analysis uses the bag-of-words model. The general procedure is relatively simple:\n\nTokenization: disassociate the string into a list of words.\nStrip all punctuation, decontract words (“don’t” becomes “do not”), strip emojis (optional) from a string (e.g., a review).\nLemmatization: collapse words with similar meaning (e.g., improve = [improve, improved, improving, improvements, improver] )\nDelete stop words.\nTF-IDF: (TF: frequency of term in a document) X (IDF2: Proportion of documents with the term). Always use both monograms and bigrams. Higher n-grams cost more computational resources for no measurable improvements.\nAppend any other features.\nYour favourite classification model. Commonly Naive Bayes, logistic regression, SVMs, and stochastic gradient descent.\n\nSide note: a useful trick I found was to run a separate TF-IDF on reviews without prices to identify irrelevant wine nomenclature. For example, most wine reviews list various flavour notes and measures of acidity, tannins, boldness, etc. These act as noise in our attempt to model price sentiments. The words can be added to your list of stop words.\nThis is a lot of work! Trust me, I tried it. The work is not understanding the math or code but, rather, ensuring the lemmatization and stop word deletion is applied appropriately for your domain. For instance, a popular NLP module called spaCy has several convenient features including lemmatization. While inspecting the samples, I found spaCy’s trained pipeline ‘en_core_web_sm-wine’ modifies the following review:\n\n“Had better $20 wine” (original)\n“Had well $20 wine” (with lemmatization)\n“well $20” (with stop word deletion)\n\nAdding a stemming exception for “better” improved my model’s prediction! While pre-built lemmatizers and stopwords are very convenient, ensuring they are appropriate for wine reviews requires careful attention and good familiarity with wine nomenclature. This is somewhat ironic considering I was doing this project to learn about wine! I guess that is still learning シ.\n\n\nSentence Embeddings. Are. Amazing.\nThe goal of the Bag of Words approach is to convert text into a set of numbers (a sparse matrix in this case) to then feed into a classifier. You can probably tell this conversion is painfully annoying and not my favourite strategy. I might write a blog about it. It is useful if your computational resources are limited. And, it is a relatively fast algorithm that competes against more sophisticated and expensive solutions. In my opinion, it’s value is to elevate solutions like sentence embeddings. /rant\nSentence embeddings do the exact same thing. Text is fed into a sentence transformer (ST) that has been trained on an enormous corpus of texts and the output is a vector of numbers. The kicker is that very minimal pre-processing is required before transforming a sentence. There are setbacks that may prevent you from using STs, however. Here is a list of practical ST restrictions and their relevance for wine reviews:\n\nInput length: STs commonly have 512 token limits which roughly corresponds to 300-400 English words. Longer documents will be truncated unless they are broken into pages, paragraphs, or sometimes sentences. Wine reviews have a 520-character limit or approximately 100 words (see this figure), so this is not an issue.\nTraining data: The ST model is trained on an enormous corpus of text data. It will perform more optimally if your text data is similar to the training data’s domain knowledge and colloquiualisms. “Optimal” can be measured by the cosine-similarity, for example, of two sentences that have similar meaning, if you have labelled data. The good news is that you can fine-tune an ST model to learn your your text’s domain knowledge (e.g., astrophysics)! More on this to come.\nGPUs: GPUs can decrease the computational time to transform a sentence by one or two orders of magnitude. If you have a large set of data, this can be an expensive step! Fortunately, I have a great GPU such that this is not the rate limiting step.\nContext Unlike bag of words, STs are designed to capture the context surrounding words as well. STs attempt to capture the intention or meaning in a sentence. Thus, the output vectors are “sentence” embeddings rather than an array of “word” embeddings generated from a vectorizer. Steps 1-5 from the bag of words procedure are collapsed into one step! Some pre-processing with punctuation might be, however (e.g., a wall of emojis is probably difficult to interpret).\n**\n\n#Part 3: Sentence Embedding\n\n\nCode\ndf = pd.read_csv(dirIn+\"unlabelled_surveys.csv\", encoding=\"utf-8-sig\")\nlen(df[df['rating']&gt;=4.5]['rating'])/len(df[df['rating']&lt;=1.5]['rating'])\n\n\n21.236842105263158\n\n\n\n\nCode\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"all_labelled_surveys.csv\"\ndf_new = pd.read_csv(filename, encoding='utf-8-sig')\n\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"labelled_surveys_no_ratings.csv\"\ndf_old = pd.read_csv(filename, encoding='utf-8-sig')\n\n\n\n\nCode\ndf = pd.concat([df_new,df_old],ignore_index=True)[['reviews','sentiment']]\n\n\n\n\nCode\ndf[df['sentiment']==\"Don't know\"]\n\n\n\n\n\n\n\n\n\nreviews\nsentiment\n\n\n\n\n2\nGotta love Christmas dinner where the uncles w...\nDon't know\n\n\n4\nFull bodied, flavorful from start to finish, n...\nDon't know\n\n\n9\nIncredible for the price, good celebratory win...\nDon't know\n\n\n12\nAt the Palm Restaurant. $100\\nHeb = 36\\nCostco...\nDon't know\n\n\n14\nExpensive but had to be done once in my life. ...\nDon't know\n\n\n...\n...\n...\n\n\n6126\nMed body, balanced acid, fruit, light tannin. ...\nDon't know\n\n\n6139\n84 Not sure how many bottles of this Wine Enth...\nDon't know\n\n\n6153\n$19.99 in Safeway.\nDon't know\n\n\n6165\n14.5% and $25. Really nice wine. Dry with righ...\nDon't know\n\n\n6166\nGreat flavor, smooth almost sweet, pairs amazi...\nDon't know\n\n\n\n\n719 rows × 2 columns"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#footnotes",
    "href": "posts/2023-07-10-sentence-embedding/index.html#footnotes",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis includes using a BERT sentence transformer which benefits from context.↩︎"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#models",
    "href": "posts/2023-07-10-sentence-embedding/index.html#models",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Models",
    "text": "Models\n\nBag of words\nThe most common sentiment analysis uses the bag-of-words model. The general procedure is relatively simple:\n\nTokenization: disassociate the string into a list of words.\nStrip all punctuation, decontract words (“don’t” becomes “do not”), strip emojis (optional) from a string (e.g., a review).\nLemmatization: collapse words with similar meaning (e.g., improve = [improve, improved, improving, improvements, improver] )\nDelete stop words.\nTF-IDF: (TF: frequency of term in a document) X (IDF2: Proportion of documents with the term). Always use both monograms and bigrams. Higher n-grams cost more computational resources for no measurable improvements.\nAppend any other features.\nYour favourite classification model. Commonly Naive Bayes, logistic regression, SVMs, and stochastic gradient descent.\n\nSide note: a useful trick I found was to run a separate TF-IDF on reviews without prices to identify irrelevant wine nomenclature. For example, most wine reviews list various flavour notes and measures of acidity, tannins, boldness, etc. These act as noise in our attempt to model price sentiments. The words can be added to your list of stop words.\nThis is a lot of work! Trust me, I tried it. The work is not understanding the math or code but, rather, ensuring the lemmatization and stop word deletion is applied appropriately for your domain. For instance, a popular NLP module called spaCy has several convenient features including lemmatization. While inspecting the samples, I found spaCy’s trained pipeline ‘en_core_web_sm-wine’ modifies the following review:\n\n“Had better $20 wine” (original)\n“Had well $20 wine” (with lemmatization)\n“well $20” (with stop word deletion)\n\nAdding a stemming exception for “better” improved my model’s prediction! While pre-built lemmatizers and stopwords are very convenient, ensuring they are appropriate for wine reviews requires careful attention and good familiarity with wine nomenclature. This is somewhat ironic considering I was doing this project to learn about wine! I guess that is still learning シ.\n\n\nSentence Embeddings. Are. Amazing.\nThe goal of the Bag of Words approach is to convert text into a set of numbers (a sparse matrix in this case) to then feed into a classifier. You can probably tell this conversion is painfully annoying and not my favourite strategy. I might write a blog about it. It is useful if your computational resources are limited. And, it is a relatively fast algorithm that competes against more sophisticated and expensive solutions. In my opinion, it’s value is to elevate solutions like sentence embeddings. /rant\nSentence embeddings do the exact same thing. Text is fed into a sentence transformer (ST) that has been trained on an enormous corpus of texts and the output is a vector of numbers. The kicker is that very minimal pre-processing is required before transforming a sentence. There are setbacks that may prevent you from using STs, however. Here is a list of practical ST restrictions and their relevance for wine reviews:\n\nInput length: STs commonly have 512 token limits which roughly corresponds to 300-400 English words. Longer documents will be truncated unless they are broken into pages, paragraphs, or sometimes sentences. Wine reviews have a 520-character limit or approximately 100 words (see this figure), so this is not an issue.\nTraining data: The ST model is trained on an enormous corpus of text data. It will perform more optimally if your text data is similar to the training data’s domain knowledge and colloquiualisms. “Optimal” can be measured by the cosine-similarity, for example, of two sentences that have similar meaning, if you have labelled data. The good news is that you can fine-tune an ST model to learn your your text’s domain knowledge (e.g., astrophysics)! More on this to come.\nGPUs: GPUs can decrease the computational time to transform a sentence by one or two orders of magnitude. If you have a large set of data, this can be an expensive step! Fortunately, I have a great GPU such that this is not the rate limiting step.\nContext Unlike bag of words, STs are designed to capture the context surrounding words as well. STs attempt to capture the intention or meaning in a sentence. Thus, the output vectors are “sentence” embeddings rather than an array of “word” embeddings generated from a vectorizer. Steps 1-5 from the bag of words procedure are collapsed into one step! Some pre-processing with punctuation might be, however (e.g., a wall of emojis is probably difficult to interpret).\n\nOkay - Let me show you how easy it is!"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#sentence-embedding-with-s-bert",
    "href": "posts/2023-07-10-sentence-embedding/index.html#sentence-embedding-with-s-bert",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Sentence Embedding with S-BERT",
    "text": "Sentence Embedding with S-BERT\n\nImport sentence-transformer\nAfter installing sentence-transformers, select an embedding model and download it automatically using the code below. Here, I show what typical values the embedding vectors take on.\n\n\nCode: sentence-transformer\nfrom sentence_transformers import SentenceTransformer\n\nembeddingModelName = \"all-MiniLM-L6-v2\"\nsentenceModel = SentenceTransformer(embeddingModelName)\noutputDim = sentenceModel.get_sentence_embedding_dimension()\nprint(f\"Number of embedding dimensions: {outputDim}\")\n\ntext = [\n    \"I love plants!\",\n    \"More wine. Give me more! This wine is amazing for $500\",\n    \"It is possible to commit no mistakes and still lose. That is not a weakness. That is life.\",\n    \"I love sentence embeddings. We need more.\"\n]\ntextEmbeddings = sentenceModel.encode(text, normalize_embeddings = True)\n\n\nNumber of embedding dimensions: 384\n\n\n\n\nCode: plot embedding vectors\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef gaussian(x, mu, sigma, a):\n    return a*np.exp(-0.5*(x-mu)**2/sigma**2)\n\n# Figures:\nfig,(ax1,ax2) = plt.subplots(2)\nfig.set_size_inches(10,8)\n# Subplot 1: scatter plot of embeddings\nfor i, embedding in enumerate(textEmbeddings):\n    ax1.scatter(list(range(outputDim)), embedding, s = 1, label=text[i])\nax1.set(xlabel=\"Output vector index\", ylabel='Embedding value')\nax1.legend()\nax1.set_ylim(np.min(textEmbeddings)-0.1, np.max(textEmbeddings) + 0.3)\n\n# Subplot 2: histogram of values and a gaussian fit.\ntextEmbeddings = [t for t in textEmbeddings] # Funny pixel tv without.\nh = ax2.hist(textEmbeddings, bins=20, alpha=0.5, stacked=True)\nxdata = h[1][1:] - 0.5*np.diff(h[1])[0] # get center from bin edges.\nydata = h[0][-1]\n\npopt, pcov = curve_fit(gaussian, xdata, ydata, p0=(0, 0.3, 100))\nSIGMA_ST = popt[1]\nax2.plot(xdata, gaussian(xdata, *popt), \n         'r-', \n         label=r'Gaussian: $\\mu$=%5.5f, $\\sigma$=%5.5f' % tuple(np.abs(popt[0:2])))\nax2.set(xlabel='Embedding value', ylabel = 'Count')\n\nplt.legend()\nplt.subplots_adjust(hspace=0.3)\nplt.show()\n\n\n\n\n\nThe normalized embedding vectors look quite Gaussian. This is useful to know since models like SVM need the feature vectors to be scaled to unity.\nHere’s an example of calculating the cosine similarities for each combination of sentence embeddings. There are four wine reviews with positive and negative price sentiments. There is also one quote from Picard, which is clearly dissimilar to the wine reviews.\n\n\n\n\n\n\n\n\n\n\n\n\nTick Label\nText\n\n\n\n\nwine-pos1\nGreat value cab - locally $10.50\n\n\nwine-pos2\nA dark berry nose and deep berry flavor on the top. Blackberries it tells me. A blend that defies the pricing at under $20. A robust hit of smoky oak with a decent finish not usually found in this price range! Can you tell I like it?\n\n\nwine-neg1\nA little lighter than expected, not in a good way. $16 is apparently overpriced. Would not seek out.\n\n\nwine-neg2\nActually not a bad wine but for the $56 we paid for it i can think of many more better wines. Nice after taste but dry.\n\n\npicard\nIt is possible to commit no mistakes and still lose. That is not a weakness. That is life.\n\n\nplants\nI love plants!"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#pipelines",
    "href": "posts/2023-07-10-sentence-embedding/index.html#pipelines",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Pipelines",
    "text": "Pipelines\nA network of pipes connect the raw data to various transformers where features from the data are engineered. Once complete, the transformed data is sent to a classifier or regressor. We first need to define the transformers before defining the pipes connecting between them.\nStep 1a: Define a transformer that pre-processes text. In this case, I want to prune text to best match embedding model’s training corpus, which were predominantly Reddit comments, article abstracts, and WikiAnswers. The reviews often have incorrect/strange punctuation, missing/excessive white spaces, and emojis. Here is a list of functions to tackle these issues.\nStep 1b: Define a transformer that encodes review text into sentence embeddings.\n\n\nCode: SBERT encoder\nclass SBERTEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # X - pandas series of text\n        embeddings = sentenceModel.encode(list(X.iloc[:,0]), normalize_embeddings = True)\n        # Normalize by the best fit STD: SIGMA_ST \n        embeddings /= SIGMA_ST\n        return embeddings\n\n\nStep 3: Define a pipeline and specify the classifier. This is where you can experiment with several classifiers to test. Here’s an example of a pipeline below with a Gradient Boosting Classifier. More on this in Part 6!\n\n\nCode: Sample pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\ngbc_pipe = Pipeline([\n    ('SBERTRatingTransformer', SBERTRatingTransformer),\n    ('KNeighborsClassifier', KNeighborsClassifier(n_jobs=8)),\n])\n\n\n\n\nCode\ndf = pd.read_csv(dirIn+\"unlabelled_surveys.csv\", encoding=\"utf-8-sig\")\nlen(df[df['rating']&gt;=4.5]['rating'])/len(df[df['rating']&lt;=1.5]['rating'])\n\n\n21.236842105263158\n\n\n\n\nCode\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"all_labelled_surveys.csv\"\ndf_new = pd.read_csv(filename, encoding='utf-8-sig')\n\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"labelled_surveys_no_ratings.csv\"\ndf_old = pd.read_csv(filename, encoding='utf-8-sig')\n\n\n\n\nCode\ndf = pd.concat([df_new,df_old],ignore_index=True)[['reviews','sentiment']]\n\n\n\n\nCode\ndf[df['sentiment']==\"Don't know\"]\n\n\n\n\n\n\n\n\n\nreviews\nsentiment\n\n\n\n\n2\nGotta love Christmas dinner where the uncles w...\nDon't know\n\n\n4\nFull bodied, flavorful from start to finish, n...\nDon't know\n\n\n9\nIncredible for the price, good celebratory win...\nDon't know\n\n\n12\nAt the Palm Restaurant. $100\\nHeb = 36\\nCostco...\nDon't know\n\n\n14\nExpensive but had to be done once in my life. ...\nDon't know\n\n\n...\n...\n...\n\n\n6126\nMed body, balanced acid, fruit, light tannin. ...\nDon't know\n\n\n6139\n84 Not sure how many bottles of this Wine Enth...\nDon't know\n\n\n6153\n$19.99 in Safeway.\nDon't know\n\n\n6165\n14.5% and $25. Really nice wine. Dry with righ...\nDon't know\n\n\n6166\nGreat flavor, smooth almost sweet, pairs amazi...\nDon't know\n\n\n\n\n719 rows × 2 columns\n\n\n\nBusiness model\nIf each category is mislabelled incorrectly by the same amount then no one’s the wiser. If the model preferentially classifies positives as negative then the distribution of sentiment-ratios shifts down. This has at least two potential effects:\n\nBottles with fewer reviews will suffer more. Since most bottles have fewer reviews, our pool of bottle recommendations decreases which may shorten the lifetime of the app. That’s not great.\nThe liquor store may be unfairly judged more poorly due to their prices being perceived more poorly. Our liquor distribution and sales are primarily run by the government. So, the profit goes back to the government and, nominally, the citizens. I’d be surprised if my app had that effect that.\n\nOn the other hand, if negatives are misclassified as positive then (1) the pool of bottle recommendations increases and (2) the average quality decreases. Thus, users are more likely to buy crappier wines.\nOkay, enough speculations. A solution is to cut away the pool of bottles with fewer reviews since they have higher sensititivities to our model’s performance.\nIf the two missclassification directions are quite unequal, I’d prefer having a more discriminating recommender system. My goal is to recommend wines that are genuinely good and well-priced. If they’re comparable\nA small complication is that DKs can be falsely classified as positive or negative and the proportions need not be equal. The distribution of DKs across star rating is somewhat uniform with a lean towards high-star ratings, which correlates with positive sentiment. This leads to more DKs falsely labelled as positive than negative. This is good news since the impact to the recommendation is relatively nerfed when adding noise to positives because they are counted in both the numerator and denominator in the sentiment-ratio (since positives are counted in both).\n\n3. Sentence Augmentation\nThere are several strategies to dealing with imbalanced data sets. The most common are under and oversampling (i.e., duplicate minority data or throw out majority data). You can also define the sample or class weights to place emphasis.\nFor natural language problems, we can employ sentence augmentation where sentences in a review are re-arranged. Here’s an example with one statement rearranged twice.\n\ntextSamples = [\n    \"I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\", \n    \"Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\"]\nemb = sentenceModel.encode(textSamples, normalize_embeddings=True)\nprint(f'(A) {textSamples[0]}')\nprint(f'(B) {textSamples[1]}')\nprint(f'Cosine similarity of (A) and (A): {np.dot(emb[0],emb[0])}')\nprint(f'Cosine similarity of (A) and (B): {np.dot(emb[0],emb[1])}')\n\n(A) I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\n(B) Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\nCosine similarity of (A) and (A): 1.0\nCosine similarity of (A) and (B): 0.9086364507675171"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#nlp-strategies",
    "href": "posts/2023-07-10-sentence-embedding/index.html#nlp-strategies",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "NLP Strategies",
    "text": "NLP Strategies\n\nBag of words - Can skip to Sentence Transformers\nA popular sentiment analysis strategy is the bag-of-words model. The general procedure is relatively simple.\nIn short, a document is disassociated into a list of words and the context for those words are destroyed. Words are lemmatized and filtered with a list of ‘stop words’. A TF-IDF vectorizer (or whatever your favourite measuring stick for a word’s importance) counts the frequency of a word in a given document (TF) and across documents (IDF) to construct a metric for a word’s “importance”. A machine learning model then delineates words that correspond to a document’s label.\nThis strategy will work poorly for price-sentiment analysis. The majority of text in reviews labelled “Don’t know” is identical to the positive and negative reviews because the price-sentiment is typically stated in a few words. In other words, trying to find a missing signal that is typically weak in a sea of noise is challenging for a bag of words model. Minimizing false positives relies strongly on the quality of stop words and lemmatization. This is a lot of work! Trust me, I tried it.\nSo, what do we do? (See next section)\nSide notes for if you try bag of words:\nA popular NLP module called spaCy has several convenient features including lemmatization. Be careful!!! Here is a shocking lemmatized sample from spaCy’s trained pipeline ‘en_core_web_sm-wine’:\n\n“Had better $20 wine” (original)\n“Had well $20 wine” (with lemmatization)\n“well $20” (with stop word deletion)\n\nAfter correcting this lemmatization, my model robustly improved by a couple percent! While the bag-of-words model is fast to compute, it requires careful attention and good familiarity with wine reviews to ensure algorithms are working as we intend.\nA useful trick I found was to run a separate TF-IDF on reviews to identify irrelevant wine nomenclature. For example, most wine reviews list various flavour notes and measures of acidity, tannins, boldness, etc. These act as noise in our bag of words and can be added to your list of stop words.\n\n\nSentence Transformers. Are. Amazing.\nIdentifying whether a review contains price-sentiment requires context. For instance, here are two real reviews with and without price-sentiment:\n\n“Awesome wine for price, selling at coop for $7/bottle”\n“Great smell and taste $62 at Binnys with 5% off, might have been on sale”\n\nHere are the versions without stopwords:\n\nawesome, price, selling, coop, $7/bottle\ngreat, $62, binnys, 5%, off, sale\n\nContext is vital to separating the DK labels! That’s where Sentence Transformers (STs) shine. STs are trained on an enormous corpus of text to learn the intent and meaning behind sentences. Very minimal pre-processing is required before transforming a sentence. There are setbacks that may prevent you from using STs, however. Here is a list of ST nuances and their relevance for wine reviews:\n\nInput length: STs commonly have 512 token limits which roughly corresponds to 300-400 English words. Longer documents will be truncated unless they are broken into pages, paragraphs, or sometimes sentences. Wine reviews have a 520-character limit or approximately 100 words (see this figure), so this is not an issue.\nTraining data: The ST model is trained on an enormous corpus of text data. It will perform better if your text data is similar to ST’s training data. The good news is that you can fine-tune an ST model to learn your your text’s domain knowledge (e.g., astrophysics)! More on this to come.\nGPU: A GPU can decrease the computational time to transform a sentence by one to two orders of magnitude compared to a CPU. If you have a large set of data, this can be an expensive step without a GPU!\nContext Unlike bag of words, STs are designed to capture the context surrounding words as well. STs attempt to capture the intention or meaning in a sentence. Thus, the output vectors are “sentence” embeddings rather than an array of “word” embeddings generated from a vectorizer. Steps 1-5 from the bag of words procedure are collapsed into one step! Some pre-processing with punctuation might be, however (e.g., a wall of emojis is probably difficult to interpret).\n\nLet’s give sentence embeddings a try."
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#training-data",
    "href": "posts/2023-07-10-sentence-embedding/index.html#training-data",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Training Data",
    "text": "Training Data\nThe review text needs to be transformed into embeddings. These embeddings are used repeatedly for all our models and should only be transformed once.\nI will be using two features:\n\nReview text\nStar ratings\n\n\n1. Read and re-label data\n\nimport pandas as pd\n\n# Read and re-label data set\nrewriteDict = {\n    'Positive': 'Positive',\n    'Neutral': 'Negative',\n    'Negative': 'Negative',\n    \"Don't know\": \"Don't know\"\n}\ndf = pd.read_csv(filename, encoding='utf-8-sig')\ndf = df[['reviews','sentiment', 'rating']]\ndf['sentiment'] = df['sentiment'].apply(lambda x: rewriteDict[x]) # Re-label 'Neutral' as 'Negative\n\n\n\n\n\n\n\n\n2. Clean review text\n\n\nCode: Text cleaning functions\nimport emoji\n\ndef remove_emoji(s):\n    # Useful emoji ripper\n    return emoji.replace_emoji(s)\n\ndef remove_excess_and_newline(s):\n    # Removes excessive fullstops (periods)..., !, and newlines.\n    s = re.sub(r\"\\n\", ' ', s)\n    s = re.sub(r\"\\.+\", '.', s)\n    s = re.sub(r\"!+\", '!', s)\n    return s\n\ndef filter_characters(s):\n    # Only permit the follow characters in reviews. \n    # Mainly deletes unwanted symbols such as: (){}#@^&*\n    s = re.sub(r\"[^a-zA-Z\\d\\s'’$\\.!\\-?£€,:éè%/]\", '', s)\n    return s\n\ndef fix_whitespace_around_punctuation(s):\n    # Many reviews have incorrect punc \"wine,try\", \"wine.try\", \"wine .try\"\n    s = re.sub(r\",\", ', ', s)\n    s = re.sub(r\" ,\", ',', s)\n    s = re.sub(r\" \\.\", '.', s)\n    s = re.sub(r\"(?&lt;=[.,!?])(?=[^\\s\\d])\", ' ', s)\n    s = re.sub(r\"(\\s)(?=[.,!])\", '', s)\n    #s = re.sub(r\" !\", '!', s)\n    return s\n\ndef strip_extra_whitespace(s):\n    # Strip 2+ whitespaces\n    s = re.sub(\"\\s+\",' ',s)\n    return s\n\ndef clean_text(s):\n    s = remove_emoji(s)\n    s = remove_excess_and_newline(s)\n    s = filter_characters(s)\n    s = fix_whitespace_around_punctuation(s)\n    s = strip_extra_whitespace(s)\n    return s\n\n\n\ndf['reviews_clean'] = df['reviews'].apply(clean_text)\n\nHere are some examples of the cleaned text.\n\n\n\n\n\nReview\nPre-processed Review\n\n\n\n\nLoved !!!! Pairs great with a 50$ steak lol or alone;)\nLoved! Pairs great with a 50$ steak lol or alone\n\n\nSuper soft mouth feel.. fruit forward. Not sure it’s worth the $50.00 but could be my pallet..\nSuper soft mouth feel. fruit forward. Not sure it’s worth the $50.00 but could be my pallet.\n\n\nHeavy cherries on the nose , followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20……wow!!! Good wine 🍷 anytime!\nHeavy cherries on the nose, followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20. wow! Good wine anytime!\n\n\n3.7 ~ 87% ($14.75) Frescobaldi 🇮🇹 Rèmole Rosso 2017 Med bodied, bruised, rustic purple ~ black berries, red cherries, and plums ~ smooth as always ~ spicy, oaky, and dry (4 gs/12.5% abv). I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help 😑\n3.7 87% $14.75 Frescobaldi Rèmole Rosso 2017 Med bodied, bruised, rustic purple black berries, red cherries, and plums smooth as always spicy, oaky, and dry 4 gs/12.5% abv. I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help\n\n\n\n\n\n\n\n3. Normalize star-ratings since many models require feature scaling\n\ndf['norm_rating'] = (df['rating'] - 3.0) * 0.5  # Normalize 5-star rating\n\n\n\n4. Split data: training and test sets\n\nfrom sklearn.model_selection import train_test_split\nseed = 42\nsplitFraction = 0.85\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    df, \n                                                    df['sentiment'], \n                                                    train_size=splitFraction, \n                                                    random_state=seed)\nprint(f'Training set size: {len(X_train)}')\nprint(f'Testing set size: {len(X_test)}')\n\nTraining set size: 4722\nTesting set size: 834\n\n\n\n\n5. Imbalanced data and class-weights\nA common solution to imbalanced data is to under- or oversample until the class frequencies are equal. Under-sampling throws out data and reduces the diversity in majority classes. Oversampling duplicates random samples from the minority classes and increases computation time when machine learning.\nI prefer re-weighting the loss function by the class weights, which works for most models. Its effectively the same as oversampling without the computational expense and random sampling.\nAnother really interesting solution is to do sentence augmentation which is oversampling plus a rearrangement of the sentences. This is more applicable considering most sentences are disjoint from others in a short review. This is a work in progress that I will revisit! For now, I stick with class re-weighting.\n\n\nCode\nfrom sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\nsample_weight = compute_sample_weight(class_weight = 'balanced', y = y_train)\nclass_name = y_train.unique()\nclass_weight = compute_class_weight(class_weight = 'balanced', classes=class_name, y = y_train)"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#training-data-preparation",
    "href": "posts/2023-07-10-sentence-embedding/index.html#training-data-preparation",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Training Data Preparation",
    "text": "Training Data Preparation\nThe review text needs to be transformed into embeddings. These embeddings are used repeatedly for all our models and should only be transformed once.\nI will be using two features:\n\nReview text\nStar ratings\n\n\n1. Read and re-label data\n\nimport pandas as pd\n\n# Read and re-label data set\nrewriteDict = {\n    'Positive': 'Positive',\n    'Neutral': 'Negative',\n    'Negative': 'Negative',\n    \"Don't know\": \"Don't know\"\n}\ndf = pd.read_csv(filename, encoding='utf-8-sig')\ndf = df[['reviews','sentiment', 'rating']]\ndf['sentiment'] = df['sentiment'].apply(lambda x: rewriteDict[x]) # Re-label 'Neutral' as 'Negative\n\n\n\n\n\n\n\n\n2. Split data: training and test sets\n\nfrom sklearn.model_selection import train_test_split\nseed = 42\nsplitFraction = 0.85\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    df, \n                                                    df['sentiment'], train_size=splitFraction, random_state=seed)\nprint(f'Training set size: {len(X_train)}')\nprint(f'Testing set size: {len(X_test)}')\n\nTraining set size: 4722\nTesting set size: 834\n\n\n\n\n3. Clean review text\n\n\nCode: Text cleaning functions\nimport emoji\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef remove_emoji(s):\n    # Useful emoji ripper\n    return emoji.replace_emoji(s)\n\ndef remove_excess_and_newline(s):\n    # Removes excessive fullstops (periods)..., !, and newlines.\n    s = re.sub(r\"\\n\", ' ', s)\n    s = re.sub(r\"\\.+\", '.', s)\n    s = re.sub(r\"!+\", '!', s)\n    return s\n\ndef filter_characters(s):\n    # Only permit the follow characters in reviews. \n    # Mainly deletes unwanted symbols such as: (){}#@^&*\n    s = re.sub(r\"[^a-zA-Z\\d\\s'’$\\.!\\-?£€,:éè%/]\", '', s)\n    return s\n\ndef fix_whitespace_around_punctuation(s):\n    # Many reviews have incorrect punc \"wine,try\", \"wine.try\", \"wine .try\"\n    s = re.sub(r\",\", ', ', s)\n    s = re.sub(r\" ,\", ',', s)\n    s = re.sub(r\" \\.\", '.', s)\n    s = re.sub(r\"(?&lt;=[.,!?])(?=[^\\s\\d])\", ' ', s)\n    s = re.sub(r\"(\\s)(?=[.,!])\", '', s)\n    #s = re.sub(r\" !\", '!', s)\n    return s\n\ndef strip_extra_whitespace(s):\n    # Strip 2+ whitespaces\n    s = re.sub(\"\\s+\",' ',s)\n    return s\n\ndef clean_text(s):\n    s = remove_emoji(s)\n    s = remove_excess_and_newline(s)\n    s = filter_characters(s)\n    s = fix_whitespace_around_punctuation(s)\n    s = strip_extra_whitespace(s)\n    return s\n\n\n\nX_train['reviews_clean'] = X_train['reviews'].apply(clean_text)\n\nHere are some examples of the cleaned text.\n\n\nCode\nsampleReviews = [\n    \"Loved !!!! \\nPairs great with a 50$ steak lol or alone;)\",\n    \"Super soft mouth feel.. fruit forward.  Not sure it’s worth the $50.00 but could be my pallet..\",\n    \"Heavy cherries on the nose , followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20......wow!!! Good wine 🍷 anytime!\",\n    \"3.7 ~ 87% ($14.75)\\nFrescobaldi 🇮🇹 Rèmole Rosso 2017\\nMed bodied, bruised, rustic purple ~ black berries, red cherries, and plums ~ smooth as always ~ spicy, oaky, and dry (4 gs/12.5% abv).\\nI had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help 😑\",\n]\n\n# Remove newlines because Markdown hates them.\nsampleReviews = [s.replace('\\n', ' ') for s in sampleReviews]\nsampleReviews_clean = [clean_text(s) for s in sampleReviews]\nMarkdown(tabulate(\n  list(zip(sampleReviews, sampleReviews_clean)), \n  headers=[ \"Review\", \"Pre-processed Review\" ]\n))\n\n\n\n\n\nReview\nPre-processed Review\n\n\n\n\nLoved !!!! Pairs great with a 50$ steak lol or alone;)\nLoved! Pairs great with a 50$ steak lol or alone\n\n\nSuper soft mouth feel.. fruit forward. Not sure it’s worth the $50.00 but could be my pallet..\nSuper soft mouth feel. fruit forward. Not sure it’s worth the $50.00 but could be my pallet.\n\n\nHeavy cherries on the nose , followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20……wow!!! Good wine 🍷 anytime!\nHeavy cherries on the nose, followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20. wow! Good wine anytime!\n\n\n3.7 ~ 87% ($14.75) Frescobaldi 🇮🇹 Rèmole Rosso 2017 Med bodied, bruised, rustic purple ~ black berries, red cherries, and plums ~ smooth as always ~ spicy, oaky, and dry (4 gs/12.5% abv). I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help 😑\n3.7 87% $14.75 Frescobaldi Rèmole Rosso 2017 Med bodied, bruised, rustic purple black berries, red cherries, and plums smooth as always spicy, oaky, and dry 4 gs/12.5% abv. I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help\n\n\n\n\n\n\n\n4. Normalize star-ratings since many models require feature scaling\n\nX_train['norm_rating'] = (X_train['rating'] - 3.0) * 0.5  # Normalize 5-star rating\n\n\n\n5. Create sentence embeddings with SBERT\nNext, we create sentence embeddings for our reviews using SBERT and rescale them. The argument normalize_embeddings returns a unit vector. The vectors are renormalized by the distribution of element values.\n\nembeddings = sentenceModel.encode(list(X_train['reviews']), normalize_embeddings = True)\nembeddings = embeddings - np.mean(embeddings)\nembeddings /= np.std(embeddings)\nX_train['reviews_embeddings'] = list(embeddings)\n\n\n\n6. Imbalanced data and class-weights\nA common solution to imbalanced data is to under- or oversample until the class frequencies are equal. Under-sampling throws out data and reduces the diversity in majority classes. Oversampling duplicates random samples from the minority classes and increases computation time when machine learning.\nI prefer re-weighting the loss function by the class weights, which works for most models. Its effectively the same as oversampling without the computational expense and random sampling.\nAnother really interesting solution is to do sentence augmentation which is oversampling plus a rearrangement of the sentences. This is more applicable considering most sentences are disjoint from others in a short review. This is a work in progress that I will revisit! For now, I stick with class re-weighting.\n\n\nCode\nfrom sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\nsample_weights = compute_sample_weight(class_weight = 'balanced', y = y_train)\nclass_names = y_train.unique()\nclass_weights = compute_class_weight(class_weight = 'balanced', classes=y_train.unique(), y = y_train)"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#a.-dummy-classifier",
    "href": "posts/2023-07-10-sentence-embedding/index.html#a.-dummy-classifier",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "A. Dummy Classifier",
    "text": "A. Dummy Classifier\nThe dummy classifier ignores input features. “Stratified” predicts based on the occurrence frequency of classes from the training set. Our custom weighted score is also shown. This is our baseline.\n\n\nModel: Dummy Classifier\nfrom sklearn.dummy import DummyClassifier\n\ndc_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('DummyClassifier', DummyClassifier(strategy=\"stratified\")), \n])\n\ndummy_model = dc_pipe\ndummy_model.fit(X_train, y_train) \n\nGenerateReport(dummy_model, X_test, y_test)\n\nfname = fdir_model + 'DummyClassifier.dill'\ndill.dump(dummy_model, open(fname, 'wb'))\n\n\n----&gt; Custom weighted score: 0.3680 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.14      0.15      0.14       118\n    Negative       0.27      0.23      0.25       225\n    Positive       0.58      0.60      0.59       491\n\n    accuracy                           0.44       834\n   macro avg       0.33      0.33      0.33       834\nweighted avg       0.43      0.44      0.44       834"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#b.-logistic-regression",
    "href": "posts/2023-07-10-sentence-embedding/index.html#b.-logistic-regression",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "B. Logistic Regression",
    "text": "B. Logistic Regression\nBasic logistic function with L2 regularization.\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nlr_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('LogisticRegression', LogisticRegression(max_iter=10000)), \n])\n\nlr_gs = GridSearchCV(lr_pipe, \n                     param_grid = {\"LogisticRegression__C\": 10**np.arange(-2,2+0.25, 0.25)},\n                     cv = kfolds,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\nlr_gs.fit(X_train, y_train, LogisticRegression__sample_weight=sample_weight) \n\nmodel = lr_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'LogisticRegression.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'LogisticRegression__C': 0.01778279410038923}\n----&gt; Custom weighted score: 0.7078 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.37      0.61      0.46       118\n    Negative       0.69      0.68      0.69       225\n    Positive       0.85      0.72      0.78       491\n\n    accuracy                           0.69       834\n   macro avg       0.64      0.67      0.64       834\nweighted avg       0.74      0.69      0.71       834"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#pipelines-1",
    "href": "posts/2023-07-10-sentence-embedding/index.html#pipelines-1",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Pipelines",
    "text": "Pipelines\nA network of pipes connect the raw data to various transformers where features from the data are engineered. Once complete, the transformed data is sent to a classifier or regressor. We first need to define the transformers before defining the pipes connecting between them.\nStep 1a: Define a transformer that pre-processes text. In this case, I want to prune text to best match embedding model’s training corpus, which were predominantly Reddit comments, article abstracts, and WikiAnswers. The reviews often have incorrect/strange punctuation, missing/excessive white spaces, and emojis. Here is a list of functions to tackle these issues.\nStep 1b: Define a transformer that encodes review text into sentence embeddings.\n\n\nCode: SBERT encoder\nclass SBERTEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # X - pandas series of text\n        embeddings = sentenceModel.encode(list(X.iloc[:,0]), normalize_embeddings = True)\n        # Normalize by the best fit STD: SIGMA_ST \n        embeddings /= SIGMA_ST\n        return embeddings\n\n\nStep 2: Define a column transformer. The CT applies transformers to specified data columns separately. The outputs of these individual transformers are concatenated into one output and passed along.\nOne transformer applies SBERTEncoder() to the ‘reviews’ column. The other selects the ‘ratings’ column and before normalizing with StandardScaler(). Any other input features are ignored.\n\n\nCode: Column Transformer\nfrom sklearn.preprocessing import StandardScaler\n\nSBERTRatingTransformer = ColumnTransformer([\n    ('SBERTEncoder_Transformer', SBERTEncoder(), ['reviews'] ),\n    ('Rating_Transformer', StandardScaler(), ['rating']),\n    ])\n\n\nStep 3: Define a pipeline and specify the classifier. This is where you can experiment with several classifiers to test. Here’s an example of a pipeline below with a Gradient Boosting Classifier. More on this in Part 6!\n\n\nCode: Sample pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\ngbc_pipe = Pipeline([\n    ('SBERTRatingTransformer', SBERTRatingTransformer),\n    ('KNeighborsClassifier', KNeighborsClassifier(n_jobs=8)),\n])\n\n\n\n\nCode\ndf = pd.read_csv(dirIn+\"unlabelled_surveys.csv\", encoding=\"utf-8-sig\")\nlen(df[df['rating']&gt;=4.5]['rating'])/len(df[df['rating']&lt;=1.5]['rating'])\n\n\n21.236842105263158\n\n\n\n\nCode\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"all_labelled_surveys.csv\"\ndf_new = pd.read_csv(filename, encoding='utf-8-sig')\n\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"labelled_surveys_no_ratings.csv\"\ndf_old = pd.read_csv(filename, encoding='utf-8-sig')\n\n\n\n\nCode\ndf = pd.concat([df_new,df_old],ignore_index=True)[['reviews','sentiment']]\n\n\n\n\nCode\ndf[df['sentiment']==\"Don't know\"]\n\n\n\n\n\n\n\n\n\nreviews\nsentiment\n\n\n\n\n2\nGotta love Christmas dinner where the uncles w...\nDon't know\n\n\n4\nFull bodied, flavorful from start to finish, n...\nDon't know\n\n\n9\nIncredible for the price, good celebratory win...\nDon't know\n\n\n12\nAt the Palm Restaurant. $100\\nHeb = 36\\nCostco...\nDon't know\n\n\n14\nExpensive but had to be done once in my life. ...\nDon't know\n\n\n...\n...\n...\n\n\n6126\nMed body, balanced acid, fruit, light tannin. ...\nDon't know\n\n\n6139\n84 Not sure how many bottles of this Wine Enth...\nDon't know\n\n\n6153\n$19.99 in Safeway.\nDon't know\n\n\n6165\n14.5% and $25. Really nice wine. Dry with righ...\nDon't know\n\n\n6166\nGreat flavor, smooth almost sweet, pairs amazi...\nDon't know\n\n\n\n\n719 rows × 2 columns\n\n\n\nBusiness model\nIf each category is mislabelled incorrectly by the same amount then no one’s the wiser. If the model preferentially classifies positives as negative then the distribution of sentiment-ratios shifts down. This has at least two potential effects:\n\nBottles with fewer reviews will suffer more. Since most bottles have fewer reviews, our pool of bottle recommendations decreases which may shorten the lifetime of the app. That’s not great.\nThe liquor store may be unfairly judged more poorly due to their prices being perceived more poorly. Our liquor distribution and sales are primarily run by the government. So, the profit goes back to the government and, nominally, the citizens. I’d be surprised if my app had that effect that.\n\nOn the other hand, if negatives are misclassified as positive then (1) the pool of bottle recommendations increases and (2) the average quality decreases. Thus, users are more likely to buy crappier wines.\nOkay, enough speculations. A solution is to cut away the pool of bottles with fewer reviews since they have higher sensititivities to our model’s performance.\nIf the two missclassification directions are quite unequal, I’d prefer having a more discriminating recommender system. My goal is to recommend wines that are genuinely good and well-priced. If they’re comparable\nA small complication is that DKs can be falsely classified as positive or negative and the proportions need not be equal. The distribution of DKs across star rating is somewhat uniform with a lean towards high-star ratings, which correlates with positive sentiment. This leads to more DKs falsely labelled as positive than negative. This is good news since the impact to the recommendation is relatively nerfed when adding noise to positives because they are counted in both the numerator and denominator in the sentiment-ratio (since positives are counted in both).\n\n3. Sentence Augmentation\nThere are several strategies to dealing with imbalanced data sets. The most common are under and oversampling (i.e., duplicate minority data or throw out majority data). You can also define the sample or class weights to place emphasis.\nFor natural language problems, we can employ sentence augmentation where sentences in a review are re-arranged. Here’s an example with one statement rearranged twice.\n\ntextSamples = [\n    \"I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\", \n    \"Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\"]\nemb = sentenceModel.encode(textSamples, normalize_embeddings=True)\nprint(f'(A) {textSamples[0]}')\nprint(f'(B) {textSamples[1]}')\nprint(f'Cosine similarity of (A) and (A): {np.dot(emb[0],emb[0])}')\nprint(f'Cosine similarity of (A) and (B): {np.dot(emb[0],emb[1])}')\n\n(A) I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\n(B) Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\nCosine similarity of (A) and (A): 1.0\nCosine similarity of (A) and (B): 0.9086364507675171"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#c.-complement-naive-bayes",
    "href": "posts/2023-07-10-sentence-embedding/index.html#c.-complement-naive-bayes",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "C. Complement Naive Bayes",
    "text": "C. Complement Naive Bayes\nThe input to Multinomial Naive Bayes must be positive. We can achieve this by rescaling all the features to be between 0 and 1 using MinMaxScaler(). Complement Naive Bayes is apparently the corrected version of MNB when there is imbalanced data.\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.preprocessing import MinMaxScaler\n\ncnb_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('MinMaxScaler', MinMaxScaler()),\n    ('ComplementNB', ComplementNB()), \n])\n\ncnb_gs = GridSearchCV(cnb_pipe, \n                     param_grid = {\"ComplementNB__alpha\": 10**np.arange(0,3+0.25, 0.25)},\n                     cv = kfolds,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\ncnb_gs.fit(X_train, y_train, ComplementNB__sample_weight=sample_weight) \n\nmodel = cnb_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'ComplementNB.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'ComplementNB__alpha': 1000.0}\n----&gt; Custom weighted score: 0.6566 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.32      0.54      0.40       118\n    Negative       0.67      0.65      0.66       225\n    Positive       0.77      0.65      0.71       491\n\n    accuracy                           0.64       834\n   macro avg       0.59      0.62      0.59       834\nweighted avg       0.68      0.64      0.65       834"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#d.-support-vector-machine",
    "href": "posts/2023-07-10-sentence-embedding/index.html#d.-support-vector-machine",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "D. Support Vector Machine",
    "text": "D. Support Vector Machine\nSVM attempts to draw boundaries between classes of points using hypersurfaces. Each surface has a margin or “thickness” where points inside the margin become ‘support vectors’ which, in fact, define the hyper surface.\nThe amount of “slack” given to a proposed hypersurface is defined by points located on the wrong side of the hypersurface. The total slack is quantified by the sum of distance (or whatever penalty you define) between these points to the surface. A regularization parameter is used to control the strength of a penalty applied from slack. This is defined as C in the following code.\nThe hypersurface is a hyperplane (i.e., linear) by default. A kernel trick is used to allow malleable surfaces. I will explore a number of kernels which have their own hyperparameters (e.g., dimension of polynomial).\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nC_param = 10**np.arange(-1,1+0.5, 0.5)\nparam_grid = [\n    {'SVC__kernel': ['poly'], 'SVC__degree': [1,2,3], 'SVC__C': C_param},\n    {'SVC__kernel':  ['rbf'], 'SVC__C': C_param},\n    {'SVC__kernel': ['sigmoid'], 'SVC__C': C_param}\n]\n\nsvc_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('SVC', SVC(class_weight='balanced', cache_size=2000, random_state=42)), # Can choose a balanced class weighting. \n])\n\n\nsvc_gs = GridSearchCV(svc_pipe, \n                     param_grid = param_grid,\n                     cv = kfolds,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\nsvc_gs.fit(X_train, y_train) \n\nmodel = svc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'SVC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'SVC__C': 1.0, 'SVC__degree': 2, 'SVC__kernel': 'poly'}\n----&gt; Custom weighted score: 0.7125 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.42      0.55      0.48       118\n    Negative       0.70      0.71      0.71       225\n    Positive       0.83      0.77      0.80       491\n\n    accuracy                           0.72       834\n   macro avg       0.65      0.68      0.66       834\nweighted avg       0.74      0.72      0.73       834"
  },
  {
    "objectID": "posts/2023-07-10-sentence-embedding/index.html#e.-random-forest",
    "href": "posts/2023-07-10-sentence-embedding/index.html#e.-random-forest",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "E. Random Forest",
    "text": "E. Random Forest\nThere are many parameters in a decision tree and many more when aggregating over a forest of decision trees. Here are all of the relevant parameters we can change:\n\n\n\n\n\nParameter\nValue\n\n\n\n\nn_estimators\nThe number of trees in the forest.\n\n\nmax_depth\nThe maximum depth of tree from the root.\n\n\nmin_samples_split\nMinimum number of samples required for a split to be considered.\n\n\nmin_samples_leaf\nMinimum number of samples required for each leaf.\n\n\nmax_features\nThe number of features to consider when choosing a split for an internal node.\n\n\nbootstrap\nWhether bootstrap samples are used when building trees.\n\n\noob_score\nWhether to use out-of-bag samples to estimate the generalization accuracy.\n\n\nn_jobs\nThe number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n\n\n\n\n\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nC_param = 10**np.arange(-1,1+0.5, 0.5)\nparam_grid = [\n    {'SVC__kernel': ['poly'], 'SVC__degree': [1,2,3], 'SVC__C': C_param},\n    {'SVC__kernel':  ['rbf'], 'SVC__C': C_param},\n    {'SVC__kernel': ['sigmoid'], 'SVC__C': C_param}\n]\n\nsvc_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('SVC', SVC(class_weight='balanced', cache_size=2000, random_state=42)), # Can choose a balanced class weighting. \n])\n\n\nsvc_gs = GridSearchCV(svc_pipe, \n                     param_grid = param_grid,\n                     cv = kfolds,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\nsvc_gs.fit(X_train, y_train) \n\nmodel = svc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'SVC.dill'\ndill.dump(model, open(fname, 'wb'))"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html",
    "href": "posts/2023-07-10-price-sentiment/index.html",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "",
    "text": "This is part one of a series where I train a machine learning model on price sentiment.\nIn part one, I’ll go over:\nFirst, here’s a motivation for why I did any of this."
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#partisan-sentiment-neutral-is-negative",
    "href": "posts/2023-07-10-price-sentiment/index.html#partisan-sentiment-neutral-is-negative",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "Partisan Sentiment! Neutral is negative",
    "text": "Partisan Sentiment! Neutral is negative\nAfter running a couple experiments, it became immediately clear that most neutral sentiments are falsely classified as positive or negative. This is because distinguishing “not bad” from “good” requires a lot more samples to capture context and language [^1]. I decided to combine the neutral and negative categories as negative because:\n\nI am less likely to purchase a bottle considered “not bad”, let alone a bottle that is “not good”. This is my bias.\nThe neutral and negative categories generally overlap with respect to star-ratings.\nThe pool of negative sentiment data is doubled.\nEliminating a category eliminates a set of potential false classifications."
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#nlp-strategies",
    "href": "posts/2023-07-10-price-sentiment/index.html#nlp-strategies",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "NLP Strategies",
    "text": "NLP Strategies\nMy price-sentiment labels capture two different ideas:\n\nThe sentiment for a price: Positive vs. Negative\nThe existence of a sentiment: “Don’t know” vs. (Positive or Negative)\n\nA popular sentiment analysis strategy is the bag-of-words model. In short, a document is disassociated into a list of words and the context for those words are destroyed. A machine learning model then delineates words (features) that correspond to a document’s label. The values for these features represent the count or some measure of significance (e.g., TF-IDF).\nPrice-sentiment typically relies on a few words and more strongly on context. For instance, here are two real reviews with and without price-sentiment:\n\n[Positive] “Awesome wine for price, selling at coop for $7/bottle”\n[Don’t know] “Great smell and taste $62 at Binnys with 5% off, might have been on sale”\n\nHere are the versions without stopwords:\n\nawesome, price, selling, coop, $7/bottle\ngreat, smell, taste, $62, binnys, 5%, off, might, been, on, sale\n\nIdentifying price-sentiment (or lack thereof) requires context. That’s where Sentence Transformers (STs) shine. STs are trained on an enormous corpus of text to learn the intent and meaning behind sentences. Very minimal pre-processing is required before transforming a sentence. There are setbacks that may prevent you from using STs, however. Here is a list of ST nuances and their relevance for wine reviews:\n\nInput length: STs commonly have 512 token limits which roughly corresponds to 300-400 English words. Longer documents will be truncated unless they are broken into pages, paragraphs, or sometimes sentences. Wine reviews have a 520-character limit or approximately 100 words (see this figure), so this is not an issue.\nTraining data: The ST model is trained on an enormous corpus of text data. It will perform better if your text data is similar to ST’s training data. The good news is that you can fine-tune an ST model to learn your your text’s domain knowledge (e.g., astrophysics)! More on this to come.\nGPU: A GPU can decrease the computational time to transform a sentence by one to two orders of magnitude compared to a CPU. If you have a large set of data, this can be an expensive step without a GPU!\nContext Unlike bag of words, STs are designed to capture the context surrounding words as well. STs attempt to capture the intention or meaning in a sentence. Thus, the output vectors are “sentence” embeddings rather than an array of “word” embeddings generated from a vectorizer. Steps 1-5 from the bag of words procedure are collapsed into one step! Some pre-processing with punctuation might be, however (e.g., a wall of emojis is probably difficult to interpret)."
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#sentence-embedding-with-s-bert",
    "href": "posts/2023-07-10-price-sentiment/index.html#sentence-embedding-with-s-bert",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "Sentence Embedding with S-BERT",
    "text": "Sentence Embedding with S-BERT\n\nImport sentence-transformer\nAfter installing sentence-transformers, select an embedding model and download it automatically using the code below. Here, I show what typical values the embedding vectors take on.\n\n\nCode: sentence-transformer\nfrom sentence_transformers import SentenceTransformer\n\nembeddingModelName = \"all-MiniLM-L6-v2\"\nsentenceModel = SentenceTransformer(embeddingModelName)\noutputDim = sentenceModel.get_sentence_embedding_dimension()\nprint(f\"Number of embedding dimensions: {outputDim}\")\n\ntext = [\n    \"I love plants!\",\n    \"More wine. Give me more! This wine is amazing for $500\",\n    \"It is possible to commit no mistakes and still lose. That is not a weakness. That is life.\",\n    \"I love sentence embeddings. We need more.\"\n]\ntextEmbeddings = sentenceModel.encode(text, normalize_embeddings = True)\n\n\nNumber of embedding dimensions: 384\n\n\n\n\nCode: plot embedding vectors\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef gaussian(x, mu, sigma, a):\n    return a*np.exp(-0.5*(x-mu)**2/sigma**2)\n\n# Figures:\nfig,(ax1,ax2) = plt.subplots(2)\nfig.set_size_inches(10,8)\n# Subplot 1: scatter plot of embeddings\nfor i, embedding in enumerate(textEmbeddings):\n    ax1.scatter(list(range(outputDim)), embedding, s = 1, label=text[i])\nax1.set(xlabel=\"Output vector index\", ylabel='Embedding value')\nax1.legend()\nax1.set_ylim(np.min(textEmbeddings)-0.1, np.max(textEmbeddings) + 0.3)\n\n# Subplot 2: histogram of values and a gaussian fit.\ntextEmbeddings = [t for t in textEmbeddings] # Funny pixel tv without.\nh = ax2.hist(textEmbeddings, bins=20, alpha=0.5, stacked=True)\nxdata = h[1][1:] - 0.5*np.diff(h[1])[0] # get center from bin edges.\nydata = h[0][-1]\n\npopt, pcov = curve_fit(gaussian, xdata, ydata, p0=(0, 0.3, 100))\nSIGMA_ST = popt[1]\nax2.plot(xdata, gaussian(xdata, *popt), \n         'r-', \n         label=r'Gaussian: $\\mu$=%5.5f, $\\sigma$=%5.5f' % tuple(np.abs(popt[0:2])))\nax2.set(xlabel='Embedding value', ylabel = 'Count')\n\nplt.legend()\nplt.subplots_adjust(hspace=0.3)\nplt.show()\n\n\n\n\n\nThe normalized embedding vectors look quite Gaussian.\nHere’s an example of calculating the cosine similarities for each combination of sentence embeddings. There are four wine reviews with positive and negative price sentiments. There is also one quote from Picard, which is clearly dissimilar to the wine reviews.\n\n\n\n\n\n\n\n\n\n\n\n\nTick Label\nText\n\n\n\n\nwine-pos1\nGreat value cab - locally $10.50\n\n\nwine-pos2\nA dark berry nose and deep berry flavor on the top. Blackberries it tells me. A blend that defies the pricing at under $20. A robust hit of smoky oak with a decent finish not usually found in this price range! Can you tell I like it?\n\n\nwine-neg1\nA little lighter than expected, not in a good way. $16 is apparently overpriced. Would not seek out.\n\n\nwine-neg2\nActually not a bad wine but for the $56 we paid for it i can think of many more better wines. Nice after taste but dry.\n\n\npicard\nIt is possible to commit no mistakes and still lose. That is not a weakness. That is life.\n\n\nplants\nI love plants!"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#training-data",
    "href": "posts/2023-07-10-price-sentiment/index.html#training-data",
    "title": "Price-Sentiment Analysis Model using BERT",
    "section": "Training Data",
    "text": "Training Data\nThe review text needs to be transformed into embeddings. These embeddings are used repeatedly for all our models and should only be transformed once.\nI will be using two features:\n\nReview text\nStar ratings\n\n\n1. Read and re-label data\n\nimport pandas as pd\n\n# Read and re-label data set\nrewriteDict = {\n    'Positive': 'Positive',\n    'Neutral': 'Negative',\n    'Negative': 'Negative',\n    \"Don't know\": \"Don't know\"\n}\ndf = pd.read_csv(filename, encoding='utf-8-sig')\ndf = df[['reviews','sentiment', 'rating']]\ndf['sentiment'] = df['sentiment'].apply(lambda x: rewriteDict[x]) # Re-label 'Neutral' as 'Negative\n\n\n\n\n\n\n\n\n2. Clean review text\n\n\nCode: Text cleaning functions\nimport emoji\n\ndef remove_emoji(s):\n    # Useful emoji ripper\n    return emoji.replace_emoji(s)\n\ndef remove_excess_and_newline(s):\n    # Removes excessive fullstops (periods)..., !, and newlines.\n    s = re.sub(r\"\\n\", ' ', s)\n    s = re.sub(r\"\\.+\", '.', s)\n    s = re.sub(r\"!+\", '!', s)\n    return s\n\ndef filter_characters(s):\n    # Only permit the follow characters in reviews. \n    # Mainly deletes unwanted symbols such as: (){}#@^&*\n    s = re.sub(r\"[^a-zA-Z\\d\\s'’$\\.!\\-?£€,:éè%/]\", '', s)\n    return s\n\ndef fix_whitespace_around_punctuation(s):\n    # Many reviews have incorrect punc \"wine,try\", \"wine.try\", \"wine .try\"\n    s = re.sub(r\",\", ', ', s)\n    s = re.sub(r\" ,\", ',', s)\n    s = re.sub(r\" \\.\", '.', s)\n    s = re.sub(r\"(?&lt;=[.,!?])(?=[^\\s\\d])\", ' ', s)\n    s = re.sub(r\"(\\s)(?=[.,!])\", '', s)\n    #s = re.sub(r\" !\", '!', s)\n    return s\n\ndef strip_extra_whitespace(s):\n    # Strip 2+ whitespaces\n    s = re.sub(\"\\s+\",' ',s)\n    return s\n\ndef clean_text(s):\n    s = remove_emoji(s)\n    s = remove_excess_and_newline(s)\n    s = filter_characters(s)\n    s = fix_whitespace_around_punctuation(s)\n    s = strip_extra_whitespace(s)\n    return s\n\n\n\ndf['reviews_clean'] = df['reviews'].apply(clean_text)\n\nHere are some examples of the cleaned text.\n\n\n\n\n\nReview\nPre-processed Review\n\n\n\n\nLoved !!!! Pairs great with a 50$ steak lol or alone;)\nLoved! Pairs great with a 50$ steak lol or alone\n\n\nSuper soft mouth feel.. fruit forward. Not sure it’s worth the $50.00 but could be my pallet..\nSuper soft mouth feel. fruit forward. Not sure it’s worth the $50.00 but could be my pallet.\n\n\nHeavy cherries on the nose , followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20……wow!!! Good wine 🍷 anytime!\nHeavy cherries on the nose, followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20. wow! Good wine anytime!\n\n\n3.7 ~ 87% ($14.75) Frescobaldi 🇮🇹 Rèmole Rosso 2017 Med bodied, bruised, rustic purple ~ black berries, red cherries, and plums ~ smooth as always ~ spicy, oaky, and dry (4 gs/12.5% abv). I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help 😑\n3.7 87% $14.75 Frescobaldi Rèmole Rosso 2017 Med bodied, bruised, rustic purple black berries, red cherries, and plums smooth as always spicy, oaky, and dry 4 gs/12.5% abv. I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help\n\n\n\n\n\n\n\n3. Normalize star-ratings since many models require feature scaling\n\ndf['norm_rating'] = (df['rating'] - 3.0) * 0.5  # Normalize 5-star rating\n\n\n\nCode\ndf['len'] = df['reviews'].apply(len)\ndf = df[df['len']&lt;=350]\n\n\n\n\n4. Transform reviews into sentence embeddings\nSentence embeddings are relatively inexpensive for a GPU. We still want to avoid repeatedly calculating embeddings and separate this process from our pipeline when we repeatedly fit for hyperparameters. I’ve defined transformers below that help add these embeddings to our dataframe. Note: I changed the embedding model to all-roberta-large-v1 to increase the output dimensions from 384 to 1024. This had a significant improvement to model performance.\nI use transformers to help simplify the construction and concatenation of embedding features.\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# New sentence embedding model\nembeddingModelName = \"all-roberta-large-v1\"\n#embeddingModelName = \"all-MiniLM-L6-v2\"\nsentenceModel = SentenceTransformer(embeddingModelName)\noutputDim = sentenceModel.get_sentence_embedding_dimension()\n\n# Embedding transformer\nclass SBERTEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Create embeddings\n        embeddings = sentenceModel.encode(list(X.iloc[:,0]), normalize_embeddings = True)\n        \n        # Normalize by pre-computed mean and std of embeddings.\n        ST_MEAN = np.mean(embeddings)\n        ST_STD = np.std(embeddings)\n        embeddings = embeddings - ST_MEAN\n        embeddings /= ST_STD\n        \n        return embeddings\n    \n# CT extracts desired features (just cleaned reviews in this case)\nEmbeddingTransformer = ColumnTransformer([\n    ('SentenceEmbeddings_transformer', SBERTEncoder(), ['reviews_clean'] ),\n    ])\n\nembeddings = EmbeddingTransformer.fit_transform(df)\n\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef tokenize_lemma(text):\n    return [w.lemma_.lower().strip() for w in nlp(emoji.replace_emoji(text))]\n\n\nfdir = 'D:/vivino_data/stopwords/'\nwith open(fdir+'all_stop_words.txt', encoding='utf-8') as file:\n    stopwords = [line.rstrip() for line in file]\nSTOP_WORDS = set(stopwords)\nSTOP_WORDS = STOP_WORDS.union({'ll', 've'})\nSTOP_WORDS = STOP_WORDS.difference({'he', 'his', 'her','hers'})  # Removing a few words that don't lemmatize well\nSTOP_WORDS = STOP_WORDS - set([''])\nstop_words_lemma = set(tokenize_lemma(' '.join(sorted(STOP_WORDS))))\nstop_words_lemma = list(stop_words_lemma)\ncustom_tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=stop_words_lemma,tokenizer=tokenize_lemma)\nreviewTransformer = ColumnTransformer([\n    ('reviews_transformer', custom_tfidf, 'reviews_clean'),\n    ])\n\ntfvector = reviewTransformer.fit_transform(df)\n\n\nC:\\Python\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\nC:\\Python\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['bro', 'citru', 'roast', '‘'] not in stop_words.\n  warnings.warn(\n\n\n\n\nCode\nfrom scipy.sparse import hstack\ntfvector2 = hstack((tfvector, embeddings,np.array(df['norm_rating'])[:,None]))\n#tfvector2 = np.concatenate((embeddings,np.array(df['norm_rating'])[:,None]), axis=1)\n\n\n\n\nCode\ntfvector2.shape\n\n\n(5162, 31410)\n\n\n\n\nCode\n#df_embeddings = pd.concat([df_embeddings, df], axis=1)\n#df_embeddings.columns = df_embeddings.columns.astype(str) #Required that column names are strings\n\n\n\n\n5. Split data: training and test sets\n\nfrom sklearn.model_selection import train_test_split\nseed = 42\nsplitFraction = 0.85\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    tfvector2, \n                                                    df['sentiment'], \n                                                    train_size=splitFraction, \n                                                    random_state=seed)\nprint(f'Training set shape: {X_train.shape}')\nprint(f'Testing set shape: {X_test.shape}')\n\nTraining set shape: (4387, 31410)\nTesting set shape: (775, 31410)\n\n\n\n\n5. Imbalanced data and class-weights\nA common solution to imbalanced data is to under- or oversample until the class frequencies are equal. Under-sampling throws out data and reduces the diversity in majority classes. Oversampling duplicates random samples from the minority classes and increases computation time when machine learning.\nI prefer re-weighting the loss function by the class weights, which works for most models. Its effectively the same as oversampling without the computational expense and random sampling.\nAnother really interesting solution is to do sentence augmentation which is oversampling plus a rearrangement of the sentences. This is more applicable considering most sentences are disjoint from others in a short review. This is a work in progress that I will revisit! For now, I stick with class re-weighting.\n\n\nCode\nfrom sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\nsample_weight = compute_sample_weight(class_weight = 'balanced', y = y_train)\nclass_name = y_train.unique()\nclass_weight = compute_class_weight(class_weight = 'balanced', classes=class_name, y = y_train)\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'XGBC__eta': [1e-1], \n     'XGBC__max_depth': [2,4,6,10], \n     'XGBC__min_child_weight': [1],\n     'XGBC__subsample': [0.3],\n     'XGBC__n_estimators': [1000],\n     'XGBC__lambda': [1],\n    }\n]\n\nxgb_gs = Pipeline([\n    ('XGBC', XGBClassifier(tree_method='gpu_hist', nthread = 8)), \n])\n\nxgb_gs = GridSearchCV(xgb_gs, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nxgb_gs.fit(X_train, y_train_encoded, XGBC__sample_weight = sample_weight) \n\nmodel = xgb_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'XGBC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nThe best hyperparameter value is:  {'XGBC__eta': 0.1, 'XGBC__lambda': 1, 'XGBC__max_depth': 4, 'XGBC__min_child_weight': 1, 'XGBC__n_estimators': 1000, 'XGBC__subsample': 0.3}\n----&gt; Custom weighted score: 0.7914 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.64      0.44      0.52       108\n    Negative       0.79      0.73      0.76       194\n    Positive       0.83      0.91      0.87       473\n\n    accuracy                           0.80       775\n   macro avg       0.75      0.69      0.72       775\nweighted avg       0.79      0.80      0.79       775\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'LGBM__learning_rate': [1e-2,1e-1], \n     'LGBM__max_depth': [2,4,8], \n     'LGBM__min_child_samples ': [1,5,10,20],\n     'LGBM__subsample': [0.2,0.4],\n     'LGBM__n_estimators': [100,1000],\n     'LGBM__reg_lambda': [1,1.5],\n     'LGBM__colsample_bytree': [0.5, 0.8]\n     \n    }\n]\n\nlgbm_pipe = Pipeline([\n    ('LGBM', LGBMClassifier(class_weight=\"balanced\", random_state =42,  device='gpu',verbose=-1)), \n])\n\nlgbm_gs = GridSearchCV(lgbm_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=4,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nlgbm_gs.fit(X_train, y_train_encoded) \n\nmodel = lgbm_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'LGBM.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 384 candidates, totalling 1920 fits\nThe best hyperparameter value is:  {'LGBM__colsample_bytree': 0.5, 'LGBM__learning_rate': 0.01, 'LGBM__max_depth': 4, 'LGBM__min_child_samples ': 5, 'LGBM__n_estimators': 1000, 'LGBM__reg_lambda': 1.5, 'LGBM__subsample': 0.4}\n----&gt; Custom weighted score: 0.8241 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.52      0.56      0.54       108\n    Negative       0.81      0.79      0.80       194\n    Positive       0.89      0.88      0.88       473\n\n    accuracy                           0.81       775\n   macro avg       0.74      0.74      0.74       775\nweighted avg       0.82      0.81      0.81       775\n\n\n\n\n\n\n\n\nCode\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\n\nThe best hyperparameter value is:  {'LGBM__colsample_bytree': 0.5, 'LGBM__learning_rate': 0.01, 'LGBM__max_depth': 4, 'LGBM__min_child_samples ': 5, 'LGBM__n_estimators': 1000, 'LGBM__reg_lambda': 1.5, 'LGBM__subsample': 0.4}\n\n\n\n\nCode\nfrom sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(model,tfvector2, df['sentiment'],cv=30,return_times=True)\nplt.plot(train_sizes,np.mean(train_scores,axis=1))\n\n\nFitting 5 folds for each of 384 candidates, totalling 1920 fits\n\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#a.-dummy-classifier",
    "href": "posts/2023-07-10-price-sentiment/index.html#a.-dummy-classifier",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "A. Dummy Classifier",
    "text": "A. Dummy Classifier\nThe dummy classifier ignores input features. “Stratified” predicts based on the occurrence frequency of classes from the training set. Our custom weighted score is also shown. This is our baseline.\n\n\nModel: Dummy Classifier\nfrom sklearn.dummy import DummyClassifier\n\ndc_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('DummyClassifier', DummyClassifier(strategy=\"stratified\")), \n])\n\ndummy_model = dc_pipe\ndummy_model.fit(X_train, y_train) \n\nGenerateReport(dummy_model, X_test, y_test)\n\nfname = fdir_model + 'DummyClassifier.dill'\ndill.dump(dummy_model, open(fname, 'wb'))\n\n\nValueError: Specifying the columns using strings is only supported for pandas DataFrames"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#b.-logistic-regression",
    "href": "posts/2023-07-10-price-sentiment/index.html#b.-logistic-regression",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "B. Logistic Regression",
    "text": "B. Logistic Regression\nBasic logistic function with L2 regularization.\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nlr_pipe = Pipeline([\n #   ('SelectorTransformer', SelectorTransformer),\n    ('LogisticRegression', LogisticRegression(max_iter=10000)), \n])\n\nlr_gs = GridSearchCV(lr_pipe, \n                     param_grid = {\"LogisticRegression__C\": 10**np.arange(0,1+0.5, 0.5)},\n                     cv = 5,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\nlr_gs.fit(X_train, y_train, LogisticRegression__sample_weight=sample_weight) \n\nmodel = lr_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'LogisticRegression.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'LogisticRegression__C': 1.0}\n----&gt; Custom weighted score: 0.7978 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.44      0.46      0.45       108\n    Negative       0.79      0.78      0.79       194\n    Positive       0.86      0.85      0.85       473\n\n    accuracy                           0.78       775\n   macro avg       0.69      0.70      0.70       775\nweighted avg       0.78      0.78      0.78       775"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#c.-complement-naive-bayes",
    "href": "posts/2023-07-10-price-sentiment/index.html#c.-complement-naive-bayes",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "C. Complement Naive Bayes",
    "text": "C. Complement Naive Bayes\nThe input to Multinomial Naive Bayes must be positive. We can achieve this by rescaling all the features to be between 0 and 1 using MinMaxScaler(). Complement Naive Bayes is apparently the corrected version of MNB when there is imbalanced data.\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.preprocessing import MinMaxScaler,MaxAbsScaler \n\ncnb_pipe = Pipeline([\n#    ('SelectorTransformer', SelectorTransformer),\n    ('MinMaxScaler', MinMaxScaler()),\n    ('ComplementNB', ComplementNB()), \n])\n\ncnb_gs = GridSearchCV(cnb_pipe, \n                     param_grid = {\"ComplementNB__alpha\": 10**np.arange(0,5+0.5, 0.5)},\n                     cv = 5,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\ncnb_gs.fit(X_train, y_train, ComplementNB__sample_weight=sample_weight) \n\nmodel = cnb_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'ComplementNB.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nValueError: \nAll the 55 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n55 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Python\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Python\\lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 776, in fit\n    self._count(X, Y)\n  File \"C:\\Python\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 1044, in _count\n    check_non_negative(X, \"ComplementNB (input X)\")\n  File \"C:\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1418, in check_non_negative\n    raise ValueError(\"Negative values in data passed to %s\" % whom)\nValueError: Negative values in data passed to ComplementNB (input X)"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#d.-support-vector-machine",
    "href": "posts/2023-07-10-price-sentiment/index.html#d.-support-vector-machine",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "D. Support Vector Machine",
    "text": "D. Support Vector Machine\nSVM attempts to draw boundaries between classes of points using hypersurfaces. Each surface has a margin or “thickness” where points inside the margin become ‘support vectors’ which, in fact, define the hyper surface.\nThe amount of “slack” given to a proposed hypersurface is defined by points located on the wrong side of the hypersurface. The total slack is quantified by the sum of distance (or whatever penalty you define) between these points to the surface. A regularization parameter is used to control the strength of a penalty applied from slack. This is defined as C in the following code.\nThe hypersurface is a hyperplane (i.e., linear) by default. A kernel trick is used to allow malleable surfaces. I will explore a number of kernels which have their own hyperparameters (e.g., dimension of polynomial).\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nC_param = 10**np.arange(-1,1+0.5, 0.5)\nparam_grid = [\n    {'SVC__kernel': ['poly'], 'SVC__degree': [1,2,3], 'SVC__C': C_param},\n    {'SVC__kernel':  ['rbf'], 'SVC__C': C_param},\n    {'SVC__kernel': ['sigmoid'], 'SVC__C': C_param}\n]\n\nsvc_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('SVC', SVC(class_weight='balanced', cache_size=2000, random_state=42)), # Can choose a balanced class weighting. \n])\n\n\nsvc_gs = GridSearchCV(svc_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\nsvc_gs.fit(X_train, y_train) \n\nmodel = svc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'SVC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'SVC__C': 1.0, 'SVC__degree': 2, 'SVC__kernel': 'poly'}\n----&gt; Custom weighted score: 0.7499 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.41      0.61      0.49       118\n    Negative       0.75      0.75      0.75       225\n    Positive       0.88      0.77      0.82       491\n\n    accuracy                           0.74       834\n   macro avg       0.68      0.71      0.69       834\nweighted avg       0.78      0.74      0.75       834\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nC_param = [1]\nparam_grid = [\n    {'SVC__kernel': ['poly'], 'SVC__degree': [3], 'SVC__C': C_param}\n]\n\nsvc_pipe = Pipeline([\n#    ('SelectorTransformer', SelectorTransformer),\n    ('SVC', SVC(class_weight='balanced', cache_size=2000, random_state=42)), # Can choose a balanced class weighting. \n])\n\n\nsvc_gs = GridSearchCV(svc_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\nsvc_gs.fit(X_train, y_train) \n\nmodel = svc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'SVC2.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'SVC__C': 1, 'SVC__degree': 3, 'SVC__kernel': 'poly'}\n----&gt; Custom weighted score: 0.7912 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.50      0.60      0.55       108\n    Negative       0.74      0.77      0.75       194\n    Positive       0.88      0.83      0.85       473\n\n    accuracy                           0.78       775\n   macro avg       0.71      0.73      0.72       775\nweighted avg       0.79      0.78      0.79       775"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#e.-random-forest",
    "href": "posts/2023-07-10-price-sentiment/index.html#e.-random-forest",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "E. Random Forest",
    "text": "E. Random Forest\nThere are many parameters in a decision tree and many more when aggregating over a forest of decision trees. Here are all of the relevant parameters we can change (source):\n\n\n\n\n\nParameter\nValue\n\n\n\n\nn_estimators\nThe number of trees in the forest.\n\n\nmax_depth\nThe maximum depth of tree from the root.\n\n\nmin_samples_split\nMinimum number of samples required for a split to be considered.\n\n\nmin_samples_leaf\nMinimum number of samples required for each leaf.\n\n\nmax_features\nThe number of features to consider when choosing a split for an internal node.\n\n\nbootstrap\nWhether bootstrap samples are used when building trees.\n\n\noob_score\nWhether to use out-of-bag samples to estimate the generalization accuracy.\n\n\nn_jobs\nThe number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = [\n    {'RFC__n_estimators': [3000,30000], \n     'RFC__max_depth': [5], \n     'RFC__min_samples_split': [2],\n     'RFC__min_samples_leaf': [1],\n     'RFC__criterion': ['log_loss']},\n]\n\nrfc_pipe = Pipeline([\n#    ('SelectorTransformer', SelectorTransformer),\n    ('RFC', RandomForestClassifier(class_weight='balanced',random_state=42, n_jobs=4)), # Can choose a balanced class weighting. \n])\n\n\nrfc_gs = GridSearchCV(rfc_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=5,\n                     scoring = sentimentScorer,\n                     )\n\nrfc_gs.fit(X_train, y_train) \n\nmodel = rfc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'RFC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'RFC__criterion': 'log_loss', 'RFC__max_depth': 5, 'RFC__min_samples_leaf': 1, 'RFC__min_samples_split': 2, 'RFC__n_estimators': 3000}\n----&gt; Custom weighted score: 0.7742 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.43      0.58      0.49       108\n    Negative       0.72      0.77      0.75       194\n    Positive       0.88      0.78      0.82       473\n\n    accuracy                           0.75       775\n   macro avg       0.68      0.71      0.69       775\nweighted avg       0.78      0.75      0.76       775"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#f.-extremely-random-forests",
    "href": "posts/2023-07-10-price-sentiment/index.html#f.-extremely-random-forests",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "F. Extremely Random Forests",
    "text": "F. Extremely Random Forests\nLike random forests except nodes are split randomly rather than optimally. Unlike random forests, the samples are not bootstrapped!\nOverall, the results are similar to random forests. The primary difference came from the bootstrap being off by default. This decreased the positive recall and increased the DKs recall, which is worse.\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nparam_grid = [\n    {'ETC__n_estimators': [3000], \n     'ETC__max_depth': [5], \n     'ETC__min_samples_split': [5],\n     'ETC__min_samples_leaf': [1],\n     'ETC__criterion': ['log_loss']},\n]\n\n\netc_pipe = Pipeline([\n #   ('SelectorTransformer', SelectorTransformer),\n    ('ETC', ExtraTreesClassifier(class_weight='balanced',random_state=42, n_jobs=8, bootstrap = True)), # Can choose a balanced class weighting. \n])\n\n\netc_gs = GridSearchCV(etc_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=4,\n                     scoring = sentimentScorer,\n                     )\n\netc_gs.fit(X_train, y_train) \n\nmodel = etc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = model.predict(X_test)\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'ETC3.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'ETC__criterion': 'log_loss', 'ETC__max_depth': 5, 'ETC__min_samples_leaf': 1, 'ETC__min_samples_split': 5, 'ETC__n_estimators': 3000}\n\n\nTypeError: unhashable type: 'csr_matrix'"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#g.-histogram-based-gradient-boosting-lightgbm",
    "href": "posts/2023-07-10-price-sentiment/index.html#g.-histogram-based-gradient-boosting-lightgbm",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "G. Histogram-based Gradient Boosting (~LightGBM)",
    "text": "G. Histogram-based Gradient Boosting (~LightGBM)\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nparam_grid = [\n    {'HGBC__learning_rate': [1e-3], \n     'HGBC__max_depth': [2], \n     'HGBC__max_leaf_nodes': [5],\n     'HGBC__min_samples_leaf': [ 50],\n     'HGBC__l2_regularization': [ 0.5,1.]},\n]\n\n\nhgbc_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('HGBC', HistGradientBoostingClassifier(max_iter=10000, class_weight='balanced', random_state=42)), # Can choose a balanced class weighting. \n])\n\n\nhgbc_gs = GridSearchCV(hgbc_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     error_score='raise',\n                     )\n\nhgbc_gs.fit(X_train, y_train) \n\nmodel = hgbc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\nGenerateReport(model, X_test, y_test)\n\nfname = fdir_model + 'HGBC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'HGBC__l2_regularization': 1.0, 'HGBC__learning_rate': 0.001, 'HGBC__max_depth': 2, 'HGBC__max_leaf_nodes': 5, 'HGBC__min_samples_leaf': 50}\n----&gt; Custom weighted score: 0.6805 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.37      0.53      0.44       118\n    Negative       0.68      0.67      0.67       225\n    Positive       0.80      0.73      0.76       491\n\n    accuracy                           0.68       834\n   macro avg       0.62      0.64      0.62       834\nweighted avg       0.71      0.68      0.69       834"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#h.-xgboost",
    "href": "posts/2023-07-10-price-sentiment/index.html#h.-xgboost",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "H. XGBoost",
    "text": "H. XGBoost\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'XGBC__eta': [1e-2], \n     'XGBC__max_depth': [1, 2, 3], \n     'XGBC__min_child_weight': [1, 2],\n     'XGBC__subsample': [0.3, 0.5, 0.8],\n     'XGBC__n_estimators': [300, 1000]\n     #'XGBC__lambda': [0.5, 1]\n    }\n]\n\nxgb_gs = Pipeline([\n    ('XGBC', XGBClassifier(tree_method='gpu_hist', nthread = 8)), \n])\n\nxgb_gs = GridSearchCV(xgb_gs, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nxgb_gs.fit(X_train, y_train_encoded, XGBC__sample_weight = sample_weight) \n\nmodel = xgb_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'XGBC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 36 candidates, totalling 180 fits\nThe best hyperparameter value is:  {'XGBC__eta': 0.01, 'XGBC__max_depth': 3, 'XGBC__min_child_weight': 1, 'XGBC__n_estimators': 1000, 'XGBC__subsample': 0.8}\n----&gt; Custom weighted score: 0.8217 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.49      0.65      0.56       108\n    Negative       0.81      0.79      0.80       194\n    Positive       0.90      0.84      0.87       473\n\n    accuracy                           0.80       775\n   macro avg       0.73      0.76      0.74       775\nweighted avg       0.82      0.80      0.81       775"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#pipelines",
    "href": "posts/2023-07-10-price-sentiment/index.html#pipelines",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "Pipelines",
    "text": "Pipelines\nA network of pipes connect the raw data to various transformers where features from the data are engineered. Once complete, the transformed data is sent to a classifier or regressor. We first need to define the transformers before defining the pipes connecting between them.\nStep 1a: Define a transformer that pre-processes text. In this case, I want to prune text to best match embedding model’s training corpus, which were predominantly Reddit comments, article abstracts, and WikiAnswers. The reviews often have incorrect/strange punctuation, missing/excessive white spaces, and emojis. Here is a list of functions to tackle these issues.\nStep 1b: Define a transformer that encodes review text into sentence embeddings.\n\n\nCode: SBERT encoder\nclass SBERTEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # X - pandas series of text\n        embeddings = sentenceModel.encode(list(X.iloc[:,0]), normalize_embeddings = True)\n        # Normalize by the best fit STD: SIGMA_ST \n        embeddings /= SIGMA_ST\n        return embeddings\n\n\nStep 3: Define a pipeline and specify the classifier. This is where you can experiment with several classifiers to test. Here’s an example of a pipeline below with a Gradient Boosting Classifier. More on this in Part 6!\n\n\nCode: Sample pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\ngbc_pipe = Pipeline([\n    ('SBERTRatingTransformer', SBERTRatingTransformer),\n    ('KNeighborsClassifier', KNeighborsClassifier(n_jobs=8)),\n])\n\n\n\n\nCode\ndf = pd.read_csv(dirIn+\"unlabelled_surveys.csv\", encoding=\"utf-8-sig\")\nlen(df[df['rating']&gt;=4.5]['rating'])/len(df[df['rating']&lt;=1.5]['rating'])\n\n\n21.236842105263158\n\n\n\n\nCode\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"all_labelled_surveys.csv\"\ndf_new = pd.read_csv(filename, encoding='utf-8-sig')\n\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"labelled_surveys_no_ratings.csv\"\ndf_old = pd.read_csv(filename, encoding='utf-8-sig')\n\n\n\n\nCode\ndf = pd.concat([df_new,df_old],ignore_index=True)[['reviews','sentiment']]\n\n\n\n\nCode\ndf[df['sentiment']==\"Don't know\"]\n\n\n\n\n\n\n\n\n\nreviews\nsentiment\n\n\n\n\n2\nGotta love Christmas dinner where the uncles w...\nDon't know\n\n\n4\nFull bodied, flavorful from start to finish, n...\nDon't know\n\n\n9\nIncredible for the price, good celebratory win...\nDon't know\n\n\n12\nAt the Palm Restaurant. $100\\nHeb = 36\\nCostco...\nDon't know\n\n\n14\nExpensive but had to be done once in my life. ...\nDon't know\n\n\n...\n...\n...\n\n\n6126\nMed body, balanced acid, fruit, light tannin. ...\nDon't know\n\n\n6139\n84 Not sure how many bottles of this Wine Enth...\nDon't know\n\n\n6153\n$19.99 in Safeway.\nDon't know\n\n\n6165\n14.5% and $25. Really nice wine. Dry with righ...\nDon't know\n\n\n6166\nGreat flavor, smooth almost sweet, pairs amazi...\nDon't know\n\n\n\n\n719 rows × 2 columns\n\n\n\nBusiness model\nIf each category is mislabelled incorrectly by the same amount then no one’s the wiser. If the model preferentially classifies positives as negative then the distribution of sentiment-ratios shifts down. This has at least two potential effects:\n\nBottles with fewer reviews will suffer more. Since most bottles have fewer reviews, our pool of bottle recommendations decreases which may shorten the lifetime of the app. That’s not great.\nThe liquor store may be unfairly judged more poorly due to their prices being perceived more poorly. Our liquor distribution and sales are primarily run by the government. So, the profit goes back to the government and, nominally, the citizens. I’d be surprised if my app had that effect that.\n\nOn the other hand, if negatives are misclassified as positive then (1) the pool of bottle recommendations increases and (2) the average quality decreases. Thus, users are more likely to buy crappier wines.\nOkay, enough speculations. A solution is to cut away the pool of bottles with fewer reviews since they have higher sensititivities to our model’s performance.\nIf the two missclassification directions are quite unequal, I’d prefer having a more discriminating recommender system. My goal is to recommend wines that are genuinely good and well-priced. If they’re comparable\nA small complication is that DKs can be falsely classified as positive or negative and the proportions need not be equal. The distribution of DKs across star rating is somewhat uniform with a lean towards high-star ratings, which correlates with positive sentiment. This leads to more DKs falsely labelled as positive than negative. This is good news since the impact to the recommendation is relatively nerfed when adding noise to positives because they are counted in both the numerator and denominator in the sentiment-ratio (since positives are counted in both).\n\n3. Sentence Augmentation\nThere are several strategies to dealing with imbalanced data sets. The most common are under and oversampling (i.e., duplicate minority data or throw out majority data). You can also define the sample or class weights to place emphasis.\nFor natural language problems, we can employ sentence augmentation where sentences in a review are re-arranged. Here’s an example with one statement rearranged twice.\n\ntextSamples = [\n    \"I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\", \n    \"Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\"]\nemb = sentenceModel.encode(textSamples, normalize_embeddings=True)\nprint(f'(A) {textSamples[0]}')\nprint(f'(B) {textSamples[1]}')\nprint(f'Cosine similarity of (A) and (A): {np.dot(emb[0],emb[0])}')\nprint(f'Cosine similarity of (A) and (B): {np.dot(emb[0],emb[1])}')\n\n(A) I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\n(B) Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\nCosine similarity of (A) and (A): 1.0\nCosine similarity of (A) and (B): 0.9086364507675171"
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#footnotes",
    "href": "posts/2023-07-10-price-sentiment/index.html#footnotes",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis includes using a BERT sentence transformer which benefits from context.↩︎"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html",
    "href": "posts/2023-07-11-price-sentiment-2/index.html",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "",
    "text": "This is part two of a series where I train a machine learning model on price sentiment.\nIn part two, I’ll go over:\n\nPreparing the wine review data\nGrid searching machine learning model candidates\nSelecting the best performing model\n\nCheck out part one to learn about the training data."
  },
  {
    "objectID": "posts/2023-07-10-price-sentiment/index.html#training-data-1",
    "href": "posts/2023-07-10-price-sentiment/index.html#training-data-1",
    "title": "Judging Wine Prices: Training Data (Part 1)",
    "section": "Training Data",
    "text": "Training Data\nThe review text needs to be transformed into embeddings. These embeddings are used repeatedly for all our models and should only be transformed once.\nI will be using two features:\n\nReview text\nStar ratings\n\n\n1. Read and re-label data\n\nimport pandas as pd\n\n# Read and re-label data set\nrewriteDict = {\n    'Positive': 'Positive',\n    'Neutral': 'Negative',\n    'Negative': 'Negative',\n    \"Don't know\": \"Don't know\"\n}\ndf = pd.read_csv(filename, encoding='utf-8-sig')\ndf = df[['reviews','sentiment', 'rating']]\ndf['sentiment'] = df['sentiment'].apply(lambda x: rewriteDict[x]) # Re-label 'Neutral' as 'Negative\n\n\n\n\n\n\n\n\n2. Clean review text\n\n\nCode: Text cleaning functions\nimport emoji\n\ndef remove_emoji(s):\n    # Useful emoji ripper\n    return emoji.replace_emoji(s)\n\ndef remove_excess_and_newline(s):\n    # Removes excessive fullstops (periods)..., !, and newlines.\n    s = re.sub(r\"\\n\", ' ', s)\n    s = re.sub(r\"\\.+\", '.', s)\n    s = re.sub(r\"!+\", '!', s)\n    return s\n\ndef filter_characters(s):\n    # Only permit the follow characters in reviews. \n    # Mainly deletes unwanted symbols such as: (){}#@^&*\n    s = re.sub(r\"[^a-zA-Z\\d\\s'’$\\.!\\-?£€,:éè%/]\", '', s)\n    return s\n\ndef fix_whitespace_around_punctuation(s):\n    # Many reviews have incorrect punc \"wine,try\", \"wine.try\", \"wine .try\"\n    s = re.sub(r\",\", ', ', s)\n    s = re.sub(r\" ,\", ',', s)\n    s = re.sub(r\" \\.\", '.', s)\n    s = re.sub(r\"(?&lt;=[.,!?])(?=[^\\s\\d])\", ' ', s)\n    s = re.sub(r\"(\\s)(?=[.,!])\", '', s)\n    #s = re.sub(r\" !\", '!', s)\n    return s\n\ndef strip_extra_whitespace(s):\n    # Strip 2+ whitespaces\n    s = re.sub(\"\\s+\",' ',s)\n    return s\n\ndef clean_text(s):\n    s = remove_emoji(s)\n    s = remove_excess_and_newline(s)\n    s = filter_characters(s)\n    s = fix_whitespace_around_punctuation(s)\n    s = strip_extra_whitespace(s)\n    return s\n\n\n\ndf['reviews_clean'] = df['reviews'].apply(clean_text)\n\nHere are some examples of the cleaned text.\n\n\n\n\n\nReview\nPre-processed Review\n\n\n\n\nLoved !!!! Pairs great with a 50$ steak lol or alone;)\nLoved! Pairs great with a 50$ steak lol or alone\n\n\nSuper soft mouth feel.. fruit forward. Not sure it’s worth the $50.00 but could be my pallet..\nSuper soft mouth feel. fruit forward. Not sure it’s worth the $50.00 but could be my pallet.\n\n\nHeavy cherries on the nose , followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20……wow!!! Good wine 🍷 anytime!\nHeavy cherries on the nose, followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20. wow! Good wine anytime!\n\n\n3.7 ~ 87% ($14.75) Frescobaldi 🇮🇹 Rèmole Rosso 2017 Med bodied, bruised, rustic purple ~ black berries, red cherries, and plums ~ smooth as always ~ spicy, oaky, and dry (4 gs/12.5% abv). I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help 😑\n3.7 87% $14.75 Frescobaldi Rèmole Rosso 2017 Med bodied, bruised, rustic purple black berries, red cherries, and plums smooth as always spicy, oaky, and dry 4 gs/12.5% abv. I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help\n\n\n\n\n\n\n\n3. Normalize star-ratings since many models require feature scaling\n\ndf['norm_rating'] = (df['rating'] - 3.0) * 0.5  # Normalize 5-star rating\n\n\n\nCode\ndf['len'] = df['reviews'].apply(len)\ndf = df[df['len']&lt;=350]\n\n\n\n\n4. Transform reviews into sentence embeddings\nSentence embeddings are relatively inexpensive for a GPU. We still want to avoid repeatedly calculating embeddings and separate this process from our pipeline when we repeatedly fit for hyperparameters. I’ve defined transformers below that help add these embeddings to our dataframe. Note: I changed the embedding model to all-roberta-large-v1 to increase the output dimensions from 384 to 1024. This had a significant improvement to model performance.\nI use transformers to help simplify the construction and concatenation of embedding features.\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# New sentence embedding model\nembeddingModelName = \"all-roberta-large-v1\"\n#embeddingModelName = \"all-MiniLM-L6-v2\"\nsentenceModel = SentenceTransformer(embeddingModelName)\noutputDim = sentenceModel.get_sentence_embedding_dimension()\n\n# Embedding transformer\nclass SBERTEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Create embeddings\n        embeddings = sentenceModel.encode(list(X.iloc[:,0]), normalize_embeddings = True)\n        \n        # Normalize by pre-computed mean and std of embeddings.\n        ST_MEAN = np.mean(embeddings)\n        ST_STD = np.std(embeddings)\n        embeddings = embeddings - ST_MEAN\n        embeddings /= ST_STD\n        \n        return embeddings\n    \n# CT extracts desired features (just cleaned reviews in this case)\nEmbeddingTransformer = ColumnTransformer([\n    ('SentenceEmbeddings_transformer', SBERTEncoder(), ['reviews_clean'] ),\n    ])\n\nembeddings = EmbeddingTransformer.fit_transform(df)\n\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef tokenize_lemma(text):\n    return [w.lemma_.lower().strip() for w in nlp(emoji.replace_emoji(text))]\n\n\nfdir = 'D:/vivino_data/stopwords/'\nwith open(fdir+'all_stop_words.txt', encoding='utf-8') as file:\n    stopwords = [line.rstrip() for line in file]\nSTOP_WORDS = set(stopwords)\nSTOP_WORDS = STOP_WORDS.union({'ll', 've'})\nSTOP_WORDS = STOP_WORDS.difference({'he', 'his', 'her','hers'})  # Removing a few words that don't lemmatize well\nSTOP_WORDS = STOP_WORDS - set([''])\nstop_words_lemma = set(tokenize_lemma(' '.join(sorted(STOP_WORDS))))\nstop_words_lemma = list(stop_words_lemma)\ncustom_tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=stop_words_lemma,tokenizer=tokenize_lemma)\nreviewTransformer = ColumnTransformer([\n    ('reviews_transformer', custom_tfidf, 'reviews_clean'),\n    ])\n\ntfvector = reviewTransformer.fit_transform(df)\n\n\nC:\\Python\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\nC:\\Python\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['bro', 'citru', 'roast', '‘'] not in stop_words.\n  warnings.warn(\n\n\n\n\nCode\nfrom scipy.sparse import hstack\ntfvector2 = hstack((tfvector, embeddings,np.array(df['norm_rating'])[:,None]))\n#tfvector2 = np.concatenate((embeddings,np.array(df['norm_rating'])[:,None]), axis=1)\n\n\n\n\nCode\ntfvector2.shape\n\n\n(5162, 31410)\n\n\n\n\nCode\n#df_embeddings = pd.concat([df_embeddings, df], axis=1)\n#df_embeddings.columns = df_embeddings.columns.astype(str) #Required that column names are strings\n\n\n\n\n5. Split data: training and test sets\n\nfrom sklearn.model_selection import train_test_split\nseed = 42\nsplitFraction = 0.85\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    tfvector2, \n                                                    df['sentiment'], \n                                                    train_size=splitFraction, \n                                                    random_state=seed)\nprint(f'Training set shape: {X_train.shape}')\nprint(f'Testing set shape: {X_test.shape}')\n\nTraining set shape: (4387, 31410)\nTesting set shape: (775, 31410)\n\n\n\n\n5. Imbalanced data and class-weights\nA common solution to imbalanced data is to under- or oversample until the class frequencies are equal. Under-sampling throws out data and reduces the diversity in majority classes. Oversampling duplicates random samples from the minority classes and increases computation time when machine learning.\nI prefer re-weighting the loss function by the class weights, which works for most models. Its effectively the same as oversampling without the computational expense and random sampling.\nAnother really interesting solution is to do sentence augmentation which is oversampling plus a rearrangement of the sentences. This is more applicable considering most sentences are disjoint from others in a short review. This is a work in progress that I will revisit! For now, I stick with class re-weighting.\n\n\nCode\nfrom sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\nsample_weight = compute_sample_weight(class_weight = 'balanced', y = y_train)\nclass_name = y_train.unique()\nclass_weight = compute_class_weight(class_weight = 'balanced', classes=class_name, y = y_train)\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'XGBC__eta': [1e-1], \n     'XGBC__max_depth': [2,4,6,10], \n     'XGBC__min_child_weight': [1],\n     'XGBC__subsample': [0.3],\n     'XGBC__n_estimators': [1000],\n     'XGBC__lambda': [1],\n    }\n]\n\nxgb_gs = Pipeline([\n    ('XGBC', XGBClassifier(tree_method='gpu_hist', nthread = 8)), \n])\n\nxgb_gs = GridSearchCV(xgb_gs, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nxgb_gs.fit(X_train, y_train_encoded, XGBC__sample_weight = sample_weight) \n\nmodel = xgb_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'XGBC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nThe best hyperparameter value is:  {'XGBC__eta': 0.1, 'XGBC__lambda': 1, 'XGBC__max_depth': 4, 'XGBC__min_child_weight': 1, 'XGBC__n_estimators': 1000, 'XGBC__subsample': 0.3}\n----&gt; Custom weighted score: 0.7914 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.64      0.44      0.52       108\n    Negative       0.79      0.73      0.76       194\n    Positive       0.83      0.91      0.87       473\n\n    accuracy                           0.80       775\n   macro avg       0.75      0.69      0.72       775\nweighted avg       0.79      0.80      0.79       775\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'LGBM__learning_rate': [1e-2,1e-1], \n     'LGBM__max_depth': [2,4,8], \n     'LGBM__min_child_samples ': [1,5,10,20],\n     'LGBM__subsample': [0.2,0.4],\n     'LGBM__n_estimators': [100,1000],\n     'LGBM__reg_lambda': [1,1.5],\n     'LGBM__colsample_bytree': [0.5, 0.8]\n     \n    }\n]\n\nlgbm_pipe = Pipeline([\n    ('LGBM', LGBMClassifier(class_weight=\"balanced\", random_state =42,  device='gpu',verbose=-1)), \n])\n\nlgbm_gs = GridSearchCV(lgbm_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=4,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nlgbm_gs.fit(X_train, y_train_encoded) \n\nmodel = lgbm_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'LGBM.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 384 candidates, totalling 1920 fits\nThe best hyperparameter value is:  {'LGBM__colsample_bytree': 0.5, 'LGBM__learning_rate': 0.01, 'LGBM__max_depth': 4, 'LGBM__min_child_samples ': 5, 'LGBM__n_estimators': 1000, 'LGBM__reg_lambda': 1.5, 'LGBM__subsample': 0.4}\n----&gt; Custom weighted score: 0.8241 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.52      0.56      0.54       108\n    Negative       0.81      0.79      0.80       194\n    Positive       0.89      0.88      0.88       473\n\n    accuracy                           0.81       775\n   macro avg       0.74      0.74      0.74       775\nweighted avg       0.82      0.81      0.81       775\n\n\n\n\n\n\n\n\nCode\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\n\nThe best hyperparameter value is:  {'LGBM__colsample_bytree': 0.5, 'LGBM__learning_rate': 0.01, 'LGBM__max_depth': 4, 'LGBM__min_child_samples ': 5, 'LGBM__n_estimators': 1000, 'LGBM__reg_lambda': 1.5, 'LGBM__subsample': 0.4}\n\n\n\n\nCode\nfrom sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(model,tfvector2, df['sentiment'],cv=30,return_times=True)\nplt.plot(train_sizes,np.mean(train_scores,axis=1))\n\n\nFitting 5 folds for each of 384 candidates, totalling 1920 fits\n\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#training-data",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#training-data",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "Training Data",
    "text": "Training Data\n\n\n1. Read and re-label data\n\n\nCode: Read and re-label ‘Neutral’ as ‘Negative’\nimport pandas as pd\n\n# Read and re-label data set\nrewriteDict = {\n    'Positive': 'Positive',\n    'Neutral': 'Negative',\n    'Negative': 'Negative',\n    \"Don't know\": \"Don't know\"\n}\ndf = pd.read_csv(filename, encoding='utf-8-sig')\ndf = df[['reviews','sentiment', 'rating']]\ndf['sentiment'] = df['sentiment'].apply(lambda x: rewriteDict[x]) # Re-label 'Neutral' as 'Negative\n\n\n\n\n\n\n\n\n\n2. Rescale star-ratings since many models require feature scaling\n\n\n\n\n\n\ndf['unit_rating'] = (df['rating'] - 1) / 4  # Normalize 5-star rating\n\n\n\n3. Filter longer reviews (1%) from training\nHere is a log-histogram of word frequency per review that I made in this post. The number of words per review cannot exceed a threshold since (a) the character limit per review is 520, and (b) the average characters per word is 4.79 (or 5.79 if you include a white space). This threshold is 520 / 5.79 ~ 90, which is approximately where the knee of the distribution is.\n\n\n\nfig-words-output-1.png\n\n\nFrom experiments, I found that filtering out reviews with more than 350 characters (roughly 60 average-length words) improved every model’s performance by a few percent. The price-sentiment takes relatively little estate in a review. So, sacrificing 1% of reviews is a small penalty for a 3% model improvement!\n\ndf['len'] = df['reviews'].apply(len)\ndf = df[df['len']&lt;=350]\n\n\n\n4. Strip and clean review text\n\n\nCode: Text cleaning functions\nimport emoji\nimport re\n\ndef remove_emoji(s):\n    # Useful emoji ripper\n    return emoji.replace_emoji(s)\n\ndef remove_excess_and_newline(s):\n    # Removes excessive fullstops (periods)..., !, and newlines.\n    s = re.sub(r\"\\n\", ' ', s)\n    s = re.sub(r\"\\.+\", '.', s)\n    s = re.sub(r\"!+\", '!', s)\n    return s\n\ndef filter_characters(s):\n    # Only permit the follow characters in reviews. \n    # Mainly deletes unwanted symbols such as: (){}#@^&*\n    s = re.sub(r\"[^a-zA-Z\\d\\s'’$\\.!\\-?£€,:éè%/]\", '', s)\n    return s\n\ndef filter_only_characters(s):\n    # Only permit characters in reviews. \n    s = re.sub(r\"[^a-zA-Z\\s]\", '', s)\n    return s\n\ndef fix_whitespace_around_punctuation(s):\n    # Many reviews have incorrect punc \"wine,try\", \"wine.try\", \"wine .try\"\n    s = re.sub(r\",\", ', ', s)\n    s = re.sub(r\" ,\", ',', s)\n    s = re.sub(r\" \\.\", '.', s)\n    s = re.sub(r\"(?&lt;=[.,!?])(?=[^\\s\\d])\", ' ', s)\n    s = re.sub(r\"(\\s)(?=[.,!])\", '', s)\n    #s = re.sub(r\" !\", '!', s)\n    return s\n\ndef strip_extra_whitespace(s):\n    # Strip 2+ whitespaces\n    s = re.sub(\"\\s+\",' ',s)\n    return s\n\ndef cleaned_text(s):\n    s = remove_emoji(s)\n    s = remove_excess_and_newline(s)\n    s = filter_characters(s)\n    s = fix_whitespace_around_punctuation(s)\n    s = strip_extra_whitespace(s)\n    return s\n\n\n\ndf['reviews_clean'] = df['reviews'].apply(cleaned_text)\n\nC:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_28044\\1431066477.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['reviews_clean'] = df['reviews'].apply(cleaned_text)\n\n\nHere are some examples of the cleaned text.\n\n\n\n\n\nReview\nPre-processed Review\n\n\n\n\nLoved !!!! Pairs great with a 50$ steak lol or alone;)\nLoved! Pairs great with a 50$ steak lol or alone\n\n\nSuper soft mouth feel.. fruit forward. Not sure it’s worth the $50.00 but could be my pallet..\nSuper soft mouth feel. fruit forward. Not sure it’s worth the $50.00 but could be my pallet.\n\n\nHeavy cherries on the nose , followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20……wow!!! Good wine 🍷 anytime!\nHeavy cherries on the nose, followed by a blackberry taste and spicy velvety smooth finish! More of a light-medium Cabernet and for the money, under $20. wow! Good wine anytime!\n\n\n3.7 ~ 87% ($14.75) Frescobaldi 🇮🇹 Rèmole Rosso 2017 Med bodied, bruised, rustic purple ~ black berries, red cherries, and plums ~ smooth as always ~ spicy, oaky, and dry (4 gs/12.5% abv). I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help 😑\n3.7 87% $14.75 Frescobaldi Rèmole Rosso 2017 Med bodied, bruised, rustic purple black berries, red cherries, and plums smooth as always spicy, oaky, and dry 4 gs/12.5% abv. I had the patience of a Buddhist monk, and let this one stew in my bat cave for THREE years! It didn’t really seem to help\n\n\n\n\n\n\n\n5. Word significance: bag of words with TF-IDF vectorizer\n\nNext, I define a TF-IDF vectorizer.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef tokenize_lemma(text):\n    text = filter_only_characters(text)\n    # Lemmatize, lowercase, strip each word token\n    return [w.lemma_.lower().strip() for w in nlp(text)]\n\nTFIDFEncoder = TfidfVectorizer(\n    ngram_range=(1, 2), \n    # A list of stop words: common, wine nomenclature, dates\n    stop_words=STOP_WORDS, \n    tokenizer=tokenize_lemma,\n    )\n\n\n\n6. Sentence abstraction: transform reviews into sentence embeddings\n\nHere is a (scikit-learn) transformer that creates embeddings using the sentence transformer.\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# New sentence embedding model\nembeddingModelName = \"all-roberta-large-v1\"\nsentenceModel = SentenceTransformer(embeddingModelName)\noutputDim = sentenceModel.get_sentence_embedding_dimension()\n\n# Embedding transformer\nclass SBERTEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, shift=1, norm=2):\n        self.shift = shift\n        self.norm = norm\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Create embeddings\n        embeddings = sentenceModel.encode(\n            list(X.iloc[:,0]), \n            normalize_embeddings = True\n            )\n        # Rescaling is needed to ensure the elements are between [0,1]\n        embeddings = embeddings + self.shift\n        embeddings *= self.norm\n        return embeddings\n\n\n\n7. Combine engineered features\n\nFinally, I create a column transformer that plucks the data columns we want and applies various transformers to them. The outputs of each transformer are merged back together into a large (semi-)sparse matrix.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\n\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, key, make_df = False):\n        self.key = key\n        self.make_df = make_df\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if self.make_df:\n            return pd.DataFrame(X[self.key])\n        return X[self.key]\n    \nFullSentenceTransformer = ColumnTransformer([\n    ('StarRating_transformer', 'passthrough', ['unit_rating']),\n    ('TFIDFEncoder_transformer', TFIDFEncoder,  'reviews_clean'),\n    ('SentenceEmbeddings_transformer', SBERTEncoder(), ['reviews_clean']),\n])\n\nX = FullSentenceTransformer.fit_transform(df)\nvocabulary = FullSentenceTransformer.transformers_[1][1].vocabulary_\n\n\n\n8. Label encode classes\nXGBoost and LightGBM require the classes be encoded as integers while the other algorithms do not. Might as well encode everything.\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_encoded = LE.fit_transform(df['sentiment'])\ny_encoded_classes = LE.classes_\ny_encoded_classes\n\n\narray([\"Don't know\", 'Negative', 'Positive'], dtype=object)\n\n\n\n\n9. Split data into training and test sets\n\nfrom sklearn.model_selection import train_test_split\nseed = 42\nsplitFraction = 0.85\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y_encoded, \n                                                    train_size=splitFraction, \n                                                    random_state=seed,\n                                                    stratify = y_encoded) \n                                # stratify tries to ensure the split has equal class proprotions\nprint(f'Training set shape: {X_train.shape}')\nprint(f'Testing set shape: {X_test.shape}')\n\nTraining set shape: (4387, 28436)\nTesting set shape: (775, 28436)\n\n\n\n\n10. Create weights for imbalanced data\nA common solution to imbalanced data is to under- or oversample until the class frequencies are equal. Under-sampling throws out data and reduces the diversity in majority classes. Oversampling duplicates random samples from the minority classes and increases computation time when machine learning.\nI prefer re-weighting the loss function by the class weights, which works for most models. Its effectively the same as oversampling without the computational expense and random sampling.\nAnother really interesting solution is to do sentence augmentation which is oversampling plus a rearrangement of the sentences. This is more applicable considering most sentences are disjoint from others in a short review. This is a work in progress that I will revisit! For now, I stick with class re-weighting.\n\n\nCode\nfrom sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\nsample_weight_train = compute_sample_weight(class_weight = 'balanced', y = y_train)\nclass_name = np.unique(y_train)\nclass_weight_train = compute_class_weight(class_weight = 'balanced', classes=class_name, y = y_train)\nclass_weight_train = class_weight_train/np.min(class_weight_train)\nclass_weight_dict = dict(zip(class_name,class_weight_train))\nclass_weight_dict\n\n\n{0: 5.014705882352941, 1: 2.4466367713004487, 2: 1.0}"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#a.-dummy-classifier",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#a.-dummy-classifier",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "A. Dummy Classifier",
    "text": "A. Dummy Classifier\nThe dummy classifier ignores input features. “Stratified” predicts based on the occurrence frequency of classes from the training set. Our custom weighted score is also shown. This is our baseline.\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.dummy import DummyClassifier\n\nmodelName = 'DummyClassifier'\npipe = Pipeline([\n    (modelName, DummyClassifier(strategy=\"stratified\")), \n])\n\nmodel = pipe\nmodel.fit(X_train, y_train) \n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\nplot_roc_pr(fname)"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#b.-logistic-regression",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#b.-logistic-regression",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "B. Logistic Regression",
    "text": "B. Logistic Regression\nBasic logistic function with L2 regularization.\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nmodelName = \"LogisticRegression\"\npipe = Pipeline([\n    (modelName, LogisticRegression(max_iter=10000, random_state=42)), \n])\n\nparam_grid = {\"C\": 10**np.arange(-1, 1+0.25, 0.2)}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     )\n\nkwargs_fit = {modelName+'__sample_weight':sample_weight_train}\nmodel.fit(X_train, y_train, **kwargs_fit) \nprint(\"The best hyperparameter value is: \")\nfor k,v in model.best_params_.items():\n    print(f\"{k}: {v}\")\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is: \nLogisticRegression__C: 0.9999999999999994\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighest polarity words: Logistic Regression \n\n\n\n\n\n\n\n\nPolarity\nPositive Words\nPolarity\nNegative Words\n\n\n\n\n2.85\ngood abv\n-4.18\nnot bargain\n\n\n2.56\ngreat absolute\n-4.16\ndecent absolute\n\n\n1.94\ngood valueprice\n-3.82\nbad acidic\n\n\n1.68\nvalue able\n-3.44\nnot able\n\n\n1.35\nexcellent approachable\n-2.79\nok atar\n\n\n1.32\npretty great\n-1.75\ndecent well\n\n\n1.32\nnice above\n-1.72\nbad purchase\n\n\n1.32\ngreat versatile\n-1.66\nnot ws\n\n\n1.28\nworth average\n-1.63\nlike grab\n\n\n1.22\npretty acidic\n-1.58\nnot worthy"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#c.-complement-naive-bayes",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#c.-complement-naive-bayes",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "C. Complement Naive Bayes",
    "text": "C. Complement Naive Bayes\nComplement Naive Bayes is apparently the corrected version of MNB when there is imbalanced data.\nNote: this is an extremely fast algorithm!\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import ComplementNB\n\nmodelName = \"ComplementNB\"\npipe = Pipeline([\n    (modelName, ComplementNB()), \n])\n\nparam_grid = {\"alpha\": 10**np.arange(-1,1, 0.01)}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 10,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     return_train_score = True,\n                     )\n\nkwargs_fit = {modelName+'__sample_weight':sample_weight_train}\nmodel.fit(X_train, y_train, **kwargs_fit) \nprint(\"The best hyperparameters: \")\nfor k,v in model.best_params_.items():\n    print(f\"{k}: {v}\")\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameters: \nComplementNB__alpha: 4.7863009232264\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighest polarity words \n\n\n\n\n\n\n\n\nPolarity\nPositive Words\nPolarity\nNegative Words\n\n\n\n\n1.68\ngreat versatile\n-1.98\nnot worthy\n\n\n1.44\ngood valueprice\n-1.94\nbad acidic\n\n\n1.2\nvalue able\n-1.93\nnot bargain\n\n\n1.16\ngreat absolute\n-1.41\nok atar\n\n\n0.95\ngreat pricepoint\n-1.04\nwould ok\n\n\n0.89\nsteal but\n-0.98\ndecent absolute\n\n\n0.86\nexcellent vfm\n-0.9\noverprice artificially\n\n\n0.86\nexcellent approachable\n-0.88\nnot able\n\n\n0.75\nbeat beat\n-0.88\nnot cad\n\n\n0.74\nwould challenge\n-0.86\nnot grab"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#d.-support-vector-machine",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#d.-support-vector-machine",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "D. Support Vector Machine",
    "text": "D. Support Vector Machine\nSVM attempts to draw boundaries between classes of points using hypersurfaces. Each surface has a margin or “thickness” where points inside the margin become ‘support vectors’ which, in fact, define the hyper surface.\nThe amount of “slack” given to a proposed hypersurface is defined by points located on the wrong side of the hypersurface. The total slack is quantified by the sum of distance (or whatever penalty you define) between these points to the surface. A regularization parameter is used to control the strength of a penalty applied from slack. This is defined as C in the following code.\nThe hypersurface is a hyperplane (i.e., linear) by default. A kernel trick is used to allow malleable surfaces. I will explore a number of kernels which have their own hyperparameters (e.g., dimension of polynomial).\nNote: The assumption that the data can be split using a hyperplane is quite strong considering the sparsity of the data and extremely high number of features. SVMs are quite expensive for high numbers of features much like KNNs since the algorithm requires an interpolation of subsets of points (a generally slow process).\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport numpy as np\n\ndef linear(X, Y):\n    return np.dot(X, Y.T)  \n\nmodelName = \"SVC\"\npipe = Pipeline([\n    (modelName, SVC(class_weight='balanced', cache_size=2000, random_state=42)), \n])\n\nC_param = 10**np.arange(0, 2+0.5, 0.25)\nparam_grid = [\n    {'kernel': [linear], 'C': C_param}, # This is much faster than default linear\n    {'kernel': ['poly'], 'degree': [2,3], 'C': C_param},\n    {'kernel':  ['rbf'], 'C': C_param},\n]\n\nparam_grid = [\n    {f\"{modelName}__{k}\": grid[k] for k in grid.keys()} for grid in param_grid\n]\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5, #Reduced for time\n                     n_jobs=8,\n                     scoring = sentimentScorer,\n                     )\n\nmodel.fit(X_train, y_train) \nprint(\"The best hyperparameters: \")\nfor k,v in model.best_params_.items():\n    print(f\"{k}: {v}\")\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameters: \nSVC__C: 56.23413251903491\nSVC__kernel: rbf"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#e.-random-forest",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#e.-random-forest",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "E. Random Forest",
    "text": "E. Random Forest\nThere are many parameters in a decision tree and many more when aggregating over a forest of decision trees. Here are all of the relevant parameters we can change (source):\nNote: Grid search was not extensive due to speed! Skipped to LGBM and XGBoost.\n\n\n\n\n\nParameter\nValue\n\n\n\n\nn_estimators\nThe number of trees in the forest.\n\n\nmax_depth\nThe maximum depth of tree from the root.\n\n\nmin_samples_split\nMinimum number of samples required for a split to be considered.\n\n\nmin_samples_leaf\nMinimum number of samples required for each leaf.\n\n\nmax_features\nThe number of features to consider when choosing a split for an internal node.\n\n\nbootstrap\nWhether bootstrap samples are used when building trees.\n\n\noob_score\nWhether to use out-of-bag samples to estimate the generalization accuracy.\n\n\nn_jobs\nThe number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.Criterion for splitting quality: Gini, Entropy, or Log Loss\n\n\n\n\n\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodelName = \"RFC\"\npipe = Pipeline([\n    (modelName, RandomForestClassifier(class_weight='balanced',random_state=42)), \n])\n\nparam_grid = {\n    'n_estimators': [1000], \n    'max_depth': [2, 4, 6, 8], \n    'min_samples_split': [2],\n    'min_samples_leaf': [1],\n    'criterion': ['gini', 'log_loss'],\n}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=8,\n                     scoring = sentimentScorer,\n                     )\n\nmodel.fit(X_train, y_train) \n\nprint(\"The best hyperparameters: \")\nfor k,v in model.best_params_.items():\n    print(f\"{k}: {v}\")\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameters: \nRFC__criterion: gini\nRFC__max_depth: 6\nRFC__min_samples_leaf: 1\nRFC__min_samples_split: 2\nRFC__n_estimators: 1000"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#f.-extremely-random-forests",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#f.-extremely-random-forests",
    "title": "Judging Wine Prices: Choosing a ML Model",
    "section": "F. Extremely Random Forests",
    "text": "F. Extremely Random Forests\nLike random forests except nodes are split randomly rather than optimally. Unlike random forests, the samples are not bootstrapped!\nOverall, the results are similar to random forests. The primary difference came from the bootstrap being off by default. This decreased the positive recall and increased the DKs recall, which is worse.\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodelName = \"ETC\"\npipe = Pipeline([\n    (modelName, ExtraTreesClassifier(class_weight='balanced',random_state=42, bootstrap = True)), \n])\n\nparam_grid = {\n    'n_estimators': [300, 1000], \n    'max_depth': [2, 4, 6], \n    'min_samples_split': [2],\n    'min_samples_leaf': [1],\n    'criterion': ['gini', 'log_loss'],\n}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=8,\n                     scoring = sentimentScorer,\n                     )\n\n\nmodel.fit(X_train, y_train) \nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = model.predict(X_test)\nGenerateReport(y_pred, y_test, encoder = LE)\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'ETC__criterion': 'gini', 'ETC__max_depth': 6, 'ETC__min_samples_leaf': 1, 'ETC__min_samples_split': 2, 'ETC__n_estimators': 1000}\n----&gt; Custom weighted score: 0.7822 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.46      0.57      0.51       108\n    Negative       0.73      0.76      0.75       194\n    Positive       0.88      0.81      0.84       473\n\n    accuracy                           0.77       775\n   macro avg       0.69      0.72      0.70       775\nweighted avg       0.78      0.77      0.77       775"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#g.-histogram-based-gradient-boosting-lightgbm",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#g.-histogram-based-gradient-boosting-lightgbm",
    "title": "Judging Wine Prices: Choosing a ML Model",
    "section": "G. Histogram-based Gradient Boosting (~LightGBM)",
    "text": "G. Histogram-based Gradient Boosting (~LightGBM)\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nparam_grid = [\n    {'HGBC__learning_rate': [1e-3], \n     'HGBC__max_depth': [2], \n     'HGBC__max_leaf_nodes': [5],\n     'HGBC__min_samples_leaf': [ 50],\n     'HGBC__l2_regularization': [ 0.5,1.]},\n]\n\n\nhgbc_pipe = Pipeline([\n    ('SelectorTransformer', SelectorTransformer),\n    ('HGBC', HistGradientBoostingClassifier(max_iter=10000, class_weight='balanced', random_state=42)), # Can choose a balanced class weighting. \n])\n\n\nhgbc_gs = GridSearchCV(hgbc_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=10,\n                     scoring = sentimentScorer,\n                     error_score='raise',\n                     )\n\nhgbc_gs.fit(X_train, y_train) \n\nmodel = hgbc_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\ny_pred = model.predict(X_test)\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'HGBC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'HGBC__l2_regularization': 1.0, 'HGBC__learning_rate': 0.001, 'HGBC__max_depth': 2, 'HGBC__max_leaf_nodes': 5, 'HGBC__min_samples_leaf': 50}\n----&gt; Custom weighted score: 0.6805 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.37      0.53      0.44       118\n    Negative       0.68      0.67      0.67       225\n    Positive       0.80      0.73      0.76       491\n\n    accuracy                           0.68       834\n   macro avg       0.62      0.64      0.62       834\nweighted avg       0.71      0.68      0.69       834"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#h.-xgboost",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#h.-xgboost",
    "title": "Judging Wine Prices: Choosing a ML Model",
    "section": "H. XGBoost",
    "text": "H. XGBoost\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'XGBC__eta': [1e-2], \n     'XGBC__max_depth': [1, 2, 3], \n     'XGBC__min_child_weight': [1, 2],\n     'XGBC__subsample': [0.3, 0.5, 0.8],\n     'XGBC__n_estimators': [300, 1000]\n     #'XGBC__lambda': [0.5, 1]\n    }\n]\n\nxgb_gs = Pipeline([\n    ('XGBC', XGBClassifier(tree_method='gpu_hist', nthread = 8)), \n])\n\nxgb_gs = GridSearchCV(xgb_gs, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nxgb_gs.fit(X_train, y_train_encoded, XGBC__sample_weight = sample_weight) \n\nmodel = xgb_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'XGBC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 36 candidates, totalling 180 fits\nThe best hyperparameter value is:  {'XGBC__eta': 0.01, 'XGBC__max_depth': 3, 'XGBC__min_child_weight': 1, 'XGBC__n_estimators': 1000, 'XGBC__subsample': 0.8}\n----&gt; Custom weighted score: 0.8217 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.49      0.65      0.56       108\n    Negative       0.81      0.79      0.80       194\n    Positive       0.90      0.84      0.87       473\n\n    accuracy                           0.80       775\n   macro avg       0.73      0.76      0.74       775\nweighted avg       0.82      0.80      0.81       775"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#pipelines",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#pipelines",
    "title": "Judging Wine Prices: Choosing a ML Model",
    "section": "Pipelines",
    "text": "Pipelines\nA network of pipes connect the raw data to various transformers where features from the data are engineered. Once complete, the transformed data is sent to a classifier or regressor. We first need to define the transformers before defining the pipes connecting between them.\nStep 1a: Define a transformer that pre-processes text. In this case, I want to prune text to best match embedding model’s training corpus, which were predominantly Reddit comments, article abstracts, and WikiAnswers. The reviews often have incorrect/strange punctuation, missing/excessive white spaces, and emojis. Here is a list of functions to tackle these issues.\nStep 1b: Define a transformer that encodes review text into sentence embeddings.\n\n\nCode: SBERT encoder\nclass SBERTEncoder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # X - pandas series of text\n        embeddings = sentenceModel.encode(list(X.iloc[:,0]), normalize_embeddings = True)\n        # Normalize by the best fit STD: SIGMA_ST \n        embeddings /= SIGMA_ST\n        return embeddings\n\n\nStep 3: Define a pipeline and specify the classifier. This is where you can experiment with several classifiers to test. Here’s an example of a pipeline below with a Gradient Boosting Classifier. More on this in Part 6!\n\n\nCode: Sample pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\ngbc_pipe = Pipeline([\n    ('SBERTRatingTransformer', SBERTRatingTransformer),\n    ('KNeighborsClassifier', KNeighborsClassifier(n_jobs=8)),\n])\n\n\n\n\nCode\ndf = pd.read_csv(dirIn+\"unlabelled_surveys.csv\", encoding=\"utf-8-sig\")\nlen(df[df['rating']&gt;=4.5]['rating'])/len(df[df['rating']&lt;=1.5]['rating'])\n\n\n21.236842105263158\n\n\n\n\nCode\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"all_labelled_surveys.csv\"\ndf_new = pd.read_csv(filename, encoding='utf-8-sig')\n\ndirIn = \"D:\\\\vivino_data\\\\survey_results\\\\\"\nfilename = dirIn + \"labelled_surveys_no_ratings.csv\"\ndf_old = pd.read_csv(filename, encoding='utf-8-sig')\n\n\n\n\nCode\ndf = pd.concat([df_new,df_old],ignore_index=True)[['reviews','sentiment']]\n\n\n\n\nCode\ndf[df['sentiment']==\"Don't know\"]\n\n\n\n\n\n\n\n\n\nreviews\nsentiment\n\n\n\n\n2\nGotta love Christmas dinner where the uncles w...\nDon't know\n\n\n4\nFull bodied, flavorful from start to finish, n...\nDon't know\n\n\n9\nIncredible for the price, good celebratory win...\nDon't know\n\n\n12\nAt the Palm Restaurant. $100\\nHeb = 36\\nCostco...\nDon't know\n\n\n14\nExpensive but had to be done once in my life. ...\nDon't know\n\n\n...\n...\n...\n\n\n6126\nMed body, balanced acid, fruit, light tannin. ...\nDon't know\n\n\n6139\n84 Not sure how many bottles of this Wine Enth...\nDon't know\n\n\n6153\n$19.99 in Safeway.\nDon't know\n\n\n6165\n14.5% and $25. Really nice wine. Dry with righ...\nDon't know\n\n\n6166\nGreat flavor, smooth almost sweet, pairs amazi...\nDon't know\n\n\n\n\n719 rows × 2 columns\n\n\n\nBusiness model\nIf each category is mislabelled incorrectly by the same amount then no one’s the wiser. If the model preferentially classifies positives as negative then the distribution of sentiment-ratios shifts down. This has at least two potential effects:\n\nBottles with fewer reviews will suffer more. Since most bottles have fewer reviews, our pool of bottle recommendations decreases which may shorten the lifetime of the app. That’s not great.\nThe liquor store may be unfairly judged more poorly due to their prices being perceived more poorly. Our liquor distribution and sales are primarily run by the government. So, the profit goes back to the government and, nominally, the citizens. I’d be surprised if my app had that effect that.\n\nOn the other hand, if negatives are misclassified as positive then (1) the pool of bottle recommendations increases and (2) the average quality decreases. Thus, users are more likely to buy crappier wines.\nOkay, enough speculations. A solution is to cut away the pool of bottles with fewer reviews since they have higher sensititivities to our model’s performance.\nIf the two missclassification directions are quite unequal, I’d prefer having a more discriminating recommender system. My goal is to recommend wines that are genuinely good and well-priced. If they’re comparable\nA small complication is that DKs can be falsely classified as positive or negative and the proportions need not be equal. The distribution of DKs across star rating is somewhat uniform with a lean towards high-star ratings, which correlates with positive sentiment. This leads to more DKs falsely labelled as positive than negative. This is good news since the impact to the recommendation is relatively nerfed when adding noise to positives because they are counted in both the numerator and denominator in the sentiment-ratio (since positives are counted in both).\n\n3. Sentence Augmentation\nThere are several strategies to dealing with imbalanced data sets. The most common are under and oversampling (i.e., duplicate minority data or throw out majority data). You can also define the sample or class weights to place emphasis.\nFor natural language problems, we can employ sentence augmentation where sentences in a review are re-arranged. Here’s an example with one statement rearranged twice.\n\ntextSamples = [\n    \"I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\", \n    \"Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\"]\nemb = sentenceModel.encode(textSamples, normalize_embeddings=True)\nprint(f'(A) {textSamples[0]}')\nprint(f'(B) {textSamples[1]}')\nprint(f'Cosine similarity of (A) and (A): {np.dot(emb[0],emb[0])}')\nprint(f'Cosine similarity of (A) and (B): {np.dot(emb[0],emb[1])}')\n\n(A) I love this for $5. Let it open for a minute and then its a lovely burst of blueberry.\n(B) Let it open for a minute and then its a lovely burst of blueberry. I love this for $5.\nCosine similarity of (A) and (A): 1.0\nCosine similarity of (A) and (B): 0.9086364507675171\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'XGBC__eta': [1e-1], \n     'XGBC__max_depth': [2,4,6,10], \n     'XGBC__min_child_weight': [1],\n     'XGBC__subsample': [0.3],\n     'XGBC__n_estimators': [1000],\n     'XGBC__lambda': [1],\n    }\n]\n\nxgb_gs = Pipeline([\n    ('XGBC', XGBClassifier(tree_method='gpu_hist', nthread = 8)), \n])\n\nxgb_gs = GridSearchCV(xgb_gs, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nxgb_gs.fit(X_train, y_train_encoded, XGBC__sample_weight = sample_weight) \n\nmodel = xgb_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'XGBC.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nThe best hyperparameter value is:  {'XGBC__eta': 0.1, 'XGBC__lambda': 1, 'XGBC__max_depth': 4, 'XGBC__min_child_weight': 1, 'XGBC__n_estimators': 1000, 'XGBC__subsample': 0.3}\n----&gt; Custom weighted score: 0.7914 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.64      0.44      0.52       108\n    Negative       0.79      0.73      0.76       194\n    Positive       0.83      0.91      0.87       473\n\n    accuracy                           0.80       775\n   macro avg       0.75      0.69      0.72       775\nweighted avg       0.79      0.80      0.79       775\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\ny_train_encoded = LE.fit_transform(y_train)\nsentimentScorer_encoded = make_scorer(weightedScore, encoder=LE, greater_is_better = True)\n\nparam_grid = [\n    {'LGBM__learning_rate': [1e-2,1e-1], \n     'LGBM__max_depth': [2,4,8], \n     'LGBM__min_child_samples ': [1,5,10,20],\n     'LGBM__subsample': [0.2,0.4],\n     'LGBM__n_estimators': [100,1000],\n     'LGBM__reg_lambda': [1,1.5],\n     'LGBM__colsample_bytree': [0.5, 0.8]\n     \n    }\n]\n\nlgbm_pipe = Pipeline([\n    ('LGBM', LGBMClassifier(class_weight=\"balanced\", random_state =42,  device='gpu',verbose=-1)), \n])\n\nlgbm_gs = GridSearchCV(lgbm_pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=4,\n                     scoring = sentimentScorer_encoded,\n                     verbose=True,\n                     )\n\nlgbm_gs.fit(X_train, y_train_encoded) \n\nmodel = lgbm_gs\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = LE.inverse_transform(model.predict(X_test))\nGenerateReport(y_pred, y_test)\n\nfname = fdir_model + 'LGBM.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 384 candidates, totalling 1920 fits\nThe best hyperparameter value is:  {'LGBM__colsample_bytree': 0.5, 'LGBM__learning_rate': 0.01, 'LGBM__max_depth': 4, 'LGBM__min_child_samples ': 5, 'LGBM__n_estimators': 1000, 'LGBM__reg_lambda': 1.5, 'LGBM__subsample': 0.4}\n----&gt; Custom weighted score: 0.8241 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.52      0.56      0.54       108\n    Negative       0.81      0.79      0.80       194\n    Positive       0.89      0.88      0.88       473\n\n    accuracy                           0.81       775\n   macro avg       0.74      0.74      0.74       775\nweighted avg       0.82      0.81      0.81       775\n\n\n\n\n\n\n\n\nCode\nprint(\"The best hyperparameter value is: \", model.best_params_)\n\n\nThe best hyperparameter value is:  {'LGBM__colsample_bytree': 0.5, 'LGBM__learning_rate': 0.01, 'LGBM__max_depth': 4, 'LGBM__min_child_samples ': 5, 'LGBM__n_estimators': 1000, 'LGBM__reg_lambda': 1.5, 'LGBM__subsample': 0.4}"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#g.-xgboost",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#g.-xgboost",
    "title": "Judging Wine Prices: Choosing a ML Model",
    "section": "G. XGBoost",
    "text": "G. XGBoost\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\nmodelName = \"XGBC\"\npipe = Pipeline([\n    (modelName, XGBClassifier(tree_method='gpu_hist', nthread = 8)), \n])\n\nparam_grid = {\n    'eta': [1e-2],\n    'n_estimators': [300, 1000], \n    'max_depth': [1, 2, 3], \n    'min_child_weight': [1, 2],\n    'subsample': [0.3, 0.5, 0.8],\n}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer,\n                     )\n\nkwargs_fit = {modelName+'__sample_weight':sample_weight_train}\nmodel.fit(X_train, y_train, **kwargs_fit) \nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = model.predict(X_test)\nGenerateReport(y_pred, y_test, encoder = LE)\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'XGBC__eta': 0.01, 'XGBC__max_depth': 3, 'XGBC__min_child_weight': 1, 'XGBC__n_estimators': 1000, 'XGBC__subsample': 0.8}\n----&gt; Custom weighted score: 0.8248 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.48      0.65      0.55       108\n    Negative       0.81      0.79      0.80       194\n    Positive       0.90      0.84      0.87       473\n\n    accuracy                           0.80       775\n   macro avg       0.73      0.76      0.74       775\nweighted avg       0.82      0.80      0.81       775"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#h.-lightgbm",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#h.-lightgbm",
    "title": "Judging Wine Prices: Choosing a ML Model",
    "section": "H. LightGBM",
    "text": "H. LightGBM\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\n\nmodelName = \"LGBM\"\npipe = Pipeline([\n    (modelName, LGBMClassifier(class_weight=\"balanced\", random_state= 42,  device='gpu', verbose=-1)), \n])\n\nparam_grid = {\n    'learning_rate': [1e-2,1e-1], \n    'max_depth': [2,4,8], \n    'min_child_samples ': [1,5,10,20],\n    'subsample': [0.2,0.4],\n    'n_estimators': [100,1000],\n    'reg_lambda': [1,1.5],\n    'colsample_bytree': [0.5, 0.8]\n}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=4,\n                     scoring = sentimentScorer,\n                     verbose=True,\n                     )\n\nmodel.fit(X_train, y_train) \nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = model.predict(X_test)\nGenerateReport(y_pred, y_test, encoder = LE)\n\nfname = fdir_model + 'LGBM.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nFitting 5 folds for each of 384 candidates, totalling 1920 fits\nThe best hyperparameter value is:  {'LGBM__colsample_bytree': 0.5, 'LGBM__learning_rate': 0.01, 'LGBM__max_depth': 4, 'LGBM__min_child_samples ': 5, 'LGBM__n_estimators': 1000, 'LGBM__reg_lambda': 1.5, 'LGBM__subsample': 0.4}\n----&gt; Custom weighted score: 0.8241 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.52      0.56      0.54       108\n    Negative       0.81      0.79      0.80       194\n    Positive       0.89      0.88      0.88       473\n\n    accuracy                           0.81       775\n   macro avg       0.74      0.74      0.74       775\nweighted avg       0.82      0.81      0.81       775\n\n\n\n\n\n\n\n\nCode\nimport dill\n\nnames = ['XGBC', 'LogisticRegression', 'ComplementNB']\nfilenames = [fdir_model + name + '.dill' for name in names]\nmodels = [(names[i], dill.load(open(filenames[i],'rb')).best_estimator_) for i in range(len(names))]\n\n\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\nmodelName = \"XGBC\"\npipe = Pipeline([\n    (modelName, XGBClassifier(tree_method='gpu_hist', nthread = 2)), \n])\n\nparam_grid = {\n    'eta': [1e-2],\n    'n_estimators': [1000], \n    'max_depth': [3], \n    'min_child_weight': [1],\n    'subsample': [0.8],\n}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer,\n                     )\n\nkwargs_fit = {modelName+'__sample_weight':sample_weight_train}\nmodel.fit(X_train, y_train, **kwargs_fit) \nprint(\"The best hyperparameter value is: \", model.best_params_)\n\ny_pred = model.predict(X_test)\nGenerateReport(y_pred, y_test, encoder = LE)\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameter value is:  {'XGBC__eta': 0.01, 'XGBC__max_depth': 3, 'XGBC__min_child_weight': 1, 'XGBC__n_estimators': 1000, 'XGBC__subsample': 0.8}\n----&gt; Custom weighted score: 0.8001 &lt;----\n                             ^^^^^^\n              precision    recall  f1-score   support\n\n  Don't know       0.48      0.68      0.56        96\n    Negative       0.76      0.76      0.76       197\n    Positive       0.90      0.82      0.86       482\n\n    accuracy                           0.79       775\n   macro avg       0.71      0.75      0.73       775\nweighted avg       0.81      0.79      0.80       775"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#f.-xgboost",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#f.-xgboost",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "F. XGBoost",
    "text": "F. XGBoost\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\nmodelName = \"XGBC\"\npipe = Pipeline([\n    (modelName, XGBClassifier(tree_method='gpu_hist', random_state=42)), \n])\n\nparam_grid = {\n    'eta': [1e-2],\n    'n_estimators': [1000], \n    'max_depth': [2,4,6], \n    'min_child_weight': [1],\n    'subsample': [0.5, 0.8],\n    'colsample_bytree': [0.5, 0.8, 1]\n}\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=2,\n                     scoring = sentimentScorer,\n                     )\n\nkwargs_fit = {modelName+'__sample_weight':sample_weight_train}\nmodel.fit(X_train, y_train, **kwargs_fit) \n\nprint(\"The best hyperparameters: \")\nfor k,v in model.best_params_.items():\n    print(f\"{k}: {v}\")\n\n# CV does not set this by default... Needed for yellowbricks.\nmodel.best_estimator_.steps[0][1].set_params(**{'num_class': len(LE.classes_)})\n\nfname = fdir_model + f'{modelName}.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameters: \nXGBC__colsample_bytree: 0.8\nXGBC__eta: 0.01\nXGBC__max_depth: 4\nXGBC__min_child_weight: 1\nXGBC__n_estimators: 1000\nXGBC__subsample: 0.8"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#g.-lightgbm",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#g.-lightgbm",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "G. LightGBM",
    "text": "G. LightGBM\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\n\nmodelName = \"LGBM\"\npipe = Pipeline([\n    (modelName, LGBMClassifier(class_weight=\"balanced\", random_state= 42,  device='gpu', verbose=-1)), \n])\n\nparam_grid = {\n    'learning_rate': [1e-2], \n    'max_depth': [2, 4, 6], \n    'subsample': [0.5, 0.8],\n    'n_estimators': [1000],\n    'reg_lambda': [1],\n    'colsample_bytree': [0.5, 0.8, 1]\n}\n\nparam_grid = {f\"{modelName}__{k}\": param_grid[k] for k in param_grid.keys()}\n\nmodel = GridSearchCV(pipe, \n                     param_grid = param_grid,\n                     cv = 5,\n                     n_jobs=4,\n                     scoring = sentimentScorer,\n                     )\n\nmodel.fit(X_train, y_train) \nprint(\"The best hyperparameters: \")\nfor k,v in model.best_params_.items():\n    print(f\"{k}: {v}\")\n\nfname = fdir_model + 'LGBM.dill'\ndill.dump(model, open(fname, 'wb'))\n\n\nThe best hyperparameters: \nLGBM__colsample_bytree: 0.5\nLGBM__learning_rate: 0.01\nLGBM__max_depth: 4\nLGBM__n_estimators: 1000\nLGBM__reg_lambda: 1\nLGBM__subsample: 0.5"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#dummy-classifier",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#dummy-classifier",
    "title": "Judging Wine Prices: Choosing a ML Model",
    "section": "Dummy Classifier",
    "text": "Dummy Classifier\n\n\n\nDummyClassifier.png"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#logistic-regression",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#logistic-regression",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "1. Logistic Regression",
    "text": "1. Logistic Regression"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#complement-naive-bayes",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#complement-naive-bayes",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "2. Complement Naive Bayes",
    "text": "2. Complement Naive Bayes"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#support-vector-machine",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#support-vector-machine",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "3. Support Vector Machine",
    "text": "3. Support Vector Machine"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#random-forest-classifier",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#random-forest-classifier",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "4. Random Forest Classifier",
    "text": "4. Random Forest Classifier"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#xgboost-winner",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#xgboost-winner",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "5. XGBoost (Winner!)",
    "text": "5. XGBoost (Winner!)"
  },
  {
    "objectID": "posts/2023-07-11-price-sentiment-2/index.html#lightgbm",
    "href": "posts/2023-07-11-price-sentiment-2/index.html#lightgbm",
    "title": "Judging Wine Prices: Selecting a Model (Part 2)",
    "section": "6. LightGBM",
    "text": "6. LightGBM"
  }
]